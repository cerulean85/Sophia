# 3번
**정답: A. Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.**

**해설:**
- **aws:PrincipalOrgID 글로벌 조건 키:**
  - **조직 내 접근 제한:** `aws:PrincipalOrgID` 글로벌 조건 키를 사용하면 특정 AWS Organizations ID에 속한 계정만 S3 버킷에 접근할 수 있도록 제한할 수 있습니다. 이는 조직 내의 모든 계정을 포함하므로, 각 계정의 사용자들이 접근할 수 있습니다.
  - **운영 오버헤드 최소화:** 이 방법은 S3 버킷 정책에 단일 조건을 추가하는 것만으로 구현할 수 있어, 운영 오버헤드가 최소화됩니다. 추가적인 관리 작업이 필요하지 않습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. aws:PrincipalOrgPaths 글로벌 조건 키:** `aws:PrincipalOrgPaths`는 조직 경로를 기반으로 접근을 제어할 수 있지만, 각 부서별로 조직 단위를 생성하고 관리해야 하므로 운영 오버헤드가 증가할 수 있습니다.
- **C. AWS CloudTrail 모니터링:** CloudTrail을 사용하여 계정 생성 및 조직 변경 이벤트를 모니터링하고 S3 버킷 정책을 업데이트하는 것은 복잡하고 운영 오버헤드가 큽니다. 실시간으로 정책을 업데이트하는 것도 어려울 수 있습니다.
- **D. aws:PrincipalTag 글로벌 조건 키:** 각 사용자에게 태그를 추가하고 이를 기반으로 접근을 제어하는 것은 많은 관리 작업이 필요합니다. 특히, 새로운 사용자가 추가될 때마다 태그를 관리해야 하므로 운영 오버헤드가 큽니다.

**출제 의도:**
이 문제는 AWS Organizations를 사용하여 여러 AWS 계정을 관리할 때, 특정 S3 버킷에 대한 접근을 조직 내의 계정으로 제한하는 방법을 평가합니다. 특히, 운영 오버헤드를 최소화하면서 접근 제어를 구현하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Organizations:**
   - AWS Organizations의 기본 개념과 사용 사례
   - 조직 ID와 조직 단위(OU)의 역할

2. **S3 버킷 정책:**
   - S3 버킷 정책의 기본 개념과 사용 사례
   - 글로벌 조건 키(`aws:PrincipalOrgID`, `aws:PrincipalOrgPaths`, `aws:PrincipalTag`)를 사용하여 접근을 제어하는 방법

3. **AWS CloudTrail:**
   - CloudTrail의 기본 개념과 사용 사례
   - CloudTrail을 사용하여 조직 이벤트를 모니터링하는 방법

이 부분들을 공부하면, AWS에서 조직 내의 계정으로 접근을 제한하는 방법을 더 잘 이해할 수 있습니다.

# 16번

## 정답
- B. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.**

**해설:**
- **Amazon QuickSight:**
  - **데이터 시각화:** QuickSight는 다양한 데이터 소스를 연결하여 대시보드를 생성하고 데이터를 시각화할 수 있는 서비스입니다. Amazon S3와 Amazon RDS for PostgreSQL을 포함한 여러 데이터 소스를 연결할 수 있습니다.
  - **사용자 및 그룹 공유:** QuickSight는 대시보드를 특정 사용자 및 그룹과 공유할 수 있는 기능을 제공합니다. 이를 통해 관리 팀에게는 전체 접근 권한을 부여하고, 나머지 직원들에게는 제한된 접근 권한을 부여할 수 있습니다.
  - **IAM 통합:** QuickSight는 AWS IAM과 통합되어, 사용자 및 그룹 기반의 접근 제어를 쉽게 설정할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. IAM 역할 공유:** IAM 역할을 사용하여 대시보드를 공유하는 것은 가능하지만, QuickSight의 사용자 및 그룹 기반 공유 기능이 더 적합합니다. 사용자 및 그룹을 통해 더 세밀한 접근 제어를 설정할 수 있습니다.
- **C. AWS Glue 및 S3:** Glue를 사용하여 데이터를 처리하고 S3에 보고서를 게시하는 것은 가능하지만, 데이터 시각화 및 대시보드 생성 기능이 부족합니다. 또한, S3 버킷 정책을 사용하여 접근을 제한하는 것은 관리 오버헤드가 큽니다.
- **D. AWS Glue 및 Athena:** Glue와 Athena를 사용하여 데이터를 처리하고 보고서를 생성하는 것은 가능하지만, 데이터 시각화 및 대시보드 생성 기능이 부족합니다. 또한, S3 버킷 정책을 사용하여 접근을 제한하는 것은 관리 오버헤드가 큽니다.

**출제 의도:**
이 문제는 AWS에서 데이터 시각화 및 보고 솔루션을 구현하는 방법을 평가합니다. 특히, Amazon QuickSight를 사용하여 다양한 데이터 소스를 연결하고, 대시보드를 생성하며, 사용자 및 그룹 기반의 접근 제어를 설정하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon QuickSight:**
   - QuickSight의 기본 개념과 사용 사례
   - QuickSight를 사용하여 데이터 소스를 연결하고 대시보드를 생성하는 방법
   - QuickSight에서 사용자 및 그룹과 대시보드를 공유하는 방법

2. **AWS Glue:**
   - Glue의 기본 개념과 사용 사례
   - Glue를 사용하여 데이터를 처리하고 ETL 작업을 수행하는 방법

3. **Amazon Athena:**
   - Athena의 기본 개념과 사용 사례
   - Athena를 사용하여 데이터를 쿼리하고 보고서를 생성하는 방법

4. **Amazon S3:**
   - S3의 기본 개념과 사용 사례
   - S3 버킷 정책을 사용하여 접근을 제어하는 방법

이 부분들을 공부하면, AWS에서 데이터 시각화 및 보고 솔루션을 효율적으로 구현하는 방법을 더 잘 이해할 수 있습니다.

# 18번
**정답: A. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.**
**정답: B. Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.**

**해설:**
- **Amazon SQS와 Lambda 통합:**
  - **내구성 및 무상태 컴포넌트:** SQS는 내구성 있는 메시지 큐잉 서비스로, 메시지를 안전하게 저장하고 전달할 수 있습니다. Lambda 함수는 무상태 컴퓨팅 서비스로, SQS 큐에서 메시지를 읽어와 이미지를 처리할 수 있습니다.
  - **자동 처리:** S3 버킷에 이미지가 업로드되면 SQS 큐에 알림을 보내고, Lambda 함수가 SQS 큐에서 메시지를 읽어와 이미지를 처리하고 압축된 이미지를 다른 S3 버킷에 저장할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **C. Lambda 함수가 S3 버킷을 모니터링:** Lambda 함수가 직접 S3 버킷을 모니터링하는 것은 가능하지만, 파일 이름을 메모리에 저장하고 추적하는 것은 내구성이 부족하고, 무상태 컴포넌트의 장점을 활용하지 못합니다.
- **D. EC2 인스턴스 사용:** EC2 인스턴스를 사용하여 SQS 큐를 모니터링하고 Lambda 함수를 호출하는 것은 불필요하게 복잡하며, 무상태 컴포넌트의 장점을 활용하지 못합니다.
- **E. EventBridge와 SNS 사용:** EventBridge와 SNS를 사용하여 알림을 보내는 것은 가능하지만, 이는 이미지 처리를 자동화하는 데 적합하지 않습니다. 이메일 알림은 추가적인 수동 작업이 필요할 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 내구성 있고 무상태 컴포넌트를 사용하여 이미지를 자동으로 처리하는 솔루션을 설계하는 방법을 평가합니다. 특히, SQS와 Lambda를 사용하여 이미지를 처리하고 압축하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon SQS:**
   - SQS의 기본 개념과 사용 사례
   - SQS를 사용하여 메시지를 큐잉하고 전달하는 방법

2. **AWS Lambda:**
   - Lambda의 기본 개념과 사용 사례
   - Lambda를 사용하여 SQS 메시지를 처리하는 방법

3. **Amazon S3:**
   - S3의 기본 개념과 사용 사례
   - S3 이벤트 알림을 설정하여 SQS 큐로 알림을 보내는 방법

이 부분들을 공부하면, AWS에서 내구성 있고 무상태 컴포넌트를 사용하여 이미지를 자동으로 처리하는 방법을 더 잘 이해할 수 있습니다.

# 19번
**정답: D. Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.**

**해설:**
- **Gateway Load Balancer:**
  - **트래픽 검사:** Gateway Load Balancer는 트래픽을 가로채고, 가상 어플라이언스(예: 방화벽)로 전달하여 트래픽을 검사할 수 있도록 설계되었습니다. 이는 트래픽을 검사하는 데 최적화된 솔루션입니다.
  - **운영 오버헤드 최소화:** Gateway Load Balancer는 자동으로 확장되고, 고가용성을 제공하며, 관리 오버헤드가 적습니다. 또한, Gateway Load Balancer 엔드포인트를 사용하여 VPC 간에 트래픽을 쉽게 전달할 수 있습니다.
  - **통합:** Gateway Load Balancer는 AWS Marketplace에서 제공되는 가상 어플라이언스와 쉽게 통합할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Network Load Balancer:** Network Load Balancer는 트래픽을 분산시키는 데 사용되지만, 트래픽을 검사하는 데 최적화되지 않았습니다. 또한, 트래픽을 가상 어플라이언스로 전달하는 기능이 부족합니다.
- **B. Application Load Balancer:** Application Load Balancer는 HTTP/HTTPS 트래픽을 분산시키는 데 사용되며, 트래픽을 검사하는 데 최적화되지 않았습니다. 또한, 트래픽을 가상 어플라이언스로 전달하는 기능이 부족합니다.
- **C. Transit Gateway:** Transit Gateway는 VPC 간의 트래픽을 라우팅하는 데 사용되지만, 트래픽을 검사하는 데 최적화되지 않았습니다. 또한, 관리 오버헤드가 높을 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 트래픽을 검사하고, 운영 오버헤드를 최소화하는 방법을 평가합니다. 특히, Gateway Load Balancer를 사용하여 트래픽을 가상 어플라이언스로 전달하고 검사하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Gateway Load Balancer:**
   - Gateway Load Balancer의 기본 개념과 사용 사례
   - Gateway Load Balancer를 사용하여 트래픽을 가상 어플라이언스로 전달하고 검사하는 방법

2. **Network Load Balancer:**
   - Network Load Balancer의 기본 개념과 사용 사례
   - Network Load Balancer를 사용하여 트래픽을 분산시키는 방법

3. **Application Load Balancer:**
   - Application Load Balancer의 기본 개념과 사용 사례
   - Application Load Balancer를 사용하여 HTTP/HTTPS 트래픽을 분산시키는 방법

4. **Transit Gateway:**
   - Transit Gateway의 기본 개념과 사용 사례
   - Transit Gateway를 사용하여 VPC 간의 트래픽을 라우팅하는 방법

이 부분들을 공부하면, AWS에서 트래픽을 검사하고 운영 오버헤드를 최소화하는 방법을 더 잘 이해할 수 있습니다.


# 20번
**정답: D. Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.**

**해설:**
- **EBS Fast Snapshot Restore:**
  - **빠른 복원:** EBS Fast Snapshot Restore(FSR) 기능을 사용하면 스냅샷에서 EBS 볼륨을 빠르게 복원할 수 있습니다. 이는 복원 시간을 최소화하여 테스트 환경을 신속하게 설정할 수 있습니다.
  - **일관된 성능:** FSR을 사용하면 복원된 볼륨이 즉시 사용 가능하며, 일관된 높은 I/O 성능을 제공합니다. 이는 테스트 환경에서 소프트웨어가 요구하는 성능을 충족할 수 있습니다.
  - **독립된 데이터:** 스냅샷을 사용하여 새로운 EBS 볼륨을 생성하므로, 테스트 환경에서의 데이터 수정이 프로덕션 환경에 영향을 미치지 않습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. EC2 인스턴스 스토어 볼륨:** 인스턴스 스토어 볼륨은 일시적 스토리지로, 인스턴스가 중지되거나 종료되면 데이터가 손실됩니다. 또한, 스냅샷을 인스턴스 스토어 볼륨으로 복원하는 것은 적절하지 않습니다.
- **B. EBS Multi-Attach:** EBS Multi-Attach 기능은 여러 인스턴스에서 동일한 EBS 볼륨을 동시에 연결할 수 있지만, 이는 데이터 일관성 문제를 일으킬 수 있으며, 프로덕션 데이터에 영향을 미칠 수 있습니다.
- **C. 새로운 EBS 볼륨 초기화:** 새로운 EBS 볼륨을 생성하고 초기화한 후 스냅샷에서 복원하는 것은 시간이 많이 걸릴 수 있습니다. FSR을 사용하면 이 과정을 크게 단축할 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 프로덕션 데이터를 테스트 환경으로 신속하게 복제하는 방법을 평가합니다. 특히, EBS Fast Snapshot Restore 기능을 사용하여 복원 시간을 최소화하고 일관된 성능을 제공하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon EBS Snapshots:**
   - EBS 스냅샷의 기본 개념과 사용 사례
   - EBS 스냅샷을 사용하여 볼륨을 복원하는 방법

2. **EBS Fast Snapshot Restore (FSR):**
   - FSR의 기본 개념과 사용 사례
   - FSR을 사용하여 스냅샷에서 볼륨을 빠르게 복원하는 방법

3. **EBS Multi-Attach:**
   - Multi-Attach의 기본 개념과 사용 사례
   - Multi-Attach를 사용하여 여러 인스턴스에서 동일한 볼륨을 연결하는 방법과 한계

4. **EC2 Instance Store:**
   - 인스턴스 스토어의 기본 개념과 사용 사례
   - 인스턴스 스토어를 사용하여 데이터를 저장하는 방법과 한계

이 부분들을 공부하면, AWS에서 프로덕션 데이터를 테스트 환경으로 신속하게 복제하는 방법을 더 잘 이해할 수 있습니다.



# 25번
**정답: D. Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue.**

**해설:**
- **AWS Lambda와 SQS 통합:**
  - **비동기 처리:** SQS를 사용하면 데이터를 비동기적으로 처리할 수 있습니다. 첫 번째 Lambda 함수가 데이터를 수신하고 SQS 큐에 메시지를 넣으면, 두 번째 Lambda 함수가 큐에서 메시지를 읽어와 데이터를 데이터베이스에 로드합니다. 이는 시스템의 확장성을 높이고, 데이터 처리량이 증가할 때도 안정적으로 작동할 수 있습니다.
  - **확장성:** SQS는 자동으로 확장되어 높은 트래픽을 처리할 수 있습니다. Lambda 함수도 자동으로 확장되어 SQS 큐에서 메시지를 처리할 수 있습니다.
  - **구성 노력 최소화:** SQS와 Lambda를 사용하면 서버를 관리할 필요 없이 자동으로 확장되는 서버리스 아키텍처를 구현할 수 있습니다. 이는 구성 노력을 최소화합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Apache Tomcat과 EC2:** Lambda 함수를 EC2 인스턴스에서 실행되는 Tomcat 코드로 리팩토링하는 것은 운영 오버헤드가 크고, 서버 관리를 필요로 합니다. 이는 구성 노력을 증가시킵니다.
- **B. DynamoDB와 DAX:** Aurora PostgreSQL에서 DynamoDB로 플랫폼을 변경하는 것은 데이터베이스 구조와 API 호출 방식을 완전히 변경해야 하므로, 이는 큰 리팩토링 작업이 필요합니다. 또한, DynamoDB는 관계형 데이터베이스가 아니므로, 기존 데이터베이스 구조와 호환되지 않을 수 있습니다.
- **C. Lambda와 SNS 통합:** SNS는 주로 메시지 브로커 역할을 하며, SQS와 달리 메시지를 큐에 저장하지 않습니다. 이는 비동기 데이터 처리를 구현하는 데 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 높은 데이터 처리량을 효율적으로 처리하고, 구성 노력을 최소화하는 방법을 평가합니다. 특히, Lambda와 SQS를 사용하여 비동기 데이터 처리를 구현하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Lambda:**
   - Lambda의 기본 개념과 사용 사례
   - Lambda를 사용하여 서버리스 아키텍처를 구현하는 방법

2. **Amazon SQS:**
   - SQS의 기본 개념과 사용 사례
   - SQS를 사용하여 비동기 데이터 처리를 구현하는 방법

3. **Amazon SNS:**
   - SNS의 기본 개념과 사용 사례
   - SNS를 사용하여 메시지를 브로커하는 방법

4. **Amazon Aurora 및 DynamoDB:**
   - Aurora와 DynamoDB의 기본 개념과 사용 사례
   - 각 데이터베이스의 장단점과 사용 사례

이 부분들을 공부하면, AWS에서 높은 데이터 처리량을 효율적으로 처리하고, 구성 노력을 최소화하는 방법을 더 잘 이해할 수 있습니다.

# 30번
**정답: A. Stop the DB instance when tests are completed. Restart the DB instance when required.**

**해설:**
- **RDS 인스턴스 중지 및 시작:**
  - **비용 절감:** RDS 인스턴스를 중지하면 스토리지 비용만 발생하고, 인스턴스 사용 비용은 발생하지 않습니다. 이는 테스트가 끝난 후 인스턴스를 중지하고, 필요할 때 다시 시작함으로써 비용을 절감할 수 있습니다.
  - **구성 유지:** 인스턴스를 중지하고 다시 시작해도 기존의 컴퓨팅 및 메모리 속성은 유지됩니다. 이는 테스트 요구 사항을 충족하면서 비용을 절감할 수 있는 방법입니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. Auto Scaling 정책 사용:** RDS 인스턴스는 Auto Scaling을 지원하지 않습니다. 또한, Auto Scaling은 주로 EC2 인스턴스에 적용됩니다.
- **C. 스냅샷 생성 및 인스턴스 종료:** 스냅샷을 생성하고 인스턴스를 종료한 후 다시 복원하는 것은 시간이 많이 걸리고, 운영 오버헤드가 큽니다. 또한, 스냅샷 복원 시 데이터베이스 설정을 다시 구성해야 할 수 있습니다.
- **D. 인스턴스 용량 조정:** 인스턴스 용량을 낮추고 다시 높이는 것은 운영 오버헤드가 크며, 인스턴스 크기를 변경하는 동안 다운타임이 발생할 수 있습니다. 이는 테스트 요구 사항을 충족하지 못할 수 있습니다.

**출제 의도:**
이 문제는 AWS RDS 인스턴스를 사용하여 비용을 절감하면서도 성능 요구 사항을 충족하는 방법을 평가합니다. 특히, RDS 인스턴스를 중지하고 다시 시작하여 비용을 절감하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon RDS:**
   - RDS의 기본 개념과 사용 사례
   - RDS 인스턴스를 중지하고 다시 시작하는 방법

2. **비용 절감 전략:**
   - AWS에서 비용을 절감하는 다양한 전략
   - 인스턴스를 중지하고 다시 시작하여 비용을 절감하는 방법

이 부분들을 공부하면, AWS에서 RDS 인스턴스를 효율적으로 관리하고 비용을 절감하는 방법을 더 잘 이해할 수 있습니다.

# 33번
**정답: C. Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.**

**해설:**
- **Amazon Kinesis Data Streams:**
  - **실시간 데이터 스트리밍:** Kinesis Data Streams는 실시간으로 데이터를 스트리밍할 수 있는 서비스로, 수백만 건의 트랜잭션 데이터를 처리하는 데 적합합니다.
  - **확장성:** Kinesis Data Streams는 자동으로 확장되어 높은 트래픽을 처리할 수 있습니다.

- **AWS Lambda 통합:**
  - **데이터 처리:** Lambda 함수를 사용하여 Kinesis Data Streams에서 데이터를 읽고, 민감한 데이터를 제거한 후, 데이터를 DynamoDB에 저장할 수 있습니다. 이는 데이터 처리와 저장을 자동화하고, 민감한 데이터를 안전하게 제거할 수 있습니다.
  - **비용 효율성:** Lambda는 서버리스 아키텍처로, 사용한 만큼만 비용을 지불하므로 비용 효율적입니다.

- **Amazon DynamoDB:**
  - **저지연 데이터베이스:** DynamoDB는 저지연 데이터베이스로, 빠른 데이터 검색이 필요한 경우에 적합합니다.
  - **내구성 및 확장성:** DynamoDB는 높은 내구성과 확장성을 제공하여, 대규모 트랜잭션 데이터를 효율적으로 저장하고 검색할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. DynamoDB와 DynamoDB Streams:** DynamoDB는 데이터 저장에 적합하지만, 데이터 처리(민감한 데이터 제거)를 위한 규칙을 설정하는 기능이 부족합니다. 또한, DynamoDB Streams는 실시간 데이터 스트리밍에 적합하지 않습니다.
- **B. Kinesis Data Firehose:** Kinesis Data Firehose는 주로 데이터를 S3, Redshift, Elasticsearch로 전송하는 데 사용됩니다. DynamoDB와의 통합이 제한적이며, 실시간 데이터 처리에 적합하지 않습니다.
- **D. S3와 Lambda:** S3에 데이터를 배치로 저장하고 Lambda를 사용하여 처리하는 것은 실시간 데이터 처리에 적합하지 않습니다. 또한, S3는 저지연 데이터베이스로 사용하기에 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 실시간으로 대규모 트랜잭션 데이터를 처리하고, 민감한 데이터를 제거한 후, 저지연 데이터베이스에 저장하는 방법을 평가합니다. 특히, Kinesis Data Streams와 Lambda를 사용하여 실시간 데이터 처리를 구현하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon Kinesis Data Streams:**
   - Kinesis Data Streams의 기본 개념과 사용 사례
   - Kinesis Data Streams를 사용하여 실시간 데이터를 스트리밍하는 방법

2. **AWS Lambda:**
   - Lambda의 기본 개념과 사용 사례
   - Lambda를 사용하여 데이터를 처리하고, 다른 서비스와 통합하는 방법

3. **Amazon DynamoDB:**
   - DynamoDB의 기본 개념과 사용 사례
   - DynamoDB를 사용하여 데이터를 저장하고, 저지연으로 검색하는 방법

이 부분들을 공부하면, AWS에서 실시간으로 대규모 트랜잭션 데이터를 처리하고, 민감한 데이터를 제거한 후, 저지연 데이터베이스에 저장하는 방법을 더 잘 이해할 수 있습니다.

# 36번
**정답: B. Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application to use the KMS key with client-side encryption.**

**해설:**
- **Multi-Region KMS Key:**
  - **다중 리전 지원:** AWS KMS는 다중 리전 키를 지원하여, 동일한 키를 여러 리전에서 사용할 수 있습니다. 이를 통해 두 리전의 S3 버킷에서 동일한 KMS 키를 사용하여 데이터를 암호화하고 복호화할 수 있습니다.
  - **운영 오버헤드 최소화:** 다중 리전 KMS 키를 사용하면, 각 리전에 별도의 키를 생성하고 관리할 필요가 없으므로 운영 오버헤드가 줄어듭니다.

- **S3 버킷 및 복제:**
  - **S3 버킷 생성:** 각 리전에 S3 버킷을 생성하고, 데이터를 저장합니다.
  - **복제 구성:** S3 버킷 간의 복제를 구성하여, 한 리전에서 다른 리전으로 데이터를 자동으로 복제할 수 있습니다.

- **클라이언트 측 암호화:**
  - **클라이언트 측 암호화:** 애플리케이션에서 KMS 키를 사용하여 데이터를 클라이언트 측에서 암호화하고, 암호화된 데이터를 S3 버킷에 저장합니다. 이를 통해 데이터가 전송 중에도 보호됩니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. SSE-S3 사용:** SSE-S3는 Amazon S3 관리형 키를 사용하여 데이터를 암호화하지만, 고객 관리형 KMS 키를 사용하지 않습니다. 또한, 동일한 키를 여러 리전에서 사용할 수 없습니다.
- **C. SSE-S3 사용:** 이 옵션도 SSE-S3를 사용하여 데이터를 암호화하므로, 고객 관리형 KMS 키를 사용하지 않습니다.
- **D. 각 리전에 별도의 KMS 키 생성:** 각 리전에 별도의 KMS 키를 생성하면, 동일한 키를 여러 리전에서 사용할 수 없으므로 요구 사항을 충족하지 않습니다. 또한, 운영 오버헤드가 증가합니다.

**출제 의도:**
이 문제는 AWS에서 다중 리전 환경에서 데이터를 암호화하고, 동일한 KMS 키를 사용하여 데이터를 보호하는 방법을 평가합니다. 특히, 다중 리전 KMS 키를 사용하여 운영 오버헤드를 최소화하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS KMS:**
   - KMS의 기본 개념과 사용 사례
   - 다중 리전 KMS 키를 생성하고 사용하는 방법

2. **Amazon S3:**
   - S3의 기본 개념과 사용 사례
   - S3 버킷 간의 복제를 구성하는 방법

3. **클라이언트 측 암호화:**
   - 클라이언트 측 암호화의 기본 개념과 사용 사례
   - KMS 키를 사용하여 데이터를 클라이언트 측에서 암호화하는 방법

이 부분들을 공부하면, AWS에서 다중 리전 환경에서 데이터를 암호화하고, 동일한 KMS 키를 사용하여 데이터를 보호하는 방법을 더 잘 이해할 수 있습니다.

# 43번
**정답: B. Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.**

**해설:**
- **AWS Direct Connect:**
  - **전용 네트워크 연결:** AWS Direct Connect는 온프레미스 데이터 센터와 AWS 간의 전용 네트워크 연결을 제공합니다. 이는 인터넷을 거치지 않으므로, 인터넷 대역폭에 영향을 주지 않고 안정적이고 빠른 데이터 전송을 가능하게 합니다.
  - **대용량 데이터 전송:** Direct Connect는 대용량 데이터 전송에 적합하며, 시간에 민감한 데이터를 신속하게 백업할 수 있습니다. 이는 인터넷 대역폭 제한 문제를 해결할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. AWS VPN 연결:** VPN 연결은 인터넷을 통해 데이터를 전송하므로, 인터넷 대역폭 제한 문제를 해결하지 못합니다. 또한, 대용량 데이터 전송에 적합하지 않습니다.
- **C. AWS Snowball:** Snowball은 대용량 데이터를 물리적으로 전송하는 데 사용되지만, 매일 장치를 주문하고 데이터를 로드하여 반환하는 것은 비효율적이며, 장기적인 솔루션으로 적합하지 않습니다.
- **D. S3 서비스 한도 제거 요청:** S3 서비스 한도 제거는 데이터 전송 속도나 인터넷 대역폭 문제를 해결하지 못합니다. 이는 문제의 근본 원인을 해결하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 온프레미스 데이터 센터와 AWS 간의 대용량 데이터 전송을 효율적으로 관리하고, 인터넷 대역폭에 미치는 영향을 최소화하는 방법을 평가합니다. 특히, AWS Direct Connect를 사용하여 안정적이고 빠른 데이터 전송을 구현하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Direct Connect:**
   - Direct Connect의 기본 개념과 사용 사례
   - Direct Connect를 사용하여 온프레미스 데이터 센터와 AWS 간의 전용 네트워크 연결을 설정하는 방법

2. **AWS VPN:**
   - VPN의 기본 개념과 사용 사례
   - VPN을 사용하여 온프레미스 데이터 센터와 AWS 간의 연결을 설정하는 방법

3. **AWS Snowball:**
   - Snowball의 기본 개념과 사용 사례
   - Snowball을 사용하여 대용량 데이터를 물리적으로 전송하는 방법

이 부분들을 공부하면, AWS에서 온프레미스 데이터 센터와 AWS 간의 대용량 데이터 전송을 효율적으로 관리하고, 인터넷 대역폭에 미치는 영향을 최소화하는 방법을 더 잘 이해할 수 있습니다.

# 47번 
**정답: D. Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.**

**해설:**
- **On-Demand Capacity Reservation:**
  - **즉시 용량 확보:** On-Demand Capacity Reservation을 사용하면 특정 리전과 가용 영역에서 EC2 인스턴스 용량을 즉시 확보할 수 있습니다. 이는 특정 기간 동안 필요한 용량을 보장합니다.
  - **단기 이벤트:** Capacity Reservation은 단기 이벤트에 적합하며, 예약된 기간 동안 용량을 보장합니다. 이는 1주일 동안 지속되는 이벤트에 적합합니다.
  - **가용 영역 지정:** Capacity Reservation을 생성할 때 특정 가용 영역을 지정할 수 있으므로, 세 개의 특정 가용 영역에서 용량을 보장할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. 리전을 지정한 예약 인스턴스 구매:** 예약 인스턴스는 특정 리전에서 할인된 요금으로 인스턴스를 사용할 수 있게 하지만, 용량을 보장하지 않습니다.
- **B. 리전을 지정한 On-Demand Capacity Reservation:** 리전만 지정한 Capacity Reservation은 특정 가용 영역에서의 용량을 보장하지 않습니다.
- **C. 리전과 가용 영역을 지정한 예약 인스턴스 구매:** 예약 인스턴스는 특정 가용 영역에서 할인된 요금으로 인스턴스를 사용할 수 있게 하지만, 용량을 보장하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 특정 기간 동안 특정 가용 영역에서 EC2 인스턴스 용량을 보장하는 방법을 평가합니다. 특히, On-Demand Capacity Reservation을 사용하여 용량을 보장하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **On-Demand Capacity Reservation:**
   - Capacity Reservation의 기본 개념과 사용 사례
   - Capacity Reservation을 사용하여 특정 리전과 가용 영역에서 용량을 보장하는 방법

2. **Reserved Instances:**
   - 예약 인스턴스의 기본 개념과 사용 사례
   - 예약 인스턴스를 사용하여 비용을 절감하는 방법과 용량 보장의 차이점

이 부분들을 공부하면, AWS에서 특정 기간 동안 특정 가용 영역에서 EC2 인스턴스 용량을 보장하는 방법을 더 잘 이해할 수 있습니다.**정답: D. Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.**

**해설:**
- **On-Demand Capacity Reservation:**
  - **즉시 용량 확보:** On-Demand Capacity Reservation을 사용하면 특정 리전과 가용 영역에서 EC2 인스턴스 용량을 즉시 확보할 수 있습니다. 이는 특정 기간 동안 필요한 용량을 보장합니다.
  - **단기 이벤트:** Capacity Reservation은 단기 이벤트에 적합하며, 예약된 기간 동안 용량을 보장합니다. 이는 1주일 동안 지속되는 이벤트에 적합합니다.
  - **가용 영역 지정:** Capacity Reservation을 생성할 때 특정 가용 영역을 지정할 수 있으므로, 세 개의 특정 가용 영역에서 용량을 보장할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. 리전을 지정한 예약 인스턴스 구매:** 예약 인스턴스는 특정 리전에서 할인된 요금으로 인스턴스를 사용할 수 있게 하지만, 용량을 보장하지 않습니다.
- **B. 리전을 지정한 On-Demand Capacity Reservation:** 리전만 지정한 Capacity Reservation은 특정 가용 영역에서의 용량을 보장하지 않습니다.
- **C. 리전과 가용 영역을 지정한 예약 인스턴스 구매:** 예약 인스턴스는 특정 가용 영역에서 할인된 요금으로 인스턴스를 사용할 수 있게 하지만, 용량을 보장하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 특정 기간 동안 특정 가용 영역에서 EC2 인스턴스 용량을 보장하는 방법을 평가합니다. 특히, On-Demand Capacity Reservation을 사용하여 용량을 보장하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **On-Demand Capacity Reservation:**
   - Capacity Reservation의 기본 개념과 사용 사례
   - Capacity Reservation을 사용하여 특정 리전과 가용 영역에서 용량을 보장하는 방법

2. **Reserved Instances:**
   - 예약 인스턴스의 기본 개념과 사용 사례
   - 예약 인스턴스를 사용하여 비용을 절감하는 방법과 용량 보장의 차이점

이 부분들을 공부하면, AWS에서 특정 기간 동안 특정 가용 영역에서 EC2 인스턴스 용량을 보장하는 방법을 더 잘 이해할 수 있습니다.

# 48번
**정답: D. Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.**

**해설:**
- **Amazon Elastic File System (EFS):**
  - **고가용성 및 내구성:** Amazon EFS는 여러 가용 영역에 걸쳐 데이터를 저장하여 고가용성과 내구성을 제공합니다. 이는 데이터 손실 위험을 줄이고, 데이터가 항상 접근 가능하도록 보장합니다.
  - **확장성:** EFS는 자동으로 확장되므로, 데이터 양이 증가해도 별도의 관리 없이 확장할 수 있습니다.
  - **공유 파일 시스템:** EFS는 여러 EC2 인스턴스에서 동시에 접근할 수 있는 공유 파일 시스템을 제공하므로, 웹사이트의 카탈로그 데이터를 여러 인스턴스에서 사용할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. ElastiCache for Redis:** ElastiCache for Redis는 주로 캐싱을 위한 인메모리 데이터 저장소로, 내구성 있는 데이터 저장소로 사용하기에는 적합하지 않습니다.
- **B. 더 큰 인스턴스 스토어:** 더 큰 인스턴스 스토어를 사용하는 것은 내구성을 보장하지 않습니다. 인스턴스 스토어는 인스턴스가 종료되면 데이터가 손실될 수 있습니다.
- **C. S3 Glacier Deep Archive:** S3 Glacier Deep Archive는 장기 보관을 위한 저비용 스토리지로, 자주 접근해야 하는 데이터에는 적합하지 않습니다. 또한, 데이터 복구 시간이 길어질 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 데이터를 고가용성 및 내구성을 보장하는 위치에 저장하는 방법을 평가합니다. 특히, Amazon EFS를 사용하여 데이터를 안전하게 저장하고, 여러 인스턴스에서 접근할 수 있도록 하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon Elastic File System (EFS):**
   - EFS의 기본 개념과 사용 사례
   - EFS를 사용하여 데이터를 고가용성 및 내구성을 보장하는 방법

2. **Amazon ElastiCache:**
   - ElastiCache의 기본 개념과 사용 사례
   - ElastiCache를 사용하여 데이터를 캐싱하는 방법

3. **Amazon S3 Glacier Deep Archive:**
   - S3 Glacier Deep Archive의 기본 개념과 사용 사례
   - S3 Glacier Deep Archive를 

# 49번
**정답: B. Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.**

**해설:**
- **Amazon S3 Intelligent-Tiering:**
  - **비용 최적화:** S3 Intelligent-Tiering은 데이터 액세스 패턴에 따라 자동으로 데이터를 가장 비용 효율적인 스토리지 클래스로 이동시킵니다. 이는 자주 액세스되는 데이터와 드물게 액세스되는 데이터를 자동으로 구분하여 비용을 절감할 수 있습니다.
  - **빠른 액세스:** 1년 이내에 자주 액세스되는 파일은 S3 Intelligent-Tiering의 표준 또는 인빈티드 액세스 계층에 저장되어 빠르게 액세스할 수 있습니다.

- **S3 Glacier Flexible Retrieval:**
  - **장기 보관:** 1년 이후에 드물게 액세스되는 파일은 S3 Glacier Flexible Retrieval로 이동하여 비용을 절감할 수 있습니다. 이 스토리지 클래스는 저렴한 비용으로 데이터를 장기 보관할 수 있으며, 액세스 지연이 허용되는 경우에 적합합니다.

- **Amazon Athena 및 S3 Glacier Select:**
  - **데이터 쿼리:** Athena를 사용하여 S3에 저장된 데이터를 쿼리하고, S3 Glacier Select를 사용하여 Glacier에 저장된 데이터를 쿼리할 수 있습니다. 이는 데이터를 효율적으로 검색하고, 필요한 경우에만 데이터를 복원할 수 있게 합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. S3 Glacier Instant Retrieval:** S3 Glacier Instant Retrieval은 빠른 액세스를 제공하지만, 비용이 더 높을 수 있습니다. 또한, 1년 이내에 자주 액세스되는 데이터를 위한 최적의 솔루션이 아닙니다.
- **C. S3 Standard 및 S3 Glacier Instant Retrieval:** S3 Standard는 비용이 더 높을 수 있으며, S3 Glacier Instant Retrieval은 비용 효율적이지 않습니다. 또한, 메타데이터를 별도로 저장하고 쿼리하는 것은 복잡성을 증가시킵니다.
- **D. S3 Standard 및 S3 Glacier Deep Archive:** S3 Glacier Deep Archive는 매우 저렴한 비용으로 데이터를 장기 보관할 수 있지만, 데이터 복구 시간이 길어질 수 있습니다. 이는 1년 이내에 자주 액세스되는 데이터를 위한 최적의 솔루션이 아닙니다.

**출제 의도:**
이 문제는 AWS에서 데이터를 비용 효율적으로 저장하고, 액세스 패턴에 따라 최적화하는 방법을 평가합니다. 

# 50번
**정답: D. Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances.**

**해설:**
- **AWS Systems Manager Run Command:**
  - **즉시 실행:** Run Command를 사용하면 EC2 인스턴스에 명령을 즉시 실행할 수 있습니다. 이는 긴급한 패치 작업을 빠르게 수행하는 데 적합합니다.
  - **대규모 인스턴스 관리:** Run Command는 여러 인스턴스에 동시에 명령을 실행할 수 있으므로, 1,000개의 EC2 인스턴스에 패치를 빠르게 적용할 수 있습니다.
  - **커스텀 명령:** Run Command를 사용하여 특정 패치 명령을 정의하고 실행할 수 있습니다. 이는 특정 소프트웨어 패치를 적용하는 데 유연성을 제공합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. AWS Lambda 함수 생성:** Lambda 함수는 짧은 실행 시간과 제한된 리소스를 가지므로, 1,000개의 EC2 인스턴스에 패치를 적용하는 데 적합하지 않습니다.
- **B. AWS Systems Manager Patch Manager 구성:** Patch Manager는 주로 운영 체제 패치를 관리하는 데 사용됩니다. 특정 서드파티 소프트웨어 패치를 적용하는 데는 적합하지 않을 수 있습니다.
- **C. AWS Systems Manager 유지 관리 창 스케줄링:** 유지 관리 창을 사용하면 패치를 예약할 수 있지만, 긴급한 패치 작업을 즉시 수행하는 데는 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 대규모 EC2 인스턴스에 긴급한 패치를 빠르게 적용하는 방법을 평가합니다. 특히, AWS Systems Manager Run Command를 사용하여 커스텀 명령을 실행하고, 패치를 빠르게 적용하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Systems Manager Run Command:**
   - Run Command의 기본 개념과 사용 사례
   - Run Command를 사용하여 여러 인스턴스에 명령을 실행하는 방법

2. **AWS Systems Manager Patch Manager:**
   - Patch Manager의 기본 개념과 사용 사례
   - Patch Manager를 사용하여 운영 체제 패치를 관리하는 방법

3. **AWS Lambda:**
   - Lambda의 기본 개념과 사용 사례
   - Lambda를 사용하여 짧은 실행 시간의 작업을 수행하는 방법

4. **AWS Systems Manager Maintenance Window:**
   - Maintenance Window의 기본 개념과 사용 사례
   - Maintenance Window를 사용하여 유지 관리 작업을 예약하는 방법

이 부분들을 공부하면, AWS에서 대규모 EC2 인스턴스에 긴급한 패치를 빠르게 적용하는 방법을 더 잘 이해할 수 있습니다.

# 57번
**정답: B. Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence predictions.**

**해설:**
- **Amazon Rekognition:**
  - **이미지 분석:** Amazon Rekognition은 이미지와 비디오에서 객체, 사람, 텍스트, 장면 및 활동을 식별할 수 있는 완전 관리형 서비스입니다. 특히, 부적절한 콘텐츠를 감지하는 데 사용될 수 있는 내장된 기능을 제공합니다.
  - **개발 노력 최소화:** Rekognition은 사전 훈련된 모델을 제공하므로, 별도의 모델을 훈련하거나 배포할 필요가 없습니다. 이는 개발 노력을 최소화합니다.
  - **인간 검토:** Rekognition은 낮은 신뢰도의 예측에 대해 인간 검토를 추가할 수 있는 기능을 제공합니다. 이는 부적절한 콘텐츠를 더 정확하게 필터링하는 데 도움이 됩니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Amazon Comprehend:** Comprehend는 주로 텍스트 분석을 위한 서비스로, 이미지에서 부적절한 콘텐츠를 감지하는 데 적합하지 않습니다.
- **C. Amazon SageMaker:** SageMaker는 맞춤형 머신 러닝 모델을 훈련하고 배포하는 데 사용됩니다. 이는 개발 노력이 많이 들며, 사전 훈련된 모델을 제공하지 않으므로 부적절한 콘텐츠 감지에 적합하지 않습니다.
- **D. AWS Fargate:** Fargate는 컨테이너를 배포하고 관리하는 데 사용됩니다. 맞춤형 머신 러닝 모델을 배포하려면 많은 개발 노력이 필요하며, 사전 훈련된 모델을 제공하지 않으므로 부적절한 콘텐츠 감지에 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 부적절한 콘텐츠를 감지하고, 개발 노력을 최소화하는 방법을 평가합니다. 특히, Amazon Rekognition을 사용하여 이미지에서 부적절한 콘텐츠를 감지하고, 낮은 신뢰도의 예측에 대해 인간 검토를 추가하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon Rekognition:**
   - Rekognition의 기본 개념과 사용 사례
   - Rekognition을 사용하여 이미지에서 부적절한 콘텐츠를 감지하는 방법

2. **Amazon Comprehend:**
   - Comprehend의 기본 개념과 사용 사례
   - Comprehend를 사용하여 텍스트를 분석하는 방법

3. **Amazon SageMaker:**
   - SageMaker의 기본 개념과 사용 사례
   - SageMaker를 사용하여 맞춤형 머신 러닝 모델을 훈련하고 배포하는 방법

4. **AWS Fargate:**
   - Fargate의 기본 개념과 사용 사례
   - Fargate를 사용하여 컨테이너를 배포하고 관리하는 방법

이 부분들을 공부하면, AWS에서 부적절한 콘텐츠를 감지하고, 개발 노력을 최소화하는 방법을 더 잘 이해할 수 있습니다.

**정답: C. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.**

**해설:**
- **S3 Standard-Infrequent Access (S3 Standard-IA):**
  - **비용 효율성:** S3 Standard-IA는 자주 접근하지 않는 데이터를 저장하는 데 적합하며, S3 Standard보다 저렴한 비용으로 데이터를 저장할 수 있습니다.
  - **즉시 접근 가능:** S3 Standard-IA는 데이터를 즉시 접근할 수 있도록 보장하므로, 중요한 비즈니스 데이터를 저장하는 데 적합합니다.
  - **사용 패턴:** 문제에서 주어진 사용 패턴(처음 30일 동안 자주 접근, 이후 드물게 접근)에 적합합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. S3 Glacier:** S3 Glacier는 장기 보관을 위한 저비용 스토리지로, 데이터 접근 시간이 길어질 수 있습니다. 즉시 접근이 필요한 데이터에는 적합하지 않습니다.
- **B. S3 One Zone-IA:** S3 One Zone-IA는 단일 가용 영역에 데이터를 저장하므로, 내구성이 낮아질 수 있습니다. 중요한 비즈니스 데이터를 저장하는 데 적합하지 않습니다.
- **D. S3 Standard-IA에서 S3 Glacier로 이동:** S3 Glacier로 이동하면 데이터 접근 시간이 길어질 수 있습니다. 즉시 접근이 필요한 데이터에는 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 데이터를 비용 효율적으로 저장하고, 즉시 접근 가능성을 보장하는 방법을 평가합니다. 특히, S3 Standard-IA를 사용하여 자주 접근하지 않는 데이터를 저장하고, S3 Lifecycle 정책을 사용하여 데이터를 관리하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon S3 Storage Classes:**
   - S3 Standard, S3 Standard-IA, S3 One Zone-IA, S3 Glacier의 기본 개념과 사용 사례
   - 각 스토리지 클래스의 비용 및 접근 시간

2. **S3 Lifecycle Policies:**
   - S3 Lifecycle 정책의 기본 개념과 사용 사례
   - S3 Lifecycle 정책을 사용하여 데이터를 자동으로 이동하고 삭제하는 방법

이 부분들을 공부하면, AWS에서 데이터를 비용 효율적으로 저장하고, 즉시 접근 가능성을 보장하는 방법을 더 잘 이해할 수 있습니다.

# 68번
**정답: A. Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.**

**해설:**
- **AWS Direct Connect:**
  - **고가용성 및 저지연:** Direct Connect는 AWS와 온프레미스 데이터 센터 간의 전용 네트워크 연결을 제공하여, 일관된 저지연과 높은 가용성을 보장합니다.
  - **비용 효율성:** Direct Connect는 대규모 데이터 전송에 대해 비용 효율적입니다.

- **VPN 연결 백업:**
  - **비용 절감:** VPN 연결은 Direct Connect에 비해 저렴하며, 백업 연결로 사용하기에 적합합니다.
  - **자동 페일오버:** Direct Connect 연결이 실패할 경우, VPN 연결을 통해 트래픽을 전송할 수 있습니다. 이는 트래픽이 느려질 수 있지만, 비용을 절감하면서 고가용성을 유지할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. 두 개의 VPN 터널:** 두 개의 VPN 터널을 사용하는 것은 비용이 저렴하지만, 일관된 저지연을 제공하지 못합니다. 이는 고가용성과 저지연 요구 사항을 충족하지 못합니다.
- **C. 두 개의 Direct Connect 연결:** 두 개의 Direct Connect 연결을 사용하는 것은 고가용성을 보장하지만, 비용이 많이 듭니다. 비용을 최소화하려는 요구 사항을 충족하지 못합니다.
- **D. Direct Connect 페일오버 속성:** Direct Connect 페일오버 속성은 자동 백업 연결을 생성하지 않습니다. 백업 연결을 수동으로 설정해야 합니다.

**출제 의도:**
이 문제는 AWS에서 온프레미스 인프라를 확장하고, 고가용성과 저지연을 보장하면서 비용을 최소화하는 방법을 평가합니다. 특히, Direct Connect와 VPN 연결을 조합하여 고가용성과 비용 효율성을 유지하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Direct Connect:**
   - Direct Connect의 기본 개념과 사용 사례
   - Direct Connect를 사용하여 고가용성과 저지연을 보장하는 방법

2. **AWS VPN:**
   - VPN의 기본 개념과 사용 사례
   - VPN을 백업 연결로 사용하는 방법

3. **하이브리드 아키텍처:**
   - 온프레미스 인프라와 AWS 간의 하이브리드 아키텍처 설계 방법
   - 고가용성과 비용 효율성을 유지하는 방법

이 부분들을 공부하면, AWS에서 온프레미스 인프라를 확장하고, 고가용성과 저지연을 보장하면서 비용을 최소화하는 방법을 더 잘 이해할 수 있습니다.

# 72번
**정답: D. Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets.**

**해설:**
- **S3 VPC Gateway Endpoint:**
  - **내부 네트워크 트래픽:** S3 VPC Gateway Endpoint를 사용하면 VPC 내에서 Amazon S3에 직접 연결할 수 있습니다. 이는 인터넷을 통해 트래픽을 라우팅할 필요가 없으므로 데이터 전송 비용을 줄일 수 있습니다.
  - **비용 절감:** VPC Endpoint를 사용하면 인터넷 게이트웨이, NAT 게이트웨이 또는 다른 네트워크 장치를 통해 트래픽을 라우팅할 필요가 없으므로 데이터 전송 비용을 절감할 수 있습니다.
  - **보안:** VPC Endpoint를 사용하면 트래픽이 AWS 네트워크 내에서 유지되므로 보안이 강화됩니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. API Gateway 사용:** API Gateway는 주로 API 관리를 위해 사용되며, S3와의 데이터 전송 비용을 줄이는 데 적합하지 않습니다.
- **B. NAT Gateway 사용:** NAT Gateway를 사용하면 인터넷을 통해 트래픽이 라우팅되므로 데이터 전송 비용이 발생합니다.
- **C. 인터넷 게이트웨이 사용:** 인터넷 게이트웨이를 통해 S3에 접근하면 데이터 전송 비용이 발생합니다.

**출제 의도:**
이 문제는 AWS에서 데이터 전송 비용을 줄이기 위해 S3 VPC Gateway Endpoint를 사용하는 방법을 평가합니다. 특히, VPC Endpoint를 사용하여 VPC 내에서 S3에 직접 연결하고, 데이터 전송 비용을 절감하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon S3 VPC Gateway Endpoint:**
   - S3 VPC Gateway Endpoint의 기본 개념과 사용 사례
   - VPC Endpoint를 사용하여 데이터 전송 비용을 절감하는 방법

2. **AWS 네트워킹:**
   - VPC, 서브넷, 인터넷 게이트웨이, NAT 게이트웨이의 기본 개념과 사용 사례
   - 각 네트워킹 구성 요소의 비용 및 보안 고려 사항

이 부분들을 공부하면, AWS에서 데이터 전송 비용을 줄이기 위해 S3 VPC Gateway Endpoint를 사용하는 방법을 더 잘 이해할 수 있습니다.


# 82번
**정답: B. Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure Amazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon Simple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource.**

**해설:**
- **AWS Config Rule:**
  - **자동화된 규칙 검사:** AWS Config는 리소스 구성을 지속적으로 평가하고, 규칙을 기반으로 비준수 리소스를 식별할 수 있습니다. 이를 통해 인증서 만료를 자동으로 감지할 수 있습니다.
  - **30일 이내 만료 확인:** AWS Config 규칙을 사용하여 30일 이내에 만료되는 인증서를 확인할 수 있습니다.

- **Amazon EventBridge (CloudWatch Events):**
  - **이벤트 기반 알림:** EventBridge를 사용하여 AWS Config에서 비준수 리소스를 감지할 때마다 이벤트를 트리거할 수 있습니다. 이를 통해 자동으로 알림을 보낼 수 있습니다.
  - **SNS 통합:** EventBridge 이벤트를 Amazon SNS를 통해 알림으로 전송할 수 있습니다. 이를 통해 보안 팀에 인증서 만료 알림을 보낼 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. ACM 규칙 추가:** ACM 자체에는 인증서 만료 알림을 위한 규칙을 설정하는 기능이 없습니다.
- **C. Trusted Advisor 사용:** Trusted Advisor는 인증서 만료를 확인할 수 있지만, AWS Config와 EventBridge를 사용하는 것이 더 자동화되고 효율적입니다.
- **D. EventBridge와 Lambda 사용:** Lambda 함수를 사용하여 알림을 보내는 것은 가능하지만, AWS Config 규칙을 사용하는 것이 더 간단하고 운영 오버헤드가 적습니다.

**출제 의도:**
이 문제는 AWS에서 인증서 만료를 자동으로 감지하고, 알림을 보내는 방법을 평가합니다. 특히, AWS Config와 EventBridge를 사용하여 인증서 만료를 감지하고, SNS를 통해 알림을 보내는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Config:**
   - AWS Config의 기본 개념과 사용 사례
   - AWS Config 규칙을 설정하여 리소스 구성을 평가하는 방법

2. **Amazon EventBridge (CloudWatch Events):**
   - EventBridge의 기본 개념과 사용 사례
   - EventBridge를 사용하여 이벤트 기반 알림을 설정하는 방법

3. **Amazon SNS:**
   - SNS의 기본 개념과 사용 사례
   - SNS를 사용하여 알림을 전송하는 방법

이 부분들을 공부하면, AWS에서 인증서 만료를 자동으로 감지하고, 알림을 보내는 방법을 더 잘 이해할 수 있습니다.

# 93번
**정답: B. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.**

**해설:**
- **Amazon Aurora MySQL:**
  - **고가용성 및 확장성:** Aurora MySQL은 고가용성과 확장성을 제공하며, Multi-AZ Aurora Replicas를 사용하여 읽기 성능을 향상시킬 수 있습니다.
  - **Aurora Replicas:** Aurora Replicas는 읽기 작업을 분산시켜 데이터베이스의 부하를 줄이고 성능을 향상시킵니다.

- **Database Cloning:**
  - **빠른 복제:** 데이터베이스 클로닝을 사용하면 전체 데이터베이스를 빠르게 복제할 수 있습니다. 이는 스테이징 환경을 신속하게 설정할 수 있도록 합니다.
  - **운영 오버헤드 최소화:** 클로닝은 백업 및 복원 과정보다 훨씬 빠르고 효율적이며, 운영 오버헤드를 최소화합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. mysqldump 사용:** mysqldump를 사용한 백업 및 복원 과정은 시간이 많이 걸리며, 여전히 애플리케이션 지연 문제를 해결하지 못할 수 있습니다.
- **C. 스탠바이 인스턴스 사용:** 스탠바이 인스턴스는 장애 복구를 위해 사용되며, 스테이징 데이터베이스로 사용하는 것은 적절하지 않습니다.
- **D. mysqldump 사용:** mysqldump를 사용한 백업 및 복원 과정은 시간이 많이 걸리며, 여전히 애플리케이션 지연 문제를 해결하지 못할 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 MySQL 데이터베이스를 고가용성과 확장성을 갖춘 아키텍처로 마이그레이션하고, 애플리케이션 지연 문제를 해결하는 방법을 평가합니다. 특히, Amazon Aurora MySQL과 데이터베이스 클로닝을 사용하여 스테이징 환경을 신속하게 설정하고, 운영 오버헤드를 최소화하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon Aurora MySQL:**
   - Aurora MySQL의 기본 개념과 사용 사례
   - Multi-AZ Aurora Replicas를 사용하여 읽기 성능을 향상시키는 방법

2. **Database Cloning:**
   - 데이터베이스 클로닝의 기본 개념과 사용 사례
   - 클로닝을 사용하여 스테이징 환경을 신속하게 설정하는 방법

이 부분들을 공부하면, AWS에서 MySQL 데이터베이스를 고가용성과 확장성을 갖춘 아키텍처로 마이그레이션하고, 애플리케이션 지연 문제를 해결하는 방법을 더 잘 이해할 수 있습니다.

# 95번
- Creating read replicas allows the application to offload read traffic from the source database, improving its performance. The read replicas should be configured with the same compute and storage resources as the source database to ensure that they can handle the read workload effectively.

- 오프로딩(offloading): 컴퓨팅 자원 및 계산 속도의 한계를 극복하기 위해 로컬 컴퓨터에서 수행하는 어플리케이션 의 일부를 컴퓨팅 자원과 처리능력이 우수한 원격지 컴퓨터에 전달하여 처리한 후 결과를 반환받는 방식

### 정답
D. Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database.

### 해설
애플리케이션 성능 저하 문제를 해결하기 위해 읽기 트래픽과 쓰기 트래픽을 분리하는 것이 필요합니다. Amazon RDS MySQL에서 읽기 복제본을 생성하면 읽기 작업을 복제본으로 분산시켜 기본 데이터베이스의 부하를 줄일 수 있습니다. 읽기 복제본을 원본 데이터베이스와 동일한 컴퓨팅 및 스토리지 리소스로 구성하면 성능을 최적화할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary Availability Zone.**
  - Multi-AZ 배포는 고가용성을 제공하지만, 읽기 트래픽을 분리하는 데는 적합하지 않습니다. 기본 가용 영역에서 읽기 요청을 처리하면 성능 저하 문제가 해결되지 않습니다.

- **B. Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary Availability Zone.**
  - Multi-AZ 배포의 보조 가용 영역은 장애 조치 용도로만 사용되며, 읽기 요청을 처리할 수 없습니다.

- **C. Create read replicas for the database. Configure the read replicas with half of the compute and storage resources as the source database.**
  - 읽기 복제본을 생성하는 것은 적절한 접근 방식이지만, 원본 데이터베이스의 절반 리소스로 구성하면 성능이 저하될 수 있습니다.

- **D. Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database.**
  - 읽기 복제본을 생성하고 원본 데이터베이스와 동일한 컴퓨팅 및 스토리지 리소스로 구성하면 성능을 최적화할 수 있습니다. 이는 읽기 트래픽을 분리하고 애플리케이션 성능을 빠르게 최적화하는 데 가장 적합한 솔루션입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **Amazon RDS 읽기 복제본**
   - 읽기 복제본은 데이터베이스의 읽기 작업을 분산시켜 성능을 최적화할 수 있습니다.
   - 원본 데이터베이스와 동일한 컴퓨팅 및 스토리지 리소스로 구성하면 성능을 최적화할 수 있습니다.

2. **Amazon RDS Multi-AZ 배포**
   - Multi-AZ 배포는 고가용성을 제공하지만, 읽기 트래픽을 분리하는 데는 적합하지 않습니다.
   - 보조 가용 영역은 장애 조치 용도로만 사용됩니다.

### 추가 학습 자료
- [Amazon RDS 읽기 복제본 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_ReadRepl.html)
- [Amazon RDS Multi-AZ 배포 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html)

이 자료들을 통해 읽기 복제본과 Multi-AZ 배포의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 96번
### 정답
C. Users can terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.

### 해설
이 정책은 두 가지 주요 조건을 포함하고 있습니다:

1. **Allow 조건**: 특정 IP 주소 범위(10.100.100.0/24)에서 `ec2:TerminateInstances` 작업을 허용합니다.
2. **Deny 조건**: `ec2:*` 작업을 us-east-1 리전이 아닌 모든 리전에서 거부합니다.

이 정책의 효과를 분석해보면:

- 첫 번째 조건은 특정 IP 주소 범위(10.100.100.0/24)에서 `ec2:TerminateInstances` 작업을 허용합니다. 예를 들어, IP 주소가 10.100.100.254인 사용자는 이 작업을 수행할 수 있습니다.
- 두 번째 조건은 us-east-1 리전이 아닌 모든 리전에서 `ec2:*` 작업을 거부합니다. 즉, us-east-1 리전에서는 이 거부 조건이 적용되지 않습니다.

따라서, 사용자가 us-east-1 리전에서 특정 IP 주소 범위(10.100.100.0/24) 내에서 `ec2:TerminateInstances` 작업을 수행할 수 있습니다. 이는 옵션 C와 일치합니다.

### 필요한 학습 내용

1. **IAM 정책 조건**
   - IAM 정책에서 조건을 사용하여 특정 상황에서만 권한을 허용하거나 거부할 수 있습니다.
   - [`IpAddress`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22file%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fe%3A%2FStudy%2FTheory-oriented%2FAmazon%2FExamTopics%20SAA-C03%20%EB%AC%B8%EC%A0%9C%2F91-100.md%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22%22%7D%2C%22pos%22%3A%7B%22line%22%3A60%2C%22character%22%3A17%7D%7D%5D%2C%22f7ec7572-e7bb-495a-bf8e-45e56902ff28%22%5D "Go to definition") 조건은 특정 IP 주소 범위에서만 작업을 허용하거나 거부할 수 있습니다.
   - [`StringNotEquals`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22file%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fe%3A%2FStudy%2FTheory-oriented%2FAmazon%2FExamTopics%20SAA-C03%20%EB%AC%B8%EC%A0%9C%2F91-100.md%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22%22%7D%2C%22pos%22%3A%7B%22line%22%3A70%2C%22character%22%3A17%7D%7D%5D%2C%22f7ec7572-e7bb-495a-bf8e-45e56902ff28%22%5D "Go to definition") 조건은 특정 문자열 값이 일치하지 않을 때 작업을 허용하거나 거부할 수 있습니다.

2. **IAM 정책의 허용 및 거부**
   - IAM 정책에서 명시적으로 허용된 작업은 조건이 충족될 때만 허용됩니다.
   - 명시적으로 거부된 작업은 조건이 충족될 때 항상 거부됩니다.

### 추가 학습 자료
- [IAM 정책 조건 공식 문서](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html)
- [IAM 정책의 허용 및 거부 공식 문서](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html)

이 자료들을 통해 IAM 정책 조건과 허용 및 거부의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 98번
**정답: B. Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.**

**해설:**
- **SQS FIFO Queue:**
  - **메시지 중복 제거:** SQS FIFO 큐는 메시지 중복 제거 기능을 제공하여 동일한 메시지가 여러 번 처리되는 것을 방지할 수 있습니다. 메시지 중복 제거 ID를 사용하면 동일한 메시지가 여러 번 처리되지 않도록 할 수 있습니다.
  - **순서 보장:** FIFO 큐는 메시지의 순서를 보장하므로, 메시지가 올바른 순서로 처리됩니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Long Polling:** Long Polling은 메시지를 더 효율적으로 수신하는 데 도움이 되지만, 메시지 중복 문제를 해결하지는 않습니다.
- **C. Visibility Timeout 증가:** Visibility Timeout을 증가시키면 메시지가 다시 처리되는 것을 방지할 수 있지만, 이는 메시지 처리 시간이 길어질 수 있는 경우에만 유효합니다. 또한, 이는 메시지 중복 문제를 완전히 해결하지 못할 수 있습니다.
- **D. Lambda 함수 수정:** Lambda 함수에서 메시지를 읽은 후 즉시 삭제하는 것은 메시지가 성공적으로 처리되었는지 확인하기 전에 삭제하는 것이므로, 데이터 손실의 위험이 있습니다.

**출제 의도:**
이 문제는 AWS에서 SQS와 Lambda를 사용하여 이벤트 기반 처리를 구현할 때 메시지 중복 문제를 해결하는 방법을 평가합니다. 특히, SQS FIFO 큐를 사용하여 메시지 중복 제거 기능을 활용하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon SQS FIFO Queue:**
   - FIFO 큐의 기본 개념과 사용 사례
   - 메시지 중복 제거 ID를 사용하여 메시지 중복을 방지하는 방법

2. **Visibility Timeout:**
   - Visibility Timeout의 기본 개념과 사용 사례
   - Visibility Timeout을 적절히 설정하여 메시지 중복 처리를 방지하는 방법

이 부분들을 공부하면, AWS에서 SQS와 Lambda를 사용하여 이벤트 기반 처리를 구현할 때 메시지 중복 문제를 해결하는 방법을 더 잘 이해할 수 있습니다.

# 102번
**정답: B. Install an AWS DataSync agent in the on-premises data center. E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.**

**해설:**
- **AWS DataSync:**
  - **데이터 전송 자동화:** DataSync는 온프레미스 데이터 센터와 AWS 간의 데이터 전송을 자동화하고 가속화하는 서비스입니다. 이를 통해 200GB의 데이터를 효율적으로 전송할 수 있습니다.
  - **NFS 지원:** DataSync는 NFS 기반 파일 시스템을 지원하므로, 온프레미스 SFTP 서버의 데이터를 EFS로 쉽게 전송할 수 있습니다.

- **B. DataSync 에이전트 설치:**
  - **온프레미스 데이터 센터:** DataSync 에이전트를 온프레미스 데이터 센터에 설치하여, 데이터를 AWS로 전송할 수 있습니다.

- **E. 위치 구성:**
  - **적절한 위치 구성:** DataSync를 사용하여 온프레미스 SFTP 서버의 위치 구성을 생성하고, 데이터를 EFS로 전송할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. 동일한 가용 영역:** EC2 인스턴스를 EFS 파일 시스템과 동일한 가용 영역에 배치하는 것은 성능에 도움이 될 수 있지만, 데이터 전송을 자동화하는 데는 직접적인 관련이 없습니다.
- **C. EBS 볼륨 생성:** EBS 볼륨을 생성하는 것은 EFS를 사용하는 요구 사항과 맞지 않습니다.
- **D. 수동 복사:** 수동으로 데이터를 복사하는 것은 자동화된 솔루션이 아니며, 운영 오버헤드가 큽니다.

**출제 의도:**
이 문제는 AWS에서 온프레미스 데이터 센터의 데이터를 자동으로 AWS로 전송하는 방법을 평가합니다. 특히, AWS DataSync를 사용하여 데이터를 효율적으로 전송하고, 운영 오버헤드를 최소화하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS DataSync:**
   - DataSync의 기본 개념과 사용 사례
   - DataSync를 사용하여 온프레미스 데이터 센터와 AWS 간의 데이터를 전송하는 방법

2. **Amazon EFS:**
   - EFS의 기본 개념과 사용 사례
   - EFS를 사용하여 데이터를 저장하고, EC2 인스턴스와 통합하는 방법

이 부분들을 공부하면, AWS에서 온프레미스 데이터 센터의 데이터를 자동으로 AWS로 전송하는 방법을 더 잘 이해할 수 있습니다.

# 104번
- B: GuardDuty는 공격자를 블락할 수 없으며, 데이터 소스나 이벤트 로그를 모니터링하고, 멀웨어 탐지 및 결과를 생성할 뿐임


# 107번
### 정답
D. Use Amazon API Gateway with Amazon Kinesis Data Analytics.

### 해설
자전거 공유 회사는 자전거 위치 데이터를 추적하고, 이 데이터를 기존 분석 플랫폼에서 사용할 수 있도록 해야 합니다. 또한, 데이터는 REST API를 통해 접근 가능해야 합니다. Amazon Kinesis Data Analytics는 실시간 데이터 스트리밍 분석을 제공하며, Amazon API Gateway는 REST API를 통해 데이터를 노출할 수 있습니다. 이 조합은 실시간 위치 데이터를 효율적으로 수집하고 분석하는 데 적합합니다.

각 옵션을 자세히 살펴보면:

- **A. Use Amazon Athena with Amazon S3.**
  - Amazon Athena는 S3에 저장된 데이터를 쿼리하는 데 사용되며, 실시간 데이터 스트리밍 분석에는 적합하지 않습니다.

- **B. Use Amazon API Gateway with AWS Lambda.**
  - API Gateway와 Lambda는 REST API를 제공하고 서버리스 컴퓨팅을 제공하지만, 실시간 데이터 스트리밍 분석에는 적합하지 않습니다.

- **C. Use Amazon QuickSight with Amazon Redshift.**
  - QuickSight는 데이터 시각화 도구이고, Redshift는 데이터 웨어하우징 솔루션입니다. 실시간 데이터 스트리밍 분석에는 적합하지 않습니다.

- **D. Use Amazon API Gateway with Amazon Kinesis Data Analytics.**
  - API Gateway는 REST API를 제공하고, Kinesis Data Analytics는 실시간 데이터 스트리밍 분석을 제공합니다. 이 조합은 실시간 위치 데이터를 효율적으로 수집하고 분석하는 데 적합합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **Amazon Kinesis Data Analytics**
   - Kinesis Data Analytics는 실시간 데이터 스트리밍 분석을 제공하는 서비스입니다.
   - 실시간 위치 데이터를 효율적으로 수집하고 분석할 수 있습니다.

2. **Amazon API Gateway**
   - API Gateway는 REST API를 제공하는 서비스입니다.
   - 데이터를 노출하고 접근할 수 있는 인터페이스를 제공합니다.

3. **Amazon Athena**
   - Athena는 S3에 저장된 데이터를 쿼리하는 서비스입니다.
   - 실시간 데이터 스트리밍 분석에는 적합하지 않습니다.

4. **AWS Lambda**
   - Lambda는 서버리스 컴퓨팅을 제공하는 서비스입니다.
   - 실시간 데이터 스트리밍 분석에는 적합하지 않습니다.

5. **Amazon QuickSight**
   - QuickSight는 데이터 시각화 도구입니다.
   - 실시간 데이터 스트리밍 분석에는 적합하지 않습니다.

6. **Amazon Redshift**
   - Redshift는 데이터 웨어하우징 솔루션입니다.
   - 실시간 데이터 스트리밍 분석에는 적합하지 않습니다.

### 추가 학습 자료
- [Amazon Kinesis Data Analytics 공식 문서](https://docs.aws.amazon.com/ko_kr/kinesisanalytics/latest/dev/what-is.html)
- [Amazon API Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/apigateway/latest/developerguide/welcome.html)
- [Amazon Athena 공식 문서](https://docs.aws.amazon.com/ko_kr/athena/latest/ug/what-is.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)
- [Amazon QuickSight 공식 문서](https://docs.aws.amazon.com/ko_kr/quicksight/latest/user/welcome.html)
- [Amazon Redshift 공식 문서](https://docs.aws.amazon.com/ko_kr/redshift/latest/mgmt/welcome.html)

이 자료들을 통해 Kinesis Data Analytics, API Gateway, Athena, Lambda, QuickSight, Redshift의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 108번
- Amazon SNS는 Pub/Sub의 브로드캐스트 모델이고, Amazon SQS 큐잉 모델
- 브로드캐스트 모델은 단순히 전달만 할 뿐, 정상적으로 전달되었는지 여부는 상관하지 않음
- 반면 큐잉 모델은 데이터가 전달되어야 하는 정상 상태의 처리자가 반드시 필요하며, 데이터를 받을 처리자가 없으면 해당 데이터는 큐에 보관됨
- 이 문제에서 회사는 Multiple target systems를 보유하므로, SNS를 통해 갱신 정보를 브로드캐스팅하여 각각의 시스템이 개별적으로 람다를 실행시키는 것이 효과적임
- A의 문제점은 단일 SQS를 사용하는 점인데, 하나의 SQS는 곧 여러 시스템에 의한 중복 소비를 발생시키는데 이를 방지하기 위한 처리가 상당히 까다롭고 복잡함

### 정답
D. Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.

### 해설
자동차 판매 웹사이트에서 자동차가 판매될 때, 해당 목록을 웹사이트에서 제거하고 데이터를 여러 대상 시스템으로 전송해야 합니다. 이를 위해 RDS 이벤트 알림을 구독하고, Amazon SNS 주제를 사용하여 여러 Amazon SQS 큐로 데이터를 팬아웃(fan-out)하는 것이 가장 적합한 솔루션입니다. 각 SQS 큐는 대상 시스템으로 데이터를 전송하는 AWS Lambda 함수를 트리거할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to consume.**
  - Lambda 함수를 사용하여 RDS 업데이트 시 SQS 큐로 데이터를 전송하는 것은 가능하지만, 여러 대상 시스템으로 데이터를 팬아웃하는 기능이 부족합니다.

- **B. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue for the targets to consume.**
  - FIFO 큐를 사용하는 것은 순서 보장이 필요한 경우에 적합하지만, 이 시나리오에서는 필요하지 않습니다. 또한, 팬아웃 기능이 부족합니다.

- **C. Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) queue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the targets.**
  - SQS 큐에서 SNS 주제로 팬아웃하는 것은 일반적인 패턴이 아니며, SNS 주제에서 SQS 큐로 팬아웃하는 것이 더 적합합니다.

- **D. Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.**
  - RDS 이벤트 알림을 구독하고, SNS 주제를 사용하여 여러 SQS 큐로 데이터를 팬아웃하는 것은 가장 적합한 솔루션입니다. 각 SQS 큐는 대상 시스템으로 데이터를 전송하는 Lambda 함수를 트리거할 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **Amazon RDS 이벤트 알림**
   - RDS 이벤트 알림을 사용하여 데이터베이스 이벤트를 감지하고, 이를 기반으로 작업을 트리거할 수 있습니다.

2. **Amazon SNS**
   - SNS는 메시지를 여러 구독자에게 팬아웃할 수 있는 메시징 서비스입니다.
   - SNS 주제를 사용하여 여러 SQS 큐로 메시지를 팬아웃할 수 있습니다.

3. **Amazon SQS**
   - SQS는 메시지를 큐에 저장하고, 이를 소비자에게 전달하는 메시징 서비스입니다.
   - 여러 SQS 큐를 사용하여 데이터를 여러 대상 시스템으로 분산시킬 수 있습니다.

4. **AWS Lambda**
   - Lambda는 서버리스 컴퓨팅 서비스로, 특정 이벤트에 따라 코드를 실행할 수 있습니다.
   - SQS 큐에서 메시지를 소비하고, 대상 시스템으로 데이터를 전송하는 데 사용할 수 있습니다.

### 추가 학습 자료
- [Amazon RDS 이벤트 알림 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_Events.html)
- [Amazon SNS 공식 문서](https://docs.aws.amazon.com/ko_kr/sns/latest/dg/welcome.html)
- [Amazon SQS 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)

이 자료들을 통해 RDS 이벤트 알림, SNS, SQS, Lambda의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 113번
- AWS Snowball Edge Storage Optimized는 대용량 데이터를 효율적으로 전송할 수 있는 장치로, 네트웤크 대역폭이 부족한 상황에서도 사용 가능
- A: AWS DataSync는 네트워크를 통한 데이터 전송 서비스로, 추가적인 대역폭이 없음
- B: AWS Snowcones는 소규모 데이터 전송에 적합
- D: AWS Snowball Edge Storage Optimized 장치에 EC2 컴퓨팅 기능 포함 가능하지만, 데이터 전송 후에 AWS Glue를 사용하는 것이 더 효율적이며 운영 오버헤드가 적음

### 정답
C. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.

### 해설
회사는 50TB의 데이터를 온프레미스에서 AWS로 이동해야 하며, 네트워크 대역폭이 부족하여 추가 작업을 수행할 수 없습니다. AWS Snowball Edge Storage Optimized 장치를 사용하면 대량의 데이터를 물리적으로 AWS로 전송할 수 있습니다. AWS Glue를 사용하여 데이터 변환 작업을 수행하면 관리 오버헤드가 최소화됩니다.

각 옵션을 자세히 살펴보면:

- **A. Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.**
  - DataSync는 네트워크를 통해 데이터를 전송하므로, 네트워크 대역폭이 부족한 상황에서는 적합하지 않습니다.

- **B. Order an AWS Snowcone device to move the data. Deploy the transformation application to the device.**
  - Snowcone는 소규모 데이터 전송에 적합하며, 50TB의 데이터를 전송하는 데는 적합하지 않습니다.

- **C. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.**
  - Snowball Edge Storage Optimized 장치는 대량의 데이터를 물리적으로 전송하는 데 적합합니다. AWS Glue를 사용하여 데이터 변환 작업을 수행하면 관리 오버헤드가 최소화됩니다.

- **D. Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.**
  - Snowball Edge Storage Optimized 장치에 EC2 컴퓨팅을 포함시키는 것은 가능하지만, AWS Glue를 사용하는 것이 더 간단하고 관리 오버헤드가 적습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **AWS Snowball Edge Storage Optimized**
   - Snowball Edge Storage Optimized 장치는 대량의 데이터를 물리적으로 전송하는 데 적합합니다.
   - 네트워크 대역폭이 부족한 상황에서도 데이터를 전송할 수 있습니다.

2. **AWS Glue**
   - AWS Glue는 완전 관리형 ETL(Extract, Transform, Load) 서비스로, 데이터 변환 작업을 자동화할 수 있습니다.
   - 관리 오버헤드가 최소화됩니다.

3. **AWS DataSync**
   - DataSync는 네트워크를 통해 데이터를 전송하는 서비스입니다.
   - 네트워크 대역폭이 부족한 상황에서는 적합하지 않습니다.

4. **AWS Snowcone**
   - Snowcone은 소규모 데이터 전송에 적합한 장치입니다.
   - 50TB의 데이터를 전송하는 데는 적합하지 않습니다.

### 추가 학습 자료
- [AWS Snowball Edge Storage Optimized 공식 문서](https://docs.aws.amazon.com/ko_kr/snowball/latest/developer-guide/what-is-snowball.html)
- [AWS Glue 공식 문서](https://docs.aws.amazon.com/ko_kr/glue/latest/dg/what-is-glue.html)
- [AWS DataSync 공식 문서](https://docs.aws.amazon.com/ko_kr/datasync/latest/userguide/what-is-datasync.html)
- [AWS Snowcone 공식 문서](https://docs.aws.amazon.com/ko_kr/snowcone/latest/developer-guide/what-is-snowcone.html)

이 자료들을 통해 Snowball Edge Storage Optimized, Glue, DataSync, Snowcone의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 116번
**정답: A. Configure Amazon CloudFront in front of the website to use HTTPS functionality. D. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.**

**해설:**
- **Amazon S3 with Static Website Hosting:**
  - **저비용 및 고확장성:** S3는 정적 웹사이트 호스팅을 지원하며, 저비용으로 높은 확장성을 제공합니다. 정적 콘텐츠를 저장하고 제공하는 데 적합합니다.
  - **운영 오버헤드 최소화:** S3는 관리형 서비스로, 패치 및 유지 관리가 필요하지 않습니다. 이는 운영 오버헤드를 최소화합니다.

- **Amazon CloudFront:**
  - **콘텐츠 전송 네트워크 (CDN):** CloudFront는 전 세계에 분산된 엣지 로케이션을 통해 콘텐츠를 제공하므로, 웹사이트의 성능과 가용성을 향상시킵니다.
  - **HTTPS 지원:** CloudFront는 HTTPS를 지원하여, 웹사이트의 보안을 강화할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. AWS WAF:** WAF는 웹 애플리케이션 방화벽으로, HTTPS 기능을 제공하지 않습니다. 보안 강화에는 도움이 되지만, HTTPS를 설정하는 데는 적합하지 않습니다.
- **C. AWS Lambda:** Lambda는 서버리스 컴퓨팅 서비스로, 정적 콘텐츠를 제공하는 데는 적합하지 않습니다. 또한, 운영 오버헤드가 증가할 수 있습니다.
- **E. EC2와 Auto Scaling:** EC2 인스턴스를 사용하여 웹사이트를 호스팅하는 것은 운영 오버헤드가 크며, 정적 콘텐츠를 제공하는 데는 과도한 솔루션입니다.

**출제 의도:**
이 문제는 AWS에서 정적 웹사이트를 호스팅하고, 높은 확장성과 보안을 제공하는 방법을 평가합니다. 특히, Amazon S3와 CloudFront를 사용하여 정적 웹사이트를 호스팅하고, HTTPS를 통해 보안을 강화하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon S3 Static Website Hosting:**
   - S3의 정적 웹사이트 호스팅 기능과 사용 사례
   - S3를 사용하여 정적 웹사이트를 호스팅하는 방법

2. **Amazon CloudFront:**
   - CloudFront의 기본 개념과 사용 사례
   - CloudFront를 사용하여 콘텐츠를 전송하고, HTTPS를 통해 보안을 강화하는 방법

이 부분들을 공부하면, AWS에서 정적 웹사이트를 호스팅하고, 높은 확장성과 보안을 제공하는 방법을 더 잘 이해할 수 있습니다.

# 119번
**정답: B. Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.**

**해설:**
- **AWS Firewall Manager:**
  - **중앙 관리:** Firewall Manager는 여러 계정과 리전에서 AWS WAF 규칙을 중앙에서 관리할 수 있습니다. 이는 관리 오버헤드를 줄이고, 일관된 보안 정책을 적용하는 데 도움이 됩니다.
  - **자동 적용:** Firewall Manager는 새로운 리소스가 생성될 때 자동으로 WAF 규칙을 적용할 수 있어, 관리 작업을 최소화합니다.

- **AWS WAF:**
  - **웹 애플리케이션 방화벽:** WAF는 SQL 인젝션 및 크로스 사이트 스크립팅(XSS) 공격을 방어할 수 있는 규칙을 제공합니다. API Gateway와 통합하여 REST API를 보호할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. AWS WAF in both Regions:** 각 리전에 별도로 WAF를 설정하고 관리하는 것은 관리 오버헤드가 큽니다. 중앙에서 관리할 수 있는 Firewall Manager를 사용하는 것이 더 효율적입니다.
- **C. AWS Shield:** AWS Shield는 주로 DDoS 공격을 방어하는 데 사용되며, SQL 인젝션 및 XSS 공격을 방어하는 데는 적합하지 않습니다.
- **D. AWS Shield in one Region:** AWS Shield는 DDoS 공격 방어에 적합하며, SQL 인젝션 및 XSS 공격을 방어하는 데는 적합하지 않습니다. 또한, 한 리전에만 설정하는 것은 글로벌 보호를 제공하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 여러 계정과 리전에 걸쳐 API Gateway를 보호하는 방법을 평가합니다. 특히, AWS Firewall Manager와 WAF를 사용하여 중앙에서 보안 규칙을 관리하고, SQL 인젝션 및 XSS 공격을 방어하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Firewall Manager:**
   - Firewall Manager의 기본 개념과 사용 사례
   - Firewall Manager를 사용하여 여러 계정과 리전에서 WAF 규칙을 중앙에서 관리하는 방법

2. **AWS WAF:**
   - WAF의 기본 개념과 사용 사례
   - WAF를 사용하여 SQL 인젝션 및 XSS 공격을 방어하는 방법

이 부분들을 공부하면, AWS에서 여러 계정과 리전에 걸쳐 API Gateway를 보호하는 방법을 더 잘 이해할 수 있습니다.

# 121번
**정답: A. Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted snapshot.**

**해설:**
- **RDS 스냅샷 암호화:**
  - **스냅샷 암호화:** RDS 인스턴스가 암호화되지 않은 경우, 직접적으로 인스턴스를 암호화할 수 없습니다. 대신, 최신 스냅샷을 암호화된 스냅샷으로 복사한 후, 이 암호화된 스냅샷을 사용하여 새로운 암호화된 RDS 인스턴스를 생성할 수 있습니다.
  - **기존 인스턴스 교체:** 암호화된 스냅샷을 복원하여 새로운 암호화된 RDS 인스턴스를 생성한 후, 기존의 암호화되지 않은 인스턴스를 교체할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. EBS 볼륨 암호화:** RDS 인스턴스의 스냅샷을 EBS 볼륨으로 복사하는 것은 적절한 방법이 아닙니다. RDS 스냅샷은 RDS 인스턴스와 직접적으로 연관되어 있어야 합니다.
- **C. 기존 인스턴스에 복원:** 암호화된 스냅샷을 기존의 암호화되지 않은 인스턴스에 복원할 수 없습니다. 새로운 암호화된 인스턴스를 생성해야 합니다.
- **D. S3로 복사:** 스냅샷을 S3로 복사하는 것은 백업 목적으로는 유용할 수 있지만, RDS 인스턴스를 암호화하는 데는 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 RDS 인스턴스와 스냅샷을 암호화하는 방법을 평가합니다. 특히, 암호화되지 않은 RDS 인스턴스를 암호화된 인스턴스로 교체하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon RDS 스냅샷 암호화:**
   - RDS 스냅샷을 암호화하는 방법
   - 암호화된 스냅샷을 사용하여 새로운 암호화된 RDS 인스턴스를 생성하는 방법

2. **AWS Key Management Service (KMS):**
   - KMS의 기본 개념과 사용 사례
   - KMS를 사용하여 RDS 스냅샷을 암호화하는 방법

이 부분들을 공부하면, AWS에서 RDS 인스턴스와 스냅샷을 암호화하는 방법을 더 잘 이해할 수 있습니다.

# 122번
문제: 회사는 개발자가 애플리케이션에서 데이터를 암호화할 수 있도록 지원하는 확장 가능한 키 관리 인프라를 구축하고자 합니다. 운영 부담을 줄이기 위해 솔루션 아키텍트는 무엇을 해야 합니까?

해설
B: AWS Key Management Service (AWS KMS)는 암호화 키를 생성, 관리 및 보호하는 완전 관리형 서비스입니다. KMS를 사용하면 키 관리와 관련된 많은 운영 부담을 AWS가 대신 처리해주므로, 운영 부담을 크게 줄일 수 있습니다. 또한, KMS는 자동으로 키를 회전하고, 키 사용을 모니터링하며, 다양한 AWS 서비스와 통합되어 사용이 편리합니다.

다른 선택지의 문제점
A: 다중 요소 인증(MFA)은 보안 강화를 위해 유용하지만, 키 관리 인프라의 운영 부담을 줄이는 데 직접적인 도움이 되지 않습니다.
C: AWS Certificate Manager (ACM)는 SSL/TLS 인증서를 관리하는 서비스로, 암호화 키 관리와는 다른 목적을 가지고 있습니다.
D: IAM 정책을 사용하여 사용자 접근 권한

### 정답
B. Use AWS Key Management Service (AWS KMS) to protect the encryption keys.

### 해설
AWS Key Management Service (AWS KMS)는 암호화 키를 생성, 관리 및 보호하는 완전 관리형 서비스입니다. KMS를 사용하면 키 관리의 운영 부담을 줄이고, 개발자가 애플리케이션에서 데이터를 암호화하는 데 필요한 인프라를 쉽게 구축할 수 있습니다. KMS는 자동 키 회전, 키 정책 관리, 감사 로그 생성 등 다양한 기능을 제공하여 운영 부담을 최소화합니다.

각 옵션을 자세히 살펴보면:

- **A. Use multi-factor authentication (MFA) to protect the encryption keys.**
  - MFA는 보안 강화를 위한 방법이지만, 키 관리 인프라를 구축하고 운영 부담을 줄이는 데 직접적인 도움이 되지 않습니다.

- **B. Use AWS Key Management Service (AWS KMS) to protect the encryption keys.**
  - AWS KMS는 암호화 키를 생성, 관리 및 보호하는 완전 관리형 서비스로, 키 관리의 운영 부담을 줄이는 데 가장 적합한 솔루션입니다.

- **C. Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys.**
  - ACM은 SSL/TLS 인증서를 관리하는 서비스로, 암호화 키 관리에는 적합하지 않습니다.

- **D. Use an IAM policy to limit the scope of users who have access permissions to protect the encryption keys.**
  - IAM 정책을 사용하여 사용자 접근 권한을 제한하는 것은 보안 강화를 위한 방법이지만, 키 관리 인프라를 구축하고 운영 부담을 줄이는 데 직접적인 도움이 되지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **AWS Key Management Service (KMS)**
   - KMS는 암호화 키를 생성, 관리 및 보호하는 완전 관리형 서비스입니다.
   - 자동 키 회전, 키 정책 관리, 감사 로그 생성 등 다양한 기능을 제공합니다.

2. **Multi-Factor Authentication (MFA)**
   - MFA는 보안 강화를 위한 방법으로, 추가적인 인증 단계를 요구합니다.
   - 키 관리 인프라를 구축하는 데 직접적인 도움이 되지 않습니다.

3. **AWS Certificate Manager (ACM)**
   - ACM은 SSL/TLS 인증서를 관리하는 서비스입니다.
   - 암호화 키 관리에는 적합하지 않습니다.

4. **IAM 정책**
   - IAM 정책을 사용하여 사용자 접근 권한을 제한할 수 있습니다.
   - 키 관리 인프라를 구축하는 데 직접적인 도움이 되지 않습니다.

### 추가 학습 자료
- [AWS Key Management Service (KMS) 공식 문서](https://docs.aws.amazon.com/ko_kr/kms/latest/developerguide/overview.html)
- [Multi-Factor Authentication (MFA) 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_credentials_mfa.html)
- [AWS Certificate Manager (ACM) 공식 문서](https://docs.aws.amazon.com/ko_kr/acm/latest/userguide/acm-overview.html)
- [IAM 정책 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/access_policies.html)

이 자료들을 통해 KMS, MFA, ACM, IAM 정책의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 124번
문제 번역
회사는 많은 Amazon EC2 인스턴스를 사용하여 완료하는 매우 동적인 배치 처리 작업을 가지고 있습니다. 이 작업은 상태가 없으며, 언제든지 시작하고 중지할 수 있으며, 총 완료 시간은 보통 60분 이상 걸립니다. 회사는 솔루션 아키텍트에게 이 작업의 요구사항을 충족하는 확장 가능하고 비용 효율적인 솔루션을 설계해 달라고 요청했습니다. 솔루션 아키텍트는 무엇을 추천해야 합니까?

정답
A. Implement EC2 Spot Instances.

해설
A: EC2 Spot Instances는 AWS의 미사용 EC2 용량을 활용하여 최대 90%까지 비용을 절감할 수 있는 옵션입니다. 이 작업은 상태가 없고 언제든지 중지 및 시작할 수 있으므로, Spot Instances의 일시적인 중단 가능성에도 불구하고 적합합니다. 이는 확장 가능하고 비용 효율적인 솔루션입니다.
다른 선택지의 문제점
B: EC2 Reserved Instances는 장기적으로 특정 용량을 예약하여 비용을 절감할 수 있지만, 동적인 배치 처리 작업에는 유연성이 부족합니다.
C: EC2 On-Demand Instances는 유연하지만, 비용이 더 높을 수 있습니다. 상태가 없는 작업에는 Spot Instances가 더 비용 효율적입니다.
D: AWS Lambda는 짧은 실행 시간의 이벤트 기반 작업에 적합하지만, 60분 이상 걸리는 배치 처리 작업에는 적합하지 않습니다.
따라서, A 선택지가 확장 가능하고 비용 효율적인 솔루션을 제공하는 데 가장 적합합니다.

# 125번
**정답: A. Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets. E. Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.**

**해설:**
- **A. EC2 인스턴스와 RDS 인스턴스를 프라이빗 서브넷에 배치:**
  - **보안:** EC2 인스턴스와 RDS 인스턴스를 프라이빗 서브넷에 배치하면, 이 인스턴스들이 공용 인터넷에 노출되지 않으므로 보안이 강화됩니다.
  - **고가용성:** Auto Scaling 그룹을 사용하여 EC2 인스턴스를 관리하면, 인스턴스의 가용성을 높일 수 있습니다. RDS Multi-AZ 배포는 데이터베이스의 고가용성을 보장합니다.

- **E. VPC 구성:**
  - **공용 및 프라이빗 서브넷:** 두 개의 공용 서브넷과 두 개의 프라이빗 서브넷을 사용하여, 애플리케이션의 웹 계층과 데이터베이스 계층을 분리할 수 있습니다.
  - **NAT 게이트웨이:** 두 개의 NAT 게이트웨이를 사용하여, 프라이빗 서브넷의 EC2 인스턴스가 인터넷에 접근할 수 있도록 합니다. 이는 EC2 인스턴스가 결제 처리와 같은 외부 웹 서비스를 사용할 수 있도록 합니다.
  - **Application Load Balancer:** ALB를 공용 서브넷에 배치하여, 외부 트래픽을 EC2 인스턴스로 라우팅할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. ALB를 프라이빗 서브넷에 배치:** ALB를 프라이빗 서브넷에 배치하면, 외부 트래픽을 수신할 수 없으므로 적합하지 않습니다.
- **C. EC2 인스턴스를 공용 서브넷에 배치:** EC2 인스턴스를 공용 서브넷에 배치하면, 인스턴스가 공용 인터넷에 노출되므로 보안 요구 사항을 충족하지 않습니다.
- **D. 하나의 공용 서브넷과 하나의 프라이빗 서브넷:** 하나의 공용 서브넷과 하나의 프라이빗 서브넷만 사용하는 것은 고가용성을 보장하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 이중 계층 아키텍처를 사용하여 웹 애플리케이션을 배포하고, 보안과 고가용성을 보장하는 방법을 평가합니다. 특히, VPC 서브넷 구성, NAT 게이트웨이 사용, Auto Scaling 그룹 및 RDS Multi-AZ 배포를 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **VPC 서브넷 구성:**
   - 공용 서브넷과 프라이빗 서브넷의 차이점과 사용 사례
   - NAT 게이트웨이를 사용하여 프라이빗 서브넷의 인스턴스가 인터넷에 접근하는 방법

2. **Auto Scaling 그룹:**
   - Auto Scaling 그룹의 기본 개념과 사용 사례
   - Auto Scaling 그룹을 사용하여 EC2 인스턴스의 가용성을 높이는 방법

3. **RDS Multi-AZ 배포:**
   - RDS Multi-AZ 배포의 기본 개념과 사용 사례
   - RDS Multi-AZ 배포를 사용하여 데이터베이스의 고가용성을 보장하는 방법

이 부분들을 공부하면, AWS에서 이중 계층 아키텍처를 사용하여 웹 애플리케이션을 배포하고, 보안과 고가용성을 보장하는 방법을 더 잘 이해할 수 있습니다.

# 127번
문제 번역
미디어 회사가 시스템을 AWS 클라우드로 이전할 가능성을 평가하고 있습니다. 회사는 비디오 처리에 최대한의 I/O 성능을 제공하는 최소 10TB의 스토리지, 미디어 콘텐츠를 저장하기 위한 300TB의 매우 내구성 있는 스토리지, 더 이상 사용되지 않는 아카이브 미디어를 위한 900TB의 스토리지가 필요합니다. 솔루션 아키텍트는 이러한 요구사항을 충족하기 위해 어떤 서비스 세트를 추천해야 합니까?

정답
A. Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage

해설
Amazon EBS (Elastic Block Store): EBS는 고성능 I/O를 제공하는 블록 스토리지 서비스로, 비디오 처리와 같은 고성능 작업에 적합합니다.
Amazon S3 (Simple Storage Service): S3는 높은 내구성과 가용성을 제공하는 객체 스토리지 서비스로, 미디어 콘텐츠와 같은 데이터를 저장하는 데 적합합니다.
Amazon S3 Glacier: S3 Glacier는 장기 아카이브 스토리지 서비스로, 자주 접근하지 않는 데이터를 비용 효율적으로 저장하는 데 적합합니다.
다른 선택지의 문제점
B: Amazon EFS는 파일 스토리지 서비스로, 대규모 데이터 저장에는 적합하지만, S3보다 비용이 높을 수 있습니다.
C: Amazon EC2 instance store는 인스턴스가 종료되면 데이터가 사라지므로, 내구성 있는 데이터 저장에는 적합하지 않습니다.
D: Amazon EC2 instance store는 내구성 있는 데이터 저장에 적합하지 않으며, S3는 아카이브 스토리지로 사용하기에는 비용이 높을 수 있습니다.
따라서, A 선택지가 요구사항을 충족하는 가장 적합한 솔루션입니다.

# 128번
문제 번역
회사는 AWS 클라우드에서 컨테이너로 애플리케이션을 실행하고자 합니다. 이 애플리케이션들은 상태가 없으며, 기본 인프라의 중단을 견딜 수 있습니다. 회사는 비용과 운영 오버헤드를 최소화하는 솔루션이 필요합니다. 솔루션 아키텍트는 이러한 요구사항을 충족하기 위해 무엇을 해야 합니까?

정답
B. Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.

해설
B: Amazon EKS는 관리형 Kubernetes 서비스로, 컨테이너 오케스트레이션을 자동화하여 운영 오버헤드를 줄입니다. Spot Instances를 사용하면 비용을 크게 절감할 수 있으며, 애플리케이션이 상태가 없고 중단을 견딜 수 있으므로 적합합니다. EKS 관리 노드 그룹을 사용하면 Kubernetes 클러스터의 노드 관리를 AWS가 대신 처리해주므로 운영 오버헤드가 최소화됩니다.
다른 선택지의 문제점
A: EC2 Auto Scaling 그룹을 사용하여 컨테이너를 실행하는 것은 가능하지만, EKS를 사용하면 Kubernetes 관리가 자동화되어 운영 오버헤드가 더 낮습니다.
C: On-Demand Instances는 유연하지만 비용이 더 높을 수 있습니다. 상태가 없는 애플리케이션에는 Spot Instances가 더 비용 효율적입니다.
D: On-Demand Instances는 비용이 더 높을 수 있으며, 상태가 없는 애플리케이션에는 Spot Instances가 더 적합합니다.
따라서, B 선택지가 요구사항을 충족하는 가장 적합한 솔루션입니다.

# 130번
문제 번역
애플리케이션이 여러 가용 영역에 걸쳐 Amazon EC2 인스턴스에서 실행됩니다. 인스턴스는 Application Load Balancer 뒤에 있는 Amazon EC2 Auto Scaling 그룹에서 실행됩니다. 애플리케이션은 EC2 인스턴스의 CPU 사용률이 40%에 가깝거나 그 이하일 때 최상의 성능을 발휘합니다. 솔루션 아키텍트는 그룹의 모든 인스턴스에서 원하는 성능을 유지하기 위해 무엇을 해야 합니까?

정답
B. Use a target tracking policy to dynamically scale the Auto Scaling group.

해설
B: 타겟 추적 정책(Target Tracking Policy)은 Auto Scaling 그룹의 평균 CPU 사용률을 특정 목표(이 경우 40%)로 유지하도록 자동으로 조정합니다. 이는 CPU 사용률이 목표 수준에 도달하도록 인스턴스를 동적으로 추가하거나 제거하여 원하는 성능을 유지하는 데 가장 적합한 방법입니다.
다른 선택지의 문제점
A: 단순 스케일링 정책(Simple Scaling Policy)은 특정 조건이 충족될 때만 인스턴스를 추가하거나 제거합니다. 이는 CPU 사용률을 일정하게 유지하는 데 충분하지 않을 수 있습니다.
C: AWS Lambda 함수를 사용하여 Auto Scaling 그룹의 용량을 업데이트하는 것은 복잡하고 유지 관리가 어려울 수 있습니다.
D: 예약된 스케일링 작업(Scheduled Scaling Actions)은 특정 시간에만 스케일링을 수행하므로, 실시간으로 변동하는 CPU 사용률을 효과적으로 관리하지 못할 수 있습니다.
따라서, B 선택지가 그룹의 모든 인스턴스에서 원하는 성능을 유지하는 데 가장 적합한 솔루션입니다.


# 131번
문제 번역
회사는 파일 공유 애플리케이션을 개발 중이며, Amazon S3 버킷을 스토리지로 사용할 예정입니다. 회사는 모든 파일을 Amazon CloudFront 배포를 통해 제공하고자 합니다. 회사는 파일이 S3 URL을 통해 직접 접근되지 않도록 하고 싶습니다. 솔루션 아키텍트는 이러한 요구사항을 충족하기 위해 무엇을 해야 합니까?

정답
D. Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the S3 bucket permissions so that only the OAI has read permission.

해설
D: Origin Access Identity (OAI)를 생성하고 이를 CloudFront 배포에 할당하면, CloudFront를 통해서만 S3 버킷의 객체에 접근할 수 있습니다. S3 버킷 권한을 설정하여 OAI만 읽기 권한을 가지도록 구성하면, S3 URL을 통한 직접 접근을 방지할 수 있습니다.
다른 선택지의 문제점
A: 각 S3 버킷에 대해 개별 정책을 작성하는 것은 복잡하고 관리하기 어렵습니다. 또한, CloudFront에 대한 접근 권한을 효과적으로 제어하지 못할 수 있습니다.
B: IAM 사용자를 생성하고 읽기 권한을 부여하는 것은 CloudFront와의 통합을 보장하지 않으며, S3 URL을 통한 직접 접근을 방지하지 못합니다.
C: S3 버킷 정책에 CloudFront 배포 ID를 Principal로 지정하는 것은 일반적인 접근 제어 방법이 아니며, OAI를 사용하는 것이 더 적합합니다.
따라서, D 선택지가 요구사항을 충족하는 가장 적합한 솔루션입니다.

# 133번
cerulean85
A company runs an Oracle database on premises. As part of the company’s migration to AWS, the company wants to upgrade the database to the most recent available version. The company also wants to set up disaster recovery (DR) for the database. The company needs to minimize the operational overhead for normal operations and DR setup. The company also needs to maintain access to the database's underlying operating system. Which solution will meet these requirements?

A. Migrate the Oracle database to an Amazon EC2 instance. Set up database replication to a different AWS Region. B. Migrate the Oracle database to Amazon RDS for Oracle. Activate Cross-Region automated backups to replicate the snapshots to another AWS Region. C. Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database in another AWS Region. D. Migrate the Oracle database to Amazon RDS for Oracle. Create a standby database in another Availability Zone.

GitHub Copilot
문제 번역
회사는 온프레미스에서 Oracle 데이터베이스를 운영하고 있습니다. 회사의 AWS로의 마이그레이션의 일환으로, 회사는 데이터베이스를 최신 버전으로 업그레이드하고자 합니다. 또한, 회사는 데이터베이스에 대한 재해 복구(DR)를 설정하고자 합니다. 회사는 정상 운영과 DR 설정에 대한 운영 오버헤드를 최소화해야 하며, 데이터베이스의 기본 운영 체제에 대한 접근을 유지해야 합니다. 이 요구사항을 충족하는 솔루션은 무엇입니까?

정답
C. Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database in another AWS Region.

해설
C: Amazon RDS Custom for Oracle는 사용자가 데이터베이스의 기본 운영 체제에 접근할 수 있도록 허용하는 관리형 데이터베이스 서비스입니다. 이는 운영 오버헤드를 줄이면서도 데이터베이스의 기본 운영 체제에 대한 접근을 유지할 수 있습니다. 또한, 다른 AWS 리전에서 읽기 복제본을 생성하여 재해 복구를 설정할 수 있습니다.
다른 선택지의 문제점
A: Oracle 데이터베이스를 Amazon EC2 인스턴스로 마이그레이션하면 운영 오버헤드가 증가할 수 있으며, 관리형 서비스의 이점을 누리지 못합니다.
B: Amazon RDS for Oracle은 기본 운영 체제에 대한 접근을 제공하지 않습니다.
D: Amazon RDS for Oracle은 기본 운영 체제에 대한 접근을 제공하지 않으며, 다른 가용 영역에 대기 데이터베이스를 생성하는 것은 재해 복구를 위한 최적의 솔루션이 아닙니다.
따라서, C 선택지가 요구사항을 충족하는 가장 적합한 솔루션입니다.


# 134번
**정답: A. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to query the data.**

**해설:**
- **Amazon S3 Cross-Region Replication (CRR):**
  - **데이터 복제:** CRR을 사용하면 S3 버킷 간에 데이터를 자동으로 복제할 수 있습니다. 이는 데이터의 가용성과 내구성을 높이는 데 도움이 됩니다.
  - **암호화:** CRR은 암호화된 객체를 다른 리전의 S3 버킷으로 복제할 수 있습니다.

- **AWS KMS Multi-Region Keys (SSE-KMS):**
  - **보안:** KMS를 사용하여 데이터를 암호화하면, 강력한 보안과 키 관리 기능을 제공받을 수 있습니다. Multi-Region Keys를 사용하면 여러 리전에 걸쳐 일관된 암호화 키를 사용할 수 있습니다.

- **Amazon Athena:**
  - **서버리스 분석:** Athena는 S3에 저장된 데이터를 SQL을 사용하여 분석할 수 있는 서버리스 쿼리 서비스입니다. 이는 운영 오버헤드를 최소화하면서 데이터를 분석하는 데 적합합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. Amazon RDS:** RDS는 관리형 관계형 데이터베이스 서비스로, 서버리스 솔루션이 아니며, 운영 오버헤드가 증가할 수 있습니다.
- **C. SSE-S3:** SSE-S3는 S3 관리형 키를 사용하여 데이터를 암호화하지만, KMS를 사용하는 것보다 보안과 관리 기능이 제한적입니다.
- **D. Amazon RDS:** RDS는 서버리스 솔루션이 아니며, 운영 오버헤드가 증가할 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 서버리스 솔루션을 사용하여 데이터를 분석하고, 데이터의 암호화와 리전 간 복제를 설정하는 방법을 평가합니다. 특히, Amazon S3, AWS KMS, Amazon Athena를 사용하여 운영 오버헤드를 최소화하면서 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon S3 Cross-Region Replication (CRR):**
   - CRR의 기본 개념과 사용 사례
   - CRR을 사용하여 데이터를 다른 리전의 S3 버킷으로 복제하는 방법

2. **AWS KMS Multi-Region Keys (SSE-KMS):**
   - KMS의 기본 개념과 사용 사례
   - Multi-Region Keys를 사용하여 데이터를 암호화하고, 여러 리전에 걸쳐 일관된 키를 사용하는 방법

3. **Amazon Athena:**
   - Athena의 기본 개념과 사용 사례
   - Athena를 사용하여 S3에 저장된 데이터를 SQL을 사용하여 분석하는 방법

이 부분들을 공부하면, AWS에서 서버리스 솔루션을 사용하여 데이터를 분석하고, 데이터의 암호화와 리전 간 복제를 설정하는 방법을 더 잘 이해할 수 있습니다.



# 136번
문제 번역
회사는 온프레미스 PostgreSQL 데이터베이스를 Amazon Aurora PostgreSQL로 마이그레이션하고 있습니다. 마이그레이션 중에도 온프레미스 데이터베이스는 온라인 상태를 유지하고 접근 가능해야 합니다. Aurora 데이터베이스는 온프레미스 데이터베이스와 동기화 상태를 유지해야 합니다. 솔루션 아키텍트는 이러한 요구사항을 충족하기 위해 어떤 조치를 취해야 합니까? (두 가지를 선택하십시오.)

정답
A. Create an ongoing replication task.

C. Create an AWS Database Migration Service (AWS DMS) replication server.

해설
A: 지속적인 복제 작업을 생성하면 온프레미스 데이터베이스와 Amazon Aurora PostgreSQL 간의 데이터 동기화를 유지할 수 있습니다.
C: AWS Database Migration Service (AWS DMS) 복제 서버를 생성하면 온프레미스 데이터베이스에서 Amazon Aurora PostgreSQL로의 데이터 마이그레이션을 관리할 수 있습니다. DMS는 데이터베이스를 마이그레이션하면서도 소스 데이터베이스를 온라인 상태로 유지할 수 있습니다.
다른 선택지의 문제점
B: 데이터베이스 백업을 생성하는 것은 초기 데이터 전송에 유용할 수 있지만, 지속적인 동기화를 보장하지 않습니다.
D: AWS Schema Conversion Tool (AWS SCT)은 데이터베이스 스키마 변환에 사용되며, 지속적인 데이터 동기화를 보장하지 않습니다.
E: Amazon EventBridge (Amazon CloudWatch Events) 규칙은 모니터링에 유용할 수 있지만, 데이터 동기화를 직접적으로 관리하지 않습니다.
따라서, A와 C 선택지가 요구사항을 충족하는 가장 적합한 솔루션입니다.

# 137번
문제 번역
회사는 각 비즈니스 유닛을 위해 전용 AWS 계정을 생성하고, 각 비즈니스 유닛의 계정을 독립적으로 관리할 수 있도록 AWS Organizations를 사용하고 있습니다. 루트 이메일 수신자가 한 계정의 루트 사용자 이메일 주소로 전송된 알림을 놓쳤습니다. 회사는 향후 모든 알림이 놓치지 않도록 하고자 합니다. 향후 알림은 계정 관리자에게만 제한되어야 합니다. 이 요구사항을 충족하는 솔루션은 무엇입니까?

정답
B. Configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.

해설
B: 모든 AWS 계정 루트 사용자 이메일 주소를 몇 명의 관리자에게 전달되는 배포 목록으로 구성하면, 알림이 놓치지 않도록 할 수 있습니다. 또한, AWS Organizations 콘솔 또는 프로그래밍 방식으로 AWS 계정 대체 연락처를 구성하여 알림을 받을 수 있는 추가적인 보안을 제공합니다.
다른 선택지의 문제점
A: 회사의 이메일 서버를 구성하여 알림 이메일 메시지를 조직의 모든 사용자에게 전달하는 것은 보안 위험이 있으며, 알림이 너무 많은 사람에게 전달될 수 있습니다.
C: 모든 알림을 한 명의 관리자에게 보내는 것은 단일 실패 지점을 만들 수 있으며, 알림이 적절한 그룹에 전달되지 않을 수 있습니다.
D: 모든 계정에 동일한 루트 사용자 이메일 주소를 사용하는 것은 관리 및 보안 측면에서 비효율적이며, 각 계정의 독립성을 유지하지 못합니다.
따라서, B 선택지가 요구사항을 충족하는 가장 적합한 솔루션입니다.

# 138번
문제 번역
회사는 AWS에서 전자상거래 애플리케이션을 운영하고 있습니다. 새로운 주문이 들어올 때마다 RabbitMQ 큐에 메시지가 게시되며, 이 큐는 단일 가용 영역의 Amazon EC2 인스턴스에서 실행됩니다. 이러한 메시지는 별도의 EC2 인스턴스에서 실행되는 다른 애플리케이션에 의해 처리되며, 이 애플리케이션은 세부 정보를 또 다른 EC2 인스턴스에서 실행되는 PostgreSQL 데이터베이스에 저장합니다. 모든 EC2 인스턴스는 동일한 가용 영역에 있습니다. 회사는 가장 높은 가용성을 제공하고 운영 오버헤드를 최소화하기 위해 아키텍처를 재설계해야 합니다. 솔루션 아키텍트는 이러한 요구사항을 충족하기 위해 무엇을 해야 합니까?

정답
B. Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.

해설
B: RabbitMQ 큐를 Amazon MQ의 활성/대기 페어로 마이그레이션하면 높은 가용성을 제공할 수 있습니다. 애플리케이션을 호스팅하는 EC2 인스턴스에 대해 다중 가용 영역(AZ) Auto Scaling 그룹을 생성하면 애플리케이션의 가용성을 높일 수 있습니다. PostgreSQL 데이터베이스를 Amazon RDS for PostgreSQL의 다중 AZ 배포로 마이그레이션하면 데이터베이스의 가용성과 관리 오버헤드를 최소화할 수 있습니다.
다른 선택지의 문제점
A: PostgreSQL 데이터베이스를 EC2 인스턴스에서 호스팅하는 것은 관리 오버헤드를 증가시키며, RDS를 사용하는 것보다 가용성이 낮을 수 있습니다.
C: RabbitMQ 큐를 EC2 인스턴스에서 호스팅하는 것은 관리 오버헤드를 증가시키며, Amazon MQ를 사용하는 것보다 가용성이 낮을 수 있습니다.
D: PostgreSQL 데이터베이스를 EC2 인스턴스에서 호스팅하는 것은 관리 오버헤드를 증가시키며, RDS를 사용하는 것보다 가용성이 낮을 수 있습니다.
따라서, B 선택지가 요구사항을 충족하는 가장 적합한 솔루션입니다.

# 139번 ---
**정답: C. Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.**

**해설:**
- **S3 Replication:**
  - **자동 복제:** S3 복제를 설정하면, 초기 S3 버킷에 파일이 업로드될 때 자동으로 분석 S3 버킷으로 복제됩니다. 이는 수동 작업을 자동화하고 운영 오버헤드를 줄이는 데 도움이 됩니다.
  - **고가용성:** S3 복제는 데이터를 다른 버킷으로 자동으로 복제하여 고가용성을 제공합니다.

- **S3 Event Notification:**
  - **이벤트 기반 처리:** 분석 S3 버킷에 이벤트 알림을 설정하여, 파일이 복제될 때 Lambda 함수와 SageMaker Pipelines를 트리거할 수 있습니다. 이는 파일이 복제될 때마다 자동으로 패턴 매칭 코드와 데이터 파이프라인을 실행할 수 있도록 합니다.
  - **s3:ObjectCreated:Put 이벤트:** 이 이벤트 유형을 사용하면, 객체가 생성될 때마다 이벤트 알림이 트리거됩니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Lambda 함수로 파일 복사:** Lambda 함수를 사용하여 파일을 복사하는 것은 가능하지만, S3 복제를 사용하는 것보다 운영 오버헤드가 큽니다.
- **B. EventBridge 사용:** EventBridge를 사용하여 이벤트를 처리하는 것은 가능하지만, S3 이벤트 알림을 직접 사용하는 것이 더 간단하고 운영 오버헤드가 적습니다.
- **D. EventBridge 사용:** EventBridge를 사용하여 이벤트를 처리하는 것은 가능하지만, S3 이벤트 알림을 직접 사용하는 것이 더 간단하고 운영 오버헤드가 적습니다.

**출제 의도:**
이 문제는 AWS에서 S3 버킷 간의 데이터 복제를 자동화하고, 이벤트 기반 처리를 설정하는 방법을 평가합니다. 특히, S3 복제와 S3 이벤트 알림을 사용하여 운영 오버헤드를 최소화하면서 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon S3 Replication:**
   - S3 복제의 기본 개념과 사용 사례
   - S3 복제를 설정하여 데이터를 자동으로 복제하는 방법

2. **S3 Event Notification:**
   - S3 이벤트 알림의 기본 개념과 사용 사례
   - S3 이벤트 알림을 설정하여 특정 이벤트가 발생할 때 Lambda 함수와 SageMaker Pipelines를 트리거하는 방법

이 부분들을 공부하면, AWS에서 S3 버킷 간의 데이터 복제를 자동화하고, 이벤트 기반 처리를 설정하는 방법을 더 잘 이해할 수 있습니다.

# 140번
**정답: A. Use Spot Instances for the data ingestion layer C. Purchase a 1-year Compute Savings Plan for the front end and API layer.**

**해설:**
- **Spot Instances:**
  - **비용 절감:** Spot Instances는 사용하지 않는 EC2 용량을 활용하여 최대 90%까지 비용을 절감할 수 있습니다. 이는 데이터 수집 계층의 스포라딕하고 예측 불가능한 사용 패턴에 적합합니다.
  - **중단 가능성:** Spot Instances는 언제든지 중단될 수 있지만, 데이터 수집 계층의 작업이 중단을 견딜 수 있으므로 적합합니다.

- **Compute Savings Plan:**
  - **비용 절감:** Compute Savings Plan은 특정 인스턴스 유형에 구애받지 않고, EC2, Fargate, Lambda 등 다양한 컴퓨팅 옵션에 대해 비용을 절감할 수 있습니다.
  - **예측 가능한 사용:** 프론트엔드와 API 계층의 사용량이 예측 가능하므로, 1년 Compute Savings Plan을 구매하면 비용을 절감할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. On-Demand Instances:** On-Demand Instances는 비용이 더 높으며, 스포라딕하고 예측 불가능한 사용 패턴에 적합하지 않습니다.
- **D. Reserved Instances:** 데이터 수집 계층의 사용 패턴이 예측 불가능하므로, Reserved Instances는 적합하지 않습니다.
- **E. EC2 instance Savings Plan:** EC2 instance Savings Plan은 특정 인스턴스 패밀리와 리전에만 적용되므로, Compute Savings Plan이 더 유연하고 비용 효율적입니다.

**출제 의도:**
이 문제는 AWS에서 다양한 컴퓨팅 옵션을 사용하여 비용을 최적화하는 방법을 평가합니다. 특히, Spot Instances와 Compute Savings Plan을 사용하여 비용을 절감하고, 예측 가능한 사용 패턴에 맞는 구매 옵션을 선택하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Spot Instances:**
   - Spot Instances의 기본 개념과 사용 사례
   - Spot Instances를 사용하여 비용을 절감하고, 중단 가능성을 관리하는 방법

2. **Savings Plans:**
   - Compute Savings Plan과 EC2 instance Savings Plan의 차이점과 사용 사례
   - Savings Plan을 사용하여 비용을 절감하는 방법

이 부분들을 공부하면, AWS에서 다양한 컴퓨팅 옵션을 사용하여 비용을 최적화하는 방법을 더 잘 이해할 수 있습니다.


# 141번
**정답: B. Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region.**

**해설:**
- **다중 리전 배포:**
  - **고가용성:** 두 개의 AWS 리전에 애플리케이션 스택을 배포하면, 하나의 리전에서 장애가 발생하더라도 다른 리전에서 서비스를 계속 제공할 수 있습니다.
  - **지연 시간 최소화:** 여러 리전에 배포하면, 사용자가 가장 가까운 리전에서 콘텐츠를 제공받을 수 있어 지연 시간을 최소화할 수 있습니다.

- **Amazon Route 53 Latency Routing Policy:**
  - **지연 시간 기반 라우팅:** Route 53의 지연 시간 라우팅 정책을 사용하면, 사용자가 가장 낮은 지연 시간을 제공하는 리전의 ALB에서 콘텐츠를 제공받을 수 있습니다. 이는 전 세계 사용자에게 최적의 성능을 제공합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. 단일 리전 배포 및 CloudFront:** 단일 리전에 배포하면, 해당 리전에서 장애가 발생할 경우 서비스가 중단될 수 있습니다. 또한, CloudFront는 정적 콘텐츠 캐싱에 적합하지만, 동적 콘텐츠의 지연 시간을 최소화하는 데는 한계가 있습니다.
- **C. 단일 리전 배포 및 CloudFront:** 단일 리전에 배포하면, 해당 리전에서 장애가 발생할 경우 서비스가 중단될 수 있습니다. 또한, 동적 콘텐츠를 ALB에서 직접 제공하면 지연 시간이 증가할 수 있습니다.
- **D. 지리적 라우팅 정책:** 지리적 라우팅 정책은 사용자의 지리적 위치에 따라 라우팅하지만, 지연 시간을 최적화하는 데는 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 전 세계 사용자에게 최적의 성능을 제공하기 위해 애플리케이션을 설계하는 방법을 평가합니다. 특히, 다중 리전 배포와 Route 53 지연 시간 라우팅 정책을 사용하여 지연 시간을 최소화하고, 고가용성을 제공하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **다중 리전 배포:**
   - 여러 리전에 애플리케이션을 배포하여 고가용성과 성능을 최적화하는 방법
   - 다중 리전 배포의 장단점

2. **Amazon Route 53 라우팅 정책:**
   - Route 53의 다양한 라우팅 정책(지연 시간, 지리적, 가중치 등)의 기본 개념과 사용 사례
   - 지연 시간 라우팅 정책을 사용하여 사용자에게 최적의 성능을 제공하는 방법

이 부분들을 공부하면, AWS에서 전 세계 사용자에게 최적의 성능을 제공하기 위해 애플리케이션을 설계하는 방법을 더 잘 이해할 수 있습니다.  


# 143번
이 문제는 AWS로 기존의 온프레미스 모놀리식 애플리케이션을 마이그레이션하려는 회사의 요구사항을 다루고 있습니다. 회사는 가능한 한 많은 프론트엔드 코드와 백엔드 코드를 유지하면서 애플리케이션을 더 작은 애플리케이션으로 나누고 싶어합니다. 각 애플리케이션은 다른 팀에서 관리하며, **고가용성**과 **운영 오버헤드 최소화**를 목표로 하는 **확장 가능한 솔루션**이 필요합니다. 

각 선택지의 설명과 정답을 분석해 보겠습니다.

### 선택지 분석

**A. Host the application on AWS Lambda. Integrate the application with Amazon API Gateway.**
- **설명**: AWS Lambda는 서버리스 컴퓨팅 서비스를 제공하여 자동으로 확장할 수 있으며, 운영 오버헤드가 적습니다. Amazon API Gateway를 사용하여 API를 생성하고 관리할 수 있습니다. 
- **장점**: 
  - 서버리스 아키텍처로 인해 운영 관리가 최소화됩니다.
  - Lambda는 요청 수에 따라 자동으로 확장됩니다.
  - API Gateway와의 통합이 간편하여 애플리케이션을 마이크로서비스로 분리할 수 있습니다.
- **단점**: 모놀리식 애플리케이션에서 Lambda 기반으로 이전할 때 기존 코드를 완전히 재작성해야 할 수 있습니다.

---

**B. Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that is integrated with AWS Lambda.**
- **설명**: AWS Amplify는 풀스택 애플리케이션을 쉽게 구축하고 배포할 수 있도록 돕는 플랫폼입니다. API Gateway와 AWS Lambda와 통합하여 프론트엔드를 서버리스로 구축할 수 있습니다.
- **장점**: 
  - Amplify는 프론트엔드 및 백엔드 개발을 간편하게 관리할 수 있습니다.
  - 서버리스 아키텍처를 통해 확장성과 운영 오버헤드를 최소화할 수 있습니다.
- **단점**: AWS Amplify는 주로 정적 웹사이트 및 서버리스 애플리케이션에 적합합니다. 모놀리식 애플리케이션을 완전히 재작성해야 할 수 있습니다.

---

**C. Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.**
- **설명**: Amazon EC2에서 애플리케이션을 호스팅하고, Auto Scaling 그룹과 함께 Application Load Balancer를 설정하여 트래픽을 분산합니다.
- **장점**: 
  - 기존의 EC2 기반 아키텍처로 쉽게 마이그레이션할 수 있습니다.
  - EC2 인스턴스는 높은 커스터마이징을 제공하고, 부하에 따라 확장할 수 있습니다.
- **단점**: 운영 관리가 상대적으로 많으며, 모놀리식 구조로 인해 애플리케이션을 마이크로서비스로 분할하는 데 어려움이 있을 수 있습니다.

---

**D. Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.**
- **설명**: Amazon ECS를 사용하여 컨테이너화된 애플리케이션을 호스팅하고, Application Load Balancer를 설정하여 트래픽을 분산합니다.
- **장점**: 
  - ECS는 컨테이너 관리 서비스로, 마이크로서비스 아키텍처를 지원하며, 애플리케이션을 더 작은 서비스로 나누는 데 유리합니다.
  - 자동 확장 기능을 제공하며, 운영 오버헤드가 적습니다.
  - 애플리케이션의 개별 구성 요소를 각 팀이 독립적으로 관리할 수 있도록 합니다.
- **단점**: 컨테이너 환경에 대한 학습 곡선이 있을 수 있습니다.

---

### **정답 및 해설**
**정답: D. Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.**

**해설**:
- **모놀리식 애플리케이션을 더 작은 서비스로 분할**하는 데 유리한 컨테이너 기반 아키텍처를 제공합니다. 
- **ECS를 사용하면** 여러 팀이 각각의 서비스를 관리할 수 있으며, **각 서비스는 독립적으로 배포되고 확장**될 수 있습니다. 
- **운영 오버헤드가 적고, 자동 확장 기능을 통해** 고가용성과 높은 트래픽 처리 능력을 제공하므로 기업의 요구 사항에 가장 적합합니다.

따라서 **ECS와 Application Load Balancer** 조합은 주어진 조건을 모두 충족하며, **기존 코드를 최대한 유지하면서** 마이크로서비스 아키텍처로 이전할 수 있는 최적의 선택입니다.


# 145번
**정답: A. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.**

**해설:**
- **Amazon RDS for MySQL:**
  - **데이터베이스 분리:** RDS를 사용하여 데이터베이스를 EC2 인스턴스에서 분리하면, 데이터베이스 관리와 성능 최적화가 쉬워집니다. 이는 애플리케이션의 성능을 향상시키는 데 도움이 됩니다.
  - **관리형 서비스:** RDS는 관리형 서비스로, 백업, 복구, 소프트웨어 패치 등을 자동으로 처리하여 운영 오버헤드를 줄입니다.

- **Application Load Balancer (ALB):**
  - **로드 밸런싱:** ALB를 사용하여 여러 EC2 인스턴스 간에 트래픽을 분산시키면, 애플리케이션의 가용성과 성능을 향상시킬 수 있습니다.
  - **확장성:** ALB와 Auto Scaling을 결합하면, 트래픽 증가에 따라 자동으로 인스턴스를 추가하여 확장할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. Route 53 weighted routing:** Route 53의 가중치 라우팅은 로드 밸런싱을 제공할 수 있지만, ALB를 사용하는 것이 더 일반적이고 효율적입니다.
- **C. Lambda와 CloudWatch:** Lambda와 CloudWatch를 사용하여 인스턴스 유형을 변경하는 것은 성능 문제를 해결할 수 있지만, 이는 자동 확장과 로드 밸런싱을 제공하지 않습니다.
- **D. Aurora와 Spot Fleet:** Aurora는 고성능 데이터베이스를 제공하지만, Spot Fleet을 사용하는 것은 비용 효율적일 수 있지만, 인스턴스가 중단될 수 있어 안정적인 서비스 제공에 문제가 있을 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 웹 애플리케이션의 성능 문제를 해결하고, 비용 효율적으로 확장 가능한 아키텍처를 설계하는 방법을 평가합니다. 특히, Amazon RDS와 Application Load Balancer를 사용하여 데이터베이스를 분리하고, 로드 밸런싱을 통해 애플리케이션의 성능을 향상시키는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon RDS:**
   - RDS의 기본 개념과 사용 사례
   - RDS를 사용하여 데이터베이스를 관리하고 성능을 최적화하는 방법

2. **Application Load Balancer (ALB):**
   - ALB의 기본 개념과 사용 사례
   - ALB를 사용하여 트래픽을 분산시키고, 애플리케이션의 가용성과 성능을 향상시키는 방법

이 부분들을 공부하면, AWS에서 웹 애플리케이션의 성능 문제를 해결하고, 비용 효율적으로 확장 가능한 아키텍처를 설계하는 방법을 더 잘 이해할 수 있습니다.


# 150번
**정답: A. Create Amazon CloudWatch composite alarms where possible.**

**해설:**
- **Amazon CloudWatch Composite Alarms:**
  - **복합 조건:** Composite Alarms는 여러 개의 단일 알람을 결합하여, 특정 조건이 모두 충족될 때만 알람을 발생시킬 수 있습니다. 이는 CPU 사용률이 50%를 초과하고 동시에 디스크의 읽기 IOPS가 높은 경우에만 알람을 발생시키는 데 적합합니다.
  - **거짓 알람 감소:** Composite Alarms를 사용하면, 단일 조건이 충족될 때 발생하는 거짓 알람을 줄일 수 있습니다. 이는 CPU 사용률이 일시적으로 50%를 초과하는 경우와 같은 상황에서 불필요한 알람을 방지합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. CloudWatch Dashboards:** 대시보드는 메트릭을 시각화하고 문제를 빠르게 파악하는 데 유용하지만, 자동으로 알람을 발생시키는 기능은 없습니다.
- **C. CloudWatch Synthetics Canaries:** Synthetics Canaries는 애플리케이션의 엔드포인트를 모니터링하고 가용성을 테스트하는 데 사용되며, 인프라 메트릭 알람을 설정하는 데 적합하지 않습니다.
- **D. Single Metric Alarms:** 단일 메트릭 알람은 여러 메트릭 조건을 결합할 수 없으므로, CPU 사용률과 디스크 읽기 IOPS를 동시에 모니터링하는 데 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 인프라 메트릭을 모니터링하고, 특정 조건이 충족될 때만 알람을 발생시키는 방법을 평가합니다. 특히, Amazon CloudWatch Composite Alarms를 사용하여 거짓 알람을 줄이고, 필요한 경우에만 알람을 발생시키는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon CloudWatch Composite Alarms:**
   - Composite Alarms의 기본 개념과 사용 사례
   - Composite Alarms를 사용하여 여러 메트릭 조건을 결합하고, 특정 조건이 충족될 때만 알람을 발생시키는 방법

2. **Amazon CloudWatch Alarms:**
   - CloudWatch Alarms의 기본 개념과 사용 사례
   - 단일 메트릭 알람과 복합 알람의 차이점과 사용 사례

이 부분들을 공부하면, AWS에서 인프라 메트릭을 모니터링하고, 특정 조건이 충족될 때만 알람을 발생시키는 방법을 더 잘 이해할 수 있습니다.


# 151번
**정답: A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3. C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.**

**해설:**
- **AWS Control Tower:**
  - **데이터 거주성 가드레일:** Control Tower는 데이터 거주성 가드레일을 사용하여 특정 리전 외의 리전에 대한 접근을 차단하고, 인터넷 접근을 차단할 수 있습니다. 이는 컴플라이언스 요구 사항을 충족하는 데 도움이 됩니다.

- **AWS Organizations와 SCPs:**
  - **서비스 제어 정책 (SCPs):** SCPs를 사용하여 특정 리전 외의 리전에 대한 접근을 차단하고, VPC가 인터넷에 접근하지 못하도록 설정할 수 있습니다. 이는 중앙에서 관리할 수 있는 강력한 보안 정책을 제공합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. AWS WAF:** WAF는 웹 애플리케이션 방화벽으로, 인터넷 접근을 차단하는 데 적합하지 않습니다. 또한, AWS 계정 설정에서 리전 접근을 차단하는 기능이 없습니다.
- **D. 네트워크 ACL 및 IAM 정책:** 네트워크 ACL을 사용하여 인터넷 접근을 차단할 수 있지만, 이는 각 VPC마다 설정해야 하므로 관리 오버헤드가 큽니다. 또한, IAM 정책으로 리전 접근을 차단하는 것은 적절하지 않습니다.
- **E. AWS Config:** AWS Config는 리소스 변경을 감지하고 알림을 제공하는 데 유용하지만, 인터넷 접근을 차단하거나 리전 접근을 차단하는 데 직접적인 역할을 하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 특정 리전으로의 데이터 마이그레이션을 설정하고, 인터넷 접근을 차단하는 방법을 평가합니다. 특히, AWS Control Tower와 AWS Organizations의 SCPs를 사용하여 중앙에서 관리할 수 있는 보안 정책을 설정하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Control Tower:**
   - Control Tower의 기본 개념과 사용 사례
   - Control Tower를 사용하여 데이터 거주성 가드레일을 설정하는 방법

2. **AWS Organizations와 SCPs:**
   - AWS Organizations의 기본 개념과 사용 사례
   - SCPs를 사용하여 특정 리전 외의 리전에 대한 접근을 차단하고, 인터넷 접근을 차단하는 방법

이 부분들을 공부하면, AWS에서 특정 리전으로의 데이터 마이그레이션을 설정하고, 인터넷 접근을 차단하는 방법을 더 잘 이해할 수 있습니다.

# 152번
**정답: D. Create AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge (Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions. Configure the Lambda functions as event targets for the rules.**

**해설:**
- **AWS Lambda와 EventBridge:**
  - **자동화:** Lambda 함수를 사용하여 RDS 인스턴스를 시작하고 중지하는 작업을 자동화할 수 있습니다. 이는 수동 작업을 줄이고 운영 오버헤드를 최소화합니다.
  - **스케줄링:** EventBridge (이전의 CloudWatch Events)를 사용하여 정기적인 스케줄을 설정하고, 특정 시간에 Lambda 함수를 트리거하여 RDS 인스턴스를 시작하거나 중지할 수 있습니다. 이는 애플리케이션이 사용되지 않는 시간 동안 비용을 절감하는 데 도움이 됩니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. IAM 정책과 Systems Manager Session Manager:** 이 방법은 RDS 인스턴스를 자동으로 시작하고 중지하는 데 적합하지 않습니다.
- **B. ElastiCache for Redis:** ElastiCache를 사용하여 데이터를 캐싱하는 것은 가능하지만, RDS 인스턴스를 중지하는 동안 데이터를 캐싱하는 것은 복잡하고 비용 효율적이지 않습니다.
- **C. EC2 인스턴스와 cron job:** EC2 인스턴스를 사용하여 스케줄링 작업을 설정하는 것은 가능하지만, Lambda와 EventBridge를 사용하는 것이 더 간단하고 비용 효율적입니다.

**출제 의도:**
이 문제는 AWS에서 RDS 인스턴스를 자동으로 시작하고 중지하여 비용을 절감하는 방법을 평가합니다. 특히, AWS Lambda와 EventBridge를 사용하여 스케줄링 작업을 자동화하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Lambda:**
   - Lambda의 기본 개념과 사용 사례
   - Lambda를 사용하여 RDS 인스턴스를 시작하고 중지하는 방법

2. **Amazon EventBridge (CloudWatch Events):**
   - EventBridge의 기본 개념과 사용 사례
   - EventBridge를 사용하여 정기적인 스케줄을 설정하고, Lambda 함수를 트리거하는 방법

이 부분들을 공부하면, AWS에서 RDS 인스턴스를 자동으로 시작하고 중지하여 비용을 절감하는 방법을 더 잘 이해할 수 있습니다.



# 156번
**정답: A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs. E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.**

**해설:**
- **Amazon Athena와 Amazon QuickSight:**
  - **Athena:** Athena는 S3에 저장된 데이터를 SQL을 사용하여 쿼리할 수 있는 서버리스 쿼리 서비스입니다. 이는 일회성 쿼리에 적합하며, 운영 오버헤드가 적습니다.
  - **QuickSight:** QuickSight는 비즈니스 인텔리전스 도구로, Athena에서 쿼리한 데이터를 시각화하고 대시보드를 생성하는 데 사용할 수 있습니다. 이는 KPI를 표시하는 데 적합합니다.

- **AWS Lake Formation과 AWS Glue:**
  - **Lake Formation:** Lake Formation은 데이터 레이크를 쉽게 설정하고 관리할 수 있도록 도와줍니다. 블루프린트를 사용하여 데이터를 식별하고, 데이터 레이크에 적재할 수 있습니다.
  - **AWS Glue:** Glue는 ETL(추출, 변환, 로드) 작업을 자동화하는 관리형 서비스입니다. Glue 크롤러를 사용하여 소스 데이터를 크롤링하고, 데이터를 S3에 Apache Parquet 형식으로 적재할 수 있습니다. 이는 다양한 데이터 소스를 통합하고, 데이터를 분석하기 쉽게 만듭니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. Kinesis Data Analytics:** Kinesis Data Analytics는 실시간 데이터 스트림을 분석하는 데 적합하지만, 일회성 쿼리에는 적합하지 않습니다.
- **C. Lambda와 Redshift:** Lambda 함수를 사용하여 데이터를 Redshift로 이동하는 것은 가능하지만, 이는 운영 오버헤드가 크고, 요구 사항에 비해 복잡합니다.
- **D. Glue와 OpenSearch Service:** OpenSearch Service는 검색 및 분석 엔진으로, 데이터 레이크와 비즈니스 인텔리전스 도구로 사용하기에는 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 다양한 데이터 소스를 통합하고, 비즈니스 인텔리전스를 위해 데이터를 분석하는 방법을 평가합니다. 특히, Amazon Athena와 QuickSight를 사용하여 데이터를 쿼리하고 시각화하는 방법과, AWS Lake Formation과 Glue를 사용하여 데이터를 통합하고 적재하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon Athena와 QuickSight:**
   - Athena의 기본 개념과 사용 사례
   - QuickSight를 사용하여 데이터를 시각화하고 대시보드를 생성하는 방법

2. **AWS Lake Formation과 AWS Glue:**
   - Lake Formation의 기본 개념과 사용 사례
   - Glue를 사용하여 데이터를 크롤링하고, ETL 작업을 자동화하는 방법

이 부분들을 공부하면, AWS에서 다양한 데이터 소스를 통합하고, 비즈니스 인텔리전스를 위해 데이터를 분석하는 방법을 더 잘 이해할 수 있습니다.


# 157번
**정답: D. Configure an Amazon CloudWatch Logs export for the DB cluster. E. Use AWS Backup to take the backups and to keep the backups for 5 years.**

**해설:**
- **Amazon CloudWatch Logs Export:**
  - **감사 로그 보관:** CloudWatch Logs를 사용하여 데이터베이스에서 수행된 작업의 감사 로그를 무기한으로 보관할 수 있습니다. 이는 감사 로그를 영구적으로 보관해야 하는 요구 사항을 충족합니다.

- **AWS Backup:**
  - **백업 보존:** AWS Backup을 사용하여 Aurora DB 클러스터의 백업을 자동으로 생성하고, 백업 보존 기간을 5년으로 설정할 수 있습니다. 이는 데이터를 5년 동안 보관하고, 5년 후에 삭제해야 하는 요구 사항을 충족합니다.
  - **자동화:** AWS Backup은 백업 작업을 자동화하여 운영 오버헤드를 줄이고, 백업 보존 정책을 쉽게 관리할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Manual Snapshot:** 수동 스냅샷은 백업을 수동으로 관리해야 하므로, 운영 오버헤드가 증가합니다. 또한, 수동 스냅샷은 자동으로 삭제되지 않으므로, 5년 후에 데이터를 자동으로 삭제하는 요구 사항을 충족하지 않습니다.
- **B. Lifecycle Policy for Automated Backups:** Aurora는 기본적으로 자동 백업에 대한 수명 주기 정책을 지원하지 않습니다.
- **C. Automated Backup Retention:** Aurora의 자동 백업 보존 기간은 최대 35일로 제한되므로, 5년 동안 데이터를 보관하는 요구 사항을 충족하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 데이터베이스 백업과 감사 로그를 관리하고, 특정 보존 기간 동안 데이터를 안전하게 보관하는 방법을 평가합니다. 특히, AWS Backup과 CloudWatch Logs를 사용하여 데이터를 자동으로 백업하고, 감사 로그를 영구적으로 보관하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Backup:**
   - AWS Backup의 기본 개념과 사용 사례
   - AWS Backup을 사용하여 데이터베이스 백업을 자동화하고, 보존 기간을 설정하는 방법

2. **Amazon CloudWatch Logs:**
   - CloudWatch Logs의 기본 개념과 사용 사례
   - CloudWatch Logs를 사용하여 데이터베이스 감사 로그를 무기한으로 보관하는 방법

이 부분들을 공부하면, AWS에서 데이터베이스 백업과 감사 로그를 관리하고, 특정 보존 기간 동안 데이터를 안전하게 보관하는 방법을 더 잘 이해할 수 있습니다.


# 162번
**정답: A. Amazon FSx for Lustre integrated with Amazon S3**

**해설:**
- **Amazon FSx for Lustre:**
  - **고성능 파일 시스템:** FSx for Lustre는 고성능 컴퓨팅(HPC) 워크로드에 최적화된 파일 시스템으로, 높은 처리량과 낮은 지연 시간을 제공합니다. 이는 금융 리스크 모델링과 같은 HPC 워크로드에 적합합니다.
  - **S3 통합:** FSx for Lustre는 Amazon S3와 통합되어, 데이터를 S3에 저장하고 필요할 때 FSx for Lustre 파일 시스템으로 가져올 수 있습니다. 이는 장기 보관 및 분석을 위한 데이터를 효율적으로 관리할 수 있게 합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. Amazon FSx for Windows File Server:** FSx for Windows File Server는 Windows 기반 워크로드에 최적화되어 있으며, Linux 기반 HPC 워크로드에는 적합하지 않습니다.
- **C. Amazon S3 Glacier와 EBS:** S3 Glacier는 장기 보관을 위한 저비용 스토리지로, 고성능 파일 시스템을 제공하지 않습니다. EBS는 블록 스토리지로, 파일 시스템으로 사용하기에는 적합하지 않습니다.
- **D. S3와 EBS:** S3와 EBS를 통합하여 사용할 수 있지만, 이는 고성능 파일 시스템을 제공하지 않으며, HPC 워크로드에 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 고성능 컴퓨팅(HPC) 워크로드를 위한 스토리지 솔루션을 선택하는 방법을 평가합니다. 특히, Amazon FSx for Lustre와 Amazon S3를 사용하여 고성능 파일 시스템과 장기 보관 스토리지를 통합하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon FSx for Lustre:**
   - FSx for Lustre의 기본 개념과 사용 사례
   - FSx for Lustre를 사용하여 고성능 컴퓨팅 워크로드를 처리하는 방법

2. **Amazon S3:**
   - S3의 기본 개념과 사용 사례
   - S3를 사용하여 데이터를 장기 보관하고, FSx for Lustre와 통합하는 방법

이 부분들을 공부하면, AWS에서 고성능 컴퓨팅(HPC) 워크로드를 위한 스토리지 솔루션을 선택하는 방법을 더 잘 이해할 수 있습니다.


# 179번
**정답: A. Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance.**

**해설:**
- **IAM Role 및 KMS 통합:**
  - **IAM Role:** EC2 인스턴스에 IAM 역할을 할당하여, 해당 역할이 Systems Manager Parameter Store의 매개변수에 대한 읽기 권한을 가지도록 설정합니다. 이는 보안 모범 사례에 부합하며, EC2 인스턴스가 필요한 권한만 가지도록 제한할 수 있습니다.
  - **KMS 키:** 매개변수를 암호화하는 데 사용된 AWS KMS 키에 대한 복호화 권한을 IAM 역할에 부여합니다. 이는 매개변수가 암호화된 상태로 저장되고, 필요한 경우에만 복호화할 수 있도록 보장합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. IAM Policy:** IAM 정책을 EC2 인스턴스에 직접 할당하는 것은 보안 모범 사례에 부합하지 않습니다. 대신 IAM 역할을 사용하여 권한을 부여하는 것이 더 안전하고 관리하기 쉽습니다.
- **C. IAM Trust Relationship:** Parameter Store와 EC2 인스턴스 간의 신뢰 관계를 설정하는 것은 적절한 접근 방식이 아닙니다. 또한, Amazon RDS를 주체로 지정하는 것은 이 시나리오와 관련이 없습니다.
- **D. IAM Trust Relationship:** DB 인스턴스와 EC2 인스턴스 간의 신뢰 관계를 설정하는 것은 적절한 접근 방식이 아닙니다. 또한, Systems Manager를 주체로 지정하는 것은 이 시나리오와 관련이 없습니다.

**출제 의도:**
이 문제는 AWS에서 보안 모범 사례를 따르면서 애플리케이션이 데이터베이스 자격 증명을 안전하게 저장하고 접근할 수 있도록 설정하는 방법을 평가합니다. 특히, IAM 역할과 KMS 키를 사용하여 Systems Manager Parameter Store의 매개변수를 안전하게 관리하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Systems Manager Parameter Store:**
   - Parameter Store의 기본 개념과 사용 사례
   - Parameter Store를 사용하여 애플리케이션 자격 증명을 안전하게 저장하고 관리하는 방법

2. **IAM Roles and Policies:**
   - IAM 역할과 정책의 기본 개념과 사용 사례
   - IAM 역할을 사용하여 EC2 인스턴스에 권한을 부여하는 방법

3. **AWS Key Management Service (KMS):**
   - KMS의 기본 개념과 사용 사례
   - KMS 키를 사용하여 데이터를 


# 184번
**정답: A. Configure the Lambda function to run in the VPC with the appropriate security group.**

**해설:**
- **Lambda 함수 VPC 설정:**
  - **VPC 내 실행:** Lambda 함수를 VPC 내에서 실행하도록 설정하면, VPC의 리소스에 접근할 수 있습니다. 이는 Lambda 함수가 VPC 내의 서브넷과 보안 그룹을 사용하여 네트워크 트래픽을 제어할 수 있게 합니다.
  - **보안 그룹:** 적절한 보안 그룹을 설정하여 Lambda 함수가 온프레미스 데이터 센터의 데이터베이스에 접근할 수 있도록 허용합니다. 보안 그룹은 인바운드 및 아웃바운드 트래픽을 제어합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. VPN 연결 설정:** 이미 Direct Connect를 통해 온프레미스 데이터 센터에 접근할 수 있으므로, 추가적인 VPN 연결을 설정할 필요가 없습니다.
- **C. 라우트 테이블 업데이트:** 라우트 테이블을 업데이트하는 것만으로는 Lambda 함수가 VPC 내에서 실행되지 않으면 온프레미스 데이터 센터에 접근할 수 없습니다.
- **D. Elastic IP 주소 사용:** Elastic IP 주소를 사용하여 트래픽을 라우팅하는 것은 적절한 방법이 아닙니다. Lambda 함수는 ENI(Elastic Network Interface)를 통해 VPC 내에서 실행되어야 합니다.

**출제 의도:**
이 문제는 AWS Lambda 함수를 VPC 내에서 실행하여 온프레미스 데이터 센터의 리소스에 접근할 수 있도록 설정하는 방법을 평가합니다. 특히, Lambda 함수를 VPC 내에서 실행하고, 적절한 보안 그룹을 설정하여 네트워크 트래픽을 제어하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Lambda와 VPC 통합:**
   - Lambda 함수를 VPC 내에서 실행하는 방법
   - Lambda 함수에 VPC 서브넷과 보안 그룹을 할당하는 방법

2. **AWS Direct Connect:**
   - Direct Connect의 기본 개념과 사용 사례
   - Direct Connect를 사용하여 온프레미스 데이터 센터와 AWS 간의 네트워크 연결을 설정하는 방법

이 부분들을 공부하면, AWS Lambda 함수를 VPC 내에서 실행하여 온프레미스 데이터 센터의 리소스에 접근할 수 있도록 설정하는 방법을 더 잘 이해할 수 있습니다.


# 186번
- 틀리지는 않았으나 FSx, Storage Gateway 차이점을 알 수 있음

**정답: B. Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system to each Windows instance.**

**해설:**
- **Amazon FSx for Windows File Server:**
  - **Windows 네이티브 파일 시스템:** Amazon FSx for Windows File Server는 완전 관리형 네이티브 Windows 파일 시스템을 제공하여, Windows 기반 애플리케이션에 적합합니다.
  - **다중 AZ 지원:** FSx for Windows File Server는 다중 가용 영역(AZ)에 걸쳐 배포할 수 있어, 고가용성과 내구성을 제공합니다.
  - **SMB 프로토콜 지원:** FSx for Windows File Server는 SMB 프로토콜을 지원하여, 여러 EC2 Windows 인스턴스에서 파일 시스템을 공유할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. AWS Storage Gateway:** Storage Gateway는 온프레미스와 AWS 간의 하이브리드 클라우드 스토리지 솔루션으로, 다중 AZ에 걸친 Windows 파일 시스템을 제공하는 데 적합하지 않습니다.
- **C. Amazon EFS:** EFS는 Linux 기반 파일 시스템으로, Windows 기반 애플리케이션에는 적합하지 않습니다.
- **D. Amazon EBS:** EBS 볼륨은 단일 EC2 인스턴스에만 연결할 수 있으며, 여러 인스턴스에서 동시에 액세스할 수 없습니다.

**출제 의도:**
이 문제는 AWS에서 Windows 기반 애플리케이션을 마이그레이션하고, 다중 AZ에 걸쳐 여러 EC2 Windows 인스턴스에서 공유할 수 있는 파일 시스템을 설정하는 방법을 평가합니다. 특히, Amazon FSx for Windows File Server를 사용하여 Windows 네이티브 파일 시스템을 제공하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon FSx for Windows File Server:**
   - FSx for Windows File Server의 기본 개념과 사용 사례
   - FSx for Windows File Server를 사용하여 Windows 기반 애플리케이션에 적합한 파일 시스템을 설정하는 방법

2. **AWS Storage Gateway:**
   - Storage Gateway의 기본 개념과 사용 사례
   - Storage Gateway를 사용하여 온프레미스와 AWS 간의 하이브리드 클라우드 스토리지를 설정하는 방법

3. **Amazon Elastic File System (EFS):**
   - EFS의 기본 개념과 사용 사례
   - EFS를 사용하여 Linux 기반 파일 시스템을 설정하는 방법

4. **Amazon Elastic Block Store (EBS):**
   - EBS의 기본 개념과 사용 사례
   - EBS 볼륨을 사용하여 단일 EC2 인스턴스에 스토리지를 제공하는 방법

이 부분들을 공부하면, AWS에서 Windows 기반 애플리케이션을 마이그레이션하고, 다중 AZ에 걸쳐 여러 EC2 Windows 인스턴스에서 공유할 수 있는 파일 시스템을 설정하는 방법을 더 잘 이해할 수 있습니다.

# 200번
### 정답
D. Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.

### 해설
Amazon Cognito를 이미 사용하여 사용자를 관리하고 있으므로, API Gateway에서 Cognito 사용자 풀(authorizer)을 사용하여 각 요청을 검증하는 것이 가장 적합한 솔루션입니다. 이는 AWS 관리형 솔루션으로, 개발 노력을 줄이고 운영 오버헤드를 최소화할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user made the request.**
  - Lambda 함수를 사용하여 요청을 검증하는 것은 가능하지만, 이는 추가적인 개발 노력과 운영 오버헤드를 발생시킵니다.

- **B. For each user, create and assign an API key that must be sent with each request. Validate the key by using an AWS Lambda function.**
  - 각 사용자에 대해 API 키를 생성하고 검증하는 것은 복잡하며, 추가적인 개발 노력과 운영 오버헤드를 발생시킵니다.

- **C. Send the user’s email address in the header with every request. Invoke an AWS Lambda function to validate that the user with that email address has proper access.**
  - 이메일 주소를 헤더에 포함시키고 Lambda 함수를 사용하여 검증하는 것은 보안상 적절하지 않으며, 추가적인 개발 노력과 운영 오버헤드를 발생시킵니다.

- **D. Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.**
  - Cognito 사용자 풀(authorizer)을 사용하면 AWS 관리형 솔루션으로, 개발 노력을 줄이고 운영 오버헤드를 최소화할 수 있습니다. Cognito는 이미 사용자를 관리하고 있으므로, 이를 활용하여 요청을 검증하는 것이 가장 적합합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **Amazon Cognito**
   - Cognito는 사용자 인증, 권한 부여 및 사용자 관리 기능을 제공하는 서비스입니다.
   - 사용자 풀을 사용하여 사용자 인증을 관리할 수 있습니다.

2. **Amazon API Gateway**
   - API Gateway는 RESTful API를 생성, 배포 및 관리할 수 있는 서비스입니다.
   - Cognito 사용자 풀(authorizer)을 사용하여 요청을 검증할 수 있습니다.

3. **AWS Lambda**
   - Lambda는 서버리스 컴퓨팅을 제공하는 서비스로, 특정 이벤트에 따라 코드를 실행할 수 있습니다.
   - 요청을 검증하는 데 사용할 수 있지만, 추가적인 개발 노력과 운영 오버헤드를 발생시킵니다.

### 추가 학습 자료
- [Amazon Cognito 공식 문서](https://docs.aws.amazon.com/ko_kr/cognito/latest/developerguide/what-is-amazon-cognito.html)
- [Amazon API Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/apigateway/latest/developerguide/welcome.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)

이 자료들을 통해 Cognito, API Gateway, Lambda의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 201번
**정답: B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.**

**해설:**
- **Amazon Pinpoint:**
  - **SMS 메시지 전송:** Amazon Pinpoint는 마케팅 커뮤니케이션 서비스로, SMS 메시지를 전송하는 데 적합합니다. Pinpoint는 사용자에게 SMS 메시지를 보내고, 사용자의 응답을 처리할 수 있습니다.
  - **이벤트 전송:** Pinpoint는 이벤트를 Amazon Kinesis 데이터 스트림으로 전송하여, 실시간으로 데이터를 분석하고 아카이빙할 수 있습니다. 이는 사용자의 응답을 1년 동안 저장하고 분석하는 요구 사항을 충족합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Amazon Connect:** Amazon Connect는 주로 콜 센터 솔루션으로 사용되며, SMS 메시지 전송에 적합하지 않습니다.
- **C. Amazon SQS:** SQS는 메시지 큐잉 서비스로, SMS 메시지 전송에 적합하지 않습니다. 또한, SQS는 메시지의 장기 저장을 제공하지 않습니다.
- **D. Amazon SNS FIFO:** SNS는 주로 알림 전송에 사용되며, SMS 메시지 전송에 적합하지 않습니다. 또한, SNS FIFO 주제는 메시지 순서를 보장하지만, SMS 메시지 전송과 응답 처리에는 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 SMS 메시지를 전송하고, 사용자의 응답을 저장하고 분석하는 솔루션을 설계하는 방법을 평가합니다. 특히, Amazon Pinpoint를 사용하여 SMS 메시지를 전송하고, Kinesis 데이터 스트림을 사용하여 데이터를 실시간으로 분석하고 아카이빙하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon Pinpoint:**
   - Pinpoint의 기본 개념과 사용 사례
   - Pinpoint를 사용하여 SMS 메시지를 전송하고, 사용자의 응답을 처리하는 방법

2. **Amazon Kinesis:**
   - Kinesis 데이터 스트림의 기본 개념과 사용 사례
   - Kinesis를 사용하여 데이터를 실시간으로 분석하고 아카이빙하는 방법

이 부분들을 공부하면, AWS에서 SMS 메시지를 전송하고, 사용자의 응답을 저장하고 분석하는 솔루션을 설계하는 방법을 더 잘 이해할 수 있습니다.

# 203번
**정답: D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SQS queue.**

**해설:**
- **Auto Scaling Group:**
  - **자동 확장:** Auto Scaling 그룹을 사용하면 애플리케이션 인스턴스 수를 자동으로 조정할 수 있습니다. SQS 큐의 깊이에 따라 인스턴스를 확장하면, 대기 중인 메시지를 더 빠르게 처리할 수 있습니다.
  - **큐 기반 확장:** SQS 큐의 깊이를 모니터링하고, 큐에 쌓인 메시지 수에 따라 인스턴스를 확장하면, 고객의 미팅 초대장이 지연 없이 발송될 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. DynamoDB Accelerator (DAX):** DAX는 DynamoDB의 읽기 성능을 향상시키는 데 사용되며, 현재 문제는 미팅 초대장 발송 지연이므로 적합하지 않습니다.
- **B. Amazon API Gateway:** API Gateway는 웹 애플리케이션 앞에 API를 제공하는 데 사용되며, 현재 문제는 미팅 초대장 발송 지연이므로 적합하지 않습니다.
- **C. Amazon CloudFront:** CloudFront는 콘텐츠 전송 네트워크(CDN)로, 웹 애플리케이션의 성능을 향상시키는 데 사용됩니다. 현재 문제는 미팅 초대장 발송 지연이므로 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 SQS 큐를 사용하여 메시지를 처리하는 애플리케이션의 성능을 향상시키는 방법을 평가합니다. 특히, Auto Scaling 그룹을 사용하여 SQS 큐의 깊이에 따라 애플리케이션 인스턴스를 자동으로 확장하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon SQS:**
   - SQS의 기본 개념과 사용 사례
   - SQS 큐의 깊이를 모니터링하고, 큐 기반 확장을 설정하는 방법

2. **Auto Scaling Group:**
   - Auto Scaling 그룹의 기본 개념과 사용 사례
   - Auto Scaling 그룹을 사용하여 애플리케이션 인스턴스를 자동으로 확장하는 방법

이 부분들을 공부하면, AWS에서 SQS 큐를 사용하여 메시지를 처리하는 애플리케이션의 성능을 향상시키는 방법을 더 잘 이해할 수 있습니다.

# 204번
**정답: C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.**

**해설:**
- **AWS Lake Formation:**
  - **데이터 레이크 관리:** Lake Formation은 데이터 레이크를 쉽게 설정하고 관리할 수 있는 서비스입니다. S3에 저장된 데이터를 등록하고, 다양한 데이터 소스와 통합할 수 있습니다.
  - **세분화된 권한 관리:** Lake Formation은 세분화된 데이터 액세스 제어를 제공하여, 다양한 팀이 필요한 데이터에만 접근할 수 있도록 설정할 수 있습니다. 이는 데이터 보안을 강화하고, 운영 오버헤드를 최소화합니다.
  - **AWS Glue 통합:** AWS Glue를 사용하여 RDS와의 JDBC 연결을 설정하고, 데이터를 크롤링하여 카탈로그에 등록할 수 있습니다. 이를 통해 데이터에 대한 통합된 뷰를 제공할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Amazon RDS:** 모든 데이터를 RDS로 마이그레이션하는 것은 비효율적이며, RDS의 비용과 성능 문제를 초래할 수 있습니다. 또한, RDS는 데이터 레이크 기능을 제공하지 않습니다.
- **B. AWS Lambda와 Athena:** Lambda를 사용하여 데이터를 복사하고, Athena를 사용하여 데이터를 쿼리하는 것은 가능하지만, 세분화된 권한 관리를 제공하는 데 한계가 있습니다.
- **D. Amazon Redshift:** Redshift는 데이터 웨어하우스 솔루션으로, 대규모 데이터 분석에 적합하지만, 데이터 레이크 기능을 제공하지 않습니다. 또한, Redshift로 데이터를 복사하는 것은 운영 오버헤드를 증가시킬 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 대규모 데이터를 관리하고, 다양한 팀이 데이터를 분석할 수 있도록 세분화된 권한 관리를 제공하는 솔루션을 설계하는 방법을 평가합니다. 특히, AWS Lake Formation을 사용하여 데이터 레이크를 설정하고, 세분화된 권한 관리를 제공하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Lake Formation:**
   - Lake Formation의 기본 개념과 사용 사례
   - Lake Formation을 사용하여 데이터 레이크를 설정하고, 세분화된 권한 관리를 제공하는 방법

2. **AWS Glue:**
   - Glue의 기본 개념과 사용 사례
   - Glue를 사용하여 다양한 데이터 소스와 통합하고, 데이터를 크롤링하여 카탈로그에 등록하는 방법

이 부분들을 공부하면, AWS에서 대규모 데이터를 관리하고, 다양한 팀이 데이터를 분석할 수 있도록 세분화된 권한 관리를 제공하는 솔루션을 설계하는 방법을 더 잘 이해할 수 있습니다.

# 205번
**정답: C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.**

**해설:**
- **Amazon S3와 CloudFront:**
  - **비용 효율성:** S3는 정적 콘텐츠를 호스팅하는 데 매우 비용 효율적입니다. S3 버킷을 사용하면 서버를 관리할 필요가 없으며, 스토리지 비용만 지불하면 됩니다.
  - **내구성 및 가용성:** S3는 높은 내구성과 가용성을 제공하여, 웹사이트 콘텐츠를 안전하게 저장하고 제공할 수 있습니다.
  - **CloudFront 통합:** CloudFront와 S3를 통합하면, 전 세계 사용자에게 빠르고 안정적으로 콘텐츠를 제공할 수 있습니다. OAI를 사용하여 S3 버킷에 대한 액세스를 제어할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Amazon Lightsail:** Lightsail은 가상 서버를 제공하지만, S3를 사용하는 것보다 비용 효율적이지 않습니다. 또한, 서버를 관리해야 하는 오버헤드가 있습니다.
- **B. AWS Auto Scaling과 EC2:** EC2 인스턴스를 사용하고 Auto Scaling 그룹을 설정하는 것은 정적 웹사이트 호스팅에 비해 과도한 솔루션입니다. 비용이 많이 들고, 관리 오버헤드가 큽니다.
- **D. Public S3 bucket과 AWS Transfer for SFTP:** S3 버킷을 공개로 설정하는 것은 보안 위험이 있습니다. 또한, AWS Transfer for SFTP를 사용하는 것은 추가 비용이 발생할 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 정적 웹사이트를 호스팅하고, 비용 효율적이고 내구성 있는 솔루션을 설계하는 방법을 평가합니다. 특히, Amazon S3와 CloudFront를 사용하여 정적 콘텐츠를 호스팅하고, OAI를 사용하여 S3 버킷에 대한 액세스를 제어하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon S3:**
   - S3의 기본 개념과 사용 사례
   - S3 버킷을 사용하여 정적 웹사이트를 호스팅하는 방법
   - S3 버킷 정책과 OAI를 사용하여 액세스를 제어하는 방법

2. **Amazon CloudFront:**
   - CloudFront의 기본 개념과 사용 사례
   - CloudFront를 사용하여 전 세계 사용자에게 콘텐츠를 빠르고 안정적으로 제공하는 방법

이 부분들을 공부하면, AWS에서 정적 웹사이트를 호스팅하고, 비용 효율적이고 내구성 있는 솔루션을 설계하는 방법을 더 잘 이해할 수 있습니다.

# 209번
**정답: A. Use Amazon ElastiCache to manage and store session data.**

**해설:**
- **Amazon ElastiCache:**
  - **분산 세션 관리:** ElastiCache는 Redis 또는 Memcached를 사용하여 세션 데이터를 관리하고 저장할 수 있습니다. 이는 분산 환경에서 세션 데이터를 중앙 집중식으로 관리할 수 있게 합니다.
  - **고성능:** ElastiCache는 인메모리 데이터 스토어로, 세션 데이터를 빠르게 읽고 쓸 수 있어 성능이 뛰어납니다.
  - **확장성:** ElastiCache는 자동으로 확장할 수 있어, EC2 인스턴스가 자주 스케일링되는 환경에서도 안정적으로 세션 데이터를 관리할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. Session Affinity (Sticky Sessions):** ALB의 세션 어피니티(스티키 세션)는 세션 데이터를 특정 인스턴스에 고정시키는 방식으로, 인스턴스가 자주 스케일링되는 환경에서는 적합하지 않습니다. 또한, 인스턴스가 종료되면 세션 데이터가 손실될 수 있습니다.
- **C. Session Manager from AWS Systems Manager:** Session Manager는 EC2 인스턴스에 대한 원격 접속을 관리하는 도구로, 세션 데이터 관리를 위한 도구가 아닙니다.
- **D. GetSessionToken API in AWS STS:** GetSessionToken API는 임시 보안 자격 증명을 생성하는 데 사용되며, 세션 데이터 관리를 위한 도구가 아닙니다.

**출제 의도:**
이 문제는 AWS에서 분산 세션 데이터를 관리하고, 자동으로 확장되는 환경에서 안정적으로 세션 데이터를 관리하는 방법을 평가합니다. 특히, Amazon ElastiCache를 사용하여 세션 데이터를 중앙 집중식으로 관리하고, 성능과 확장성을 보장하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon ElastiCache:**
   - ElastiCache의 기본 개념과 사용 사례
   - ElastiCache를 사용하여 세션 데이터를 관리하고 저장하는 방법

2. **Application Load Balancer (ALB):**
   - ALB의 기본 개념과 사용 사례
   - ALB의 세션 어피니티(스티키 세션) 기능과 그 한계

이 부분들을 공부하면, AWS에서 분산 세션 데이터를 관리하고, 자동으로 확장되는 환경에서 안정적으로 세션 데이터를 관리하는 방법을 더 잘 이해할 수 있습니다.


# 215번
**정답: A. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.**

**해설:**
- **AWS Snowball:**
  - **대용량 데이터 전송:** AWS Snowball는 대용량 데이터를 물리적으로 전송하는 데 적합한 솔루션입니다. 700 TB의 데이터를 1개월 내에 전송해야 하는 경우, 인터넷 연결을 통한 전송은 시간이 많이 걸릴 수 있습니다.
  - **비용 효율성:** Snowball을 사용하면 대규모 데이터를 빠르고 안전하게 전송할 수 있으며, 인터넷 대역폭을 사용하지 않으므로 비용 효율적입니다.
  - **S3 Glacier Deep Archive:** 데이터를 S3로 전송한 후, 수명 주기 정책을 사용하여 데이터를 S3 Glacier Deep Archive로 전환하면, 장기 보관 비용을 최소화할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. VPN과 AWS CLI:** VPN 연결을 통해 데이터를 전송하는 것은 500 Mbps의 대역폭으로는 700 TB의 데이터를 1개월 내에 전송하기 어렵습니다.
- **C. AWS Direct Connect:** Direct Connect를 통해 데이터를 전송하는 것은 가능하지만, 500 Mbps의 대역폭으로는 700 TB의 데이터를 1개월 내에 전송하기 어렵습니다. 또한, Direct Connect는 설정과 비용 측면에서 더 복잡할 수 있습니다.
- **D. AWS DataSync:** DataSync를 사용하여 데이터를 전송하는 것은 가능하지만, 500 Mbps의 대역폭으로는 700 TB의 데이터를 1개월 내에 전송하기 어렵습니다.

**출제 의도:**
이 문제는 AWS에서 대규모 데이터를 효율적으로 전송하고, 장기 보관 비용을 최소화하는 방법을 평가합니다. 특히, AWS Snowball을 사용하여 대규모 데이터를 빠르고 안전하게 전송하고, S3 Glacier Deep Archive를 사용하여 장기 보관 비용을 최소화하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Snowball:**
   - Snowball의 기본 개념과 사용 사례
   - Snowball을 사용하여 대규모 데이터를 전송하는 방법

2. **Amazon S3 Glacier Deep Archive:**
   - S3 Glacier Deep Archive의 기본 개념과 사용 사례
   - 수명 주기 정책을 사용하여 데이터를 S3 Glacier Deep Archive로 전환하는 방법

이 부분들을 공부하면, AWS에서 대규모 데이터를 효율적으로 전송하고, 장기 보관 비용을 최소화하는 방법을 더 잘 이해할 수 있습니다.


# 217번
**정답: A. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.**

**해설:**
- **Active-Passive Failover:**
  - **Route 53 Active-Passive Failover:** Route 53을 사용하여 활성-수동 페일오버를 구성하면, 기본 인프라가 정상일 때는 트래픽을 기본 리전으로 라우팅하고, 장애가 발생하면 대기 리전으로 트래픽을 전환할 수 있습니다.
  - **Aurora Replica:** Aurora 복제본을 두 번째 AWS 리전에 생성하면, 데이터베이스의 복구 시간을 최소화할 수 있습니다. Aurora 복제본은 기본 인스턴스의 데이터를 실시간으로 복제하므로, 데이터 손실을 최소화할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. Active-Active Failover:** 활성-활성 페일오버는 두 리전 모두에서 트래픽을 처리할 수 있도록 설정하는 방식으로, 현재 요구 사항(기본 인프라가 정상일 때는 대기 리전이 로드를 처리할 필요 없음)과 맞지 않습니다.
- **C. Snapshot 복구:** 스냅샷에서 복구된 Aurora 데이터베이스를 사용하는 것은 데이터 복구 시간이 길어질 수 있으며, 30분의 다운타임 요구 사항을 충족하지 못할 수 있습니다.
- **D. AWS Backup:** AWS Backup을 사용하여 데이터를 백업하고, 이를 사용하여 두 번째 리전에 인프라를 생성하는 것은 데이터 복구 시간이 길어질 수 있으며, 30분의 다운타임 요구 사항을 충족하지 못할 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 재해 복구 솔루션을 설계하고, 특정 다운타임 및 데이터 손실 허용 범위 내에서 요구 사항을 충족하는 방법을 평가합니다. 특히, Route 53을 사용하여 활성-수동 페일오버를 구성하고, Aurora 복제본을 사용하여 데이터베이스 복구 시간을 최소화하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon Route 53:**
   - Route 53의 기본 개념과 사용 사례
   - Route 53을 사용하여 활성-수동 페일오버를 구성하는 방법

2. **Amazon Aurora:**
   - Aurora의 기본 개념과 사용 사례
   - Aurora 복제본을 생성하고, 이를 사용하여 데이터베이스 복구 시간을 최소화하는 방법

이 부분들을 공부하면, AWS에서 재해 복구 솔루션을 설계하고, 특정 다운타임 및 데이터 손실 허용 범위 내에서 요구 사항을 충족하는 방법을 더 잘 이해할 수 있습니다.


# 219번
**정답: D. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.**

**해설:**
- **R5 EC2 인스턴스:**
  - **메모리 최적화:** R5 인스턴스는 메모리 최적화 인스턴스로, 메모리 집약적인 작업을 수행하는 애플리케이션에 적합합니다. 현재 애플리케이션이 메모리 집약적인 작업을 수행하고 있으므로, R5 인스턴스로 교체하면 성능이 향상될 수 있습니다.
  
- **CloudWatch 에이전트:**
  - **커스텀 메트릭:** CloudWatch 에이전트를 사용하여 애플리케이션 지연 시간과 같은 커스텀 메트릭을 생성하면, 애플리케이션 성능을 모니터링하고 향후 용량 계획에 도움이 됩니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. T3 인스턴스:** T3 인스턴스는 버스트 가능한 성능을 제공하지만, 지속적인 고성능이 필요한 메모리 집약적인 작업에는 적합하지 않습니다.
- **B. Auto Scaling 그룹:** Auto Scaling 그룹을 사용하여 인스턴스를 확장하는 것은 가능하지만, 상태 저장 애플리케이션의 경우 상태를 유지하는 데 어려움이 있을 수 있습니다. 또한, 메모리 최적화 인스턴스로 교체하는 것이 더 효과적입니다.
- **C. CloudWatch 메모리 메트릭:** CloudWatch의 기본 메모리 메트릭을 사용하는 것은 가능하지만, 애플리케이션 지연 시간과 같은 커스텀 메트릭을 생성하는 것이 더 유용합니다.

**출제 의도:**
이 문제는 AWS에서 메모리 집약적인 애플리케이션의 성능 문제를 해결하고, 운영 효율성을 극대화하는 방법을 평가합니다. 특히, 메모리 최적화 인스턴스로 교체하고, CloudWatch 에이전트를 사용하여 커스텀 메트릭을 생성하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **EC2 인스턴스 유형:**
   - M5, T3, R5 인스턴스의 기본 개념과 사용 사례
   - 메모리 최적화 인스턴스(R5)를 사용하여 메모리 집약적인 작업을 수행하는 방법

2. **Amazon CloudWatch:**
   - CloudWatch의 기본 개념과 사용 사례
   - CloudWatch 에이전트를 사용하여 커스텀 메트릭을 생성하고, 애플리케이션 성능을 모니터링하는 방법

이 부분들을 공부하면, AWS에서 메모리 집약적인 애플리케이션의 성능 문제를 해결하고, 운영 효율성을 극대화하는 방법을 더 잘 이해할 수 있습니다.

# 224번
**정답: C. Create an Amazon Route 53 multivalue answer routing policy. E. Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.**

**해설:**
- **Amazon Route 53 Multivalue Answer Routing Policy:**
  - **C. Multivalue Answer Routing Policy:** 이 라우팅 정책은 여러 IP 주소를 반환하여 트래픽을 무작위로 분산시킬 수 있습니다. 이는 모든 실행 중인 EC2 인스턴스에 트래픽을 무작위로 분산시키는 요구 사항을 충족합니다.

- **다중 가용 영역 배포:**
  - **E. Launch four EC2 instances:** 두 개의 가용 영역에 각각 두 개의 EC2 인스턴스를 배포하면, 고가용성과 장애 내성을 보장할 수 있습니다. 이는 하나의 가용 영역에 장애가 발생하더라도 다른 가용 영역의 인스턴스가 트래픽을 처리할 수 있도록 합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Failover Routing Policy:** 이 라우팅 정책은 주로 장애 조치(failover)를 위해 사용되며, 트래픽을 무작위로 분산시키는 데 적합하지 않습니다.
- **B. Weighted Routing Policy:** 가중치 라우팅 정책은 트래픽을 특정 비율로 분산시키는 데 사용되며, 무작위 분산을 보장하지 않습니다.
- **D. Launch three EC2 instances:** 세 개의 인스턴스를 배포하는 것은 고가용성과 장애 내성을 보장하기에 충분하지 않을 수 있습니다. 두 개의 가용 영역에 각각 두 개의 인스턴스를 배포하는 것이 더 적합합니다.

**출제 의도:**
이 문제는 AWS에서 웹 애플리케이션의 아키텍처를 고가용성과 장애 내성을 갖추도록 설계하는 방법을 평가합니다. 특히, Route 53의 Multivalue Answer Routing Policy를 사용하여 트래픽을 무작위로 분산시키고, 다중 가용 영역에 인스턴스를 배포하여 고가용성을 보장하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon Route 53 Routing Policies:**
   - Route 53의 다양한 라우팅 정책(예: Multivalue Answer, Weighted, Failover)의 기본 개념과 사용 사례
   - Multivalue Answer Routing Policy를 사용하여 트래픽을 무작위로 분산시키는 방법

2. **고가용성과 장애 내성:**
   - 다중 가용 영역에 인스턴스를 배포하여 고가용성과 장애 내성을 보장하는 방법
   - AWS에서 고가용성과 장애 내성을 갖춘 아키텍처를 설계하는 모범 사례

이 부분들을 공부하면, AWS에서 웹 애플리케이션의 아키텍처를 고가용성과 장애 내성을 갖추도록 설계하는 방법을 더 잘 이해할 수 있습니다.


# 230번
**정답: C. Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.**

**해설:**
- **NAT 게이트웨이:**
  - **고가용성 및 장애 내성:** NAT 게이트웨이는 AWS에서 관리되는 서비스로, 고가용성과 장애 내성을 제공합니다. 서로 다른 가용 영역에 NAT 게이트웨이를 배포하면, 하나의 가용 영역에 장애가 발생하더라도 다른 가용 영역의 NAT 게이트웨이가 트래픽을 처리할 수 있습니다.
  - **자동 확장:** NAT 게이트웨이는 자동으로 확장되어 트래픽 증가를 처리할 수 있습니다. 이는 수동으로 인스턴스를 관리할 필요가 없으므로 운영 오버헤드를 줄여줍니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. 동일한 가용 영역에 두 개의 NAT 게이트웨이:** 동일한 가용 영역에 두 개의 NAT 게이트웨이를 배포하는 것은 고가용성과 장애 내성을 제공하지 않습니다. 하나의 가용 영역에 장애가 발생하면 두 개의 NAT 게이트웨이 모두 사용할 수 없게 됩니다.
- **B. Auto Scaling 그룹과 Network Load Balancers:** NAT 인스턴스를 Auto Scaling 그룹과 Network Load Balancers로 관리하는 것은 가능하지만, NAT 게이트웨이를 사용하는 것이 더 간단하고 관리 오버헤드가 적습니다.
- **D. Spot Instances와 Network Load Balancer:** Spot Instances는 비용 효율적일 수 있지만, 인스턴스가 언제든지 종료될 수 있으므로 안정적인 NAT 솔루션으로 적합하지 않습니다. 또한, Network Load Balancer를 사용하는 것은 NAT 게이트웨이를 사용하는 것보다 복잡합니다.

**출제 의도:**
이 문제는 AWS에서 NAT 인스턴스를 NAT 게이트웨이로 교체하여 고가용성, 장애 내성, 자동 확장을 제공하는 방법을 평가합니다. 특히, 서로 다른 가용 영역에 NAT 게이트웨이를 배포하여 고가용성과 장애 내성을 보장하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **NAT 게이트웨이:**
   - NAT 게이트웨이의 기본 개념과 사용 사례
   - NAT 게이트웨이를 사용하여 고가용성, 장애 내성, 자동 확장을 제공하는 방법

2. **NAT 인스턴스:**
   - NAT 인스턴스의 기본 개념과 사용 사례
   - NAT 인스턴스를 NAT 게이트웨이로 교체하는 이유와 장점

이 부분들을 공부하면, AWS에서 NAT 인스턴스를 NAT 게이트웨이로 교체하여 고가용성, 장애 내성, 자동 확장을 제공하는 방법을 더 잘 이해할 수 있습니다.

# 232번
**정답: C. Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for when the alarm is in the ALARM state.**

**해설:**
- **VPC Flow Logs와 CloudWatch Logs:**
  - **VPC Flow Logs:** VPC Flow Logs를 사용하면 VPC 내의 네트워크 트래픽을 캡처하고 분석할 수 있습니다. RDP(포트 3389) 또는 SSH(포트 22) 트래픽을 감지할 수 있습니다.
  - **CloudWatch Logs:** VPC Flow Logs를 CloudWatch Logs로 전송하면, 로그 데이터를 기반으로 메트릭 필터를 생성할 수 있습니다.
  - **CloudWatch Metric Alarm:** 메트릭 필터를 사용하여 특정 조건(RDP 또는 SSH 트래픽 감지)에 대한 알람을 생성하고, 알람 상태가 ALARM일 때 알림을 보낼 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. CloudWatch Application Insights:** Application Insights는 애플리케이션 모니터링 도구로, RDP 또는 SSH 접근을 감지하는 데 적합하지 않습니다.
- **B. IAM Instance Profile:** IAM 인스턴스 프로파일과 AmazonSSMManagedInstanceCore 정책은 Systems Manager를 사용하여 인스턴스를 관리하는 데 사용됩니다. RDP 또는 SSH 접근을 감지하는 데 직접적인 도움이 되지 않습니다.
- **D. EventBridge Rule:** EC2 Instance State-change Notification은 인스턴스 상태 변경(예: 시작, 중지, 종료)에 대한 알림을 제공합니다. RDP 또는 SSH 접근을 감지하는 데 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 VPC 내의 네트워크 트래픽을 모니터링하고, 특정 조건(RDP 또는 SSH 접근)에 대한 알림을 설정하는 방법을 평가합니다. 특히, VPC Flow Logs와 CloudWatch Logs를 사용하여 네트워크 트래픽을 분석하고, CloudWatch Metric Alarm을 설정하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **VPC Flow Logs:**
   - VPC Flow Logs의 기본 개념과 사용 사례
   - VPC Flow Logs를 사용하여 네트워크 트래픽을 캡처하고 분석하는 방법

2. **Amazon CloudWatch Logs:**
   - CloudWatch Logs의 기본 개념과 사용 사례
   - CloudWatch Logs를 사용하여 로그 데이터를 분석하고, 메트릭 필터를 생성하는 방법

3. **Amazon CloudWatch Alarms:**
   - CloudWatch Alarms의 기본 개념과 사용 사례
   - CloudWatch Alarms를 사용하여 특정 조건에 대한 알림을 설정하는 방법

이 부분들을 공부하면, AWS에서 VPC 내의 네트워크 트래픽을 모니터링하고, 특정 조건에 대한 알림을 설정하는 방법을 더 잘 이해할 수 있습니다.

# 235번
**정답: C. Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.**

**해설:**
- **AWS Schema Conversion Tool (SCT):**
  - **스키마 변환:** SCT는 Oracle 데이터베이스 스키마를 Aurora PostgreSQL로 변환하는 데 사용됩니다. 이는 데이터베이스 구조를 호환되게 만듭니다.
  
- **AWS Database Migration Service (DMS):**
  - **메모리 최적화 인스턴스:** 메모리 최적화 인스턴스를 사용하면, 높은 읽기 및 쓰기 트래픽을 처리하는 데 적합합니다.
  - **전체 로드 및 CDC:** 전체 로드와 변경 데이터 캡처(CDC)를 사용하면, 초기 데이터 로드 후에도 데이터베이스 간의 동기화를 유지할 수 있습니다. 이는 데이터가 계속해서 동기화된 상태로 유지되도록 합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. DataSync와 CDC:** DataSync는 파일 기반 데이터 전송에 적합하며, 데이터베이스 마이그레이션에는 적합하지 않습니다. 또한, 전체 로드 없이 CDC만 사용하는 것은 초기 데이터 로드를 처리하지 못합니다.
- **B. DataSync와 전체 로드 + CDC:** DataSync는 데이터베이스 마이그레이션에 적합하지 않습니다. SCT를 사용하여 스키마 변환을 수행하는 것이 더 적합합니다.
- **D. Compute 최적화 인스턴스와 일부 테이블 선택:** Compute 최적화 인스턴스는 메모리 최적화 인스턴스보다 높은 읽기 및 쓰기 트래픽을 처리하는 데 적합하지 않습니다. 또한, 일부 테이블만 선택하는 것은 전체 데이터 동기화를 보장하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 데이터베이스를 마이그레이션하고, 높은 읽기 및 쓰기 트래픽을 처리하며, 데이터 동기화를 유지하는 방법을 평가합니다. 특히, AWS SCT와 DMS를 사용하여 스키마 변환과 데이터 마이그레이션을 수행하고, 메모리 최적화 인스턴스를 사용하여 높은 트래픽을 처리하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Schema Conversion Tool (SCT):**
   - SCT의 기본 개념과 사용 사례
   - SCT를 사용하여 Oracle 스키마를 Aurora PostgreSQL로 변환하는 방법

2. **AWS Database Migration Service (DMS):**
   - DMS의 기본 개념과 사용 사례
   - DMS를 사용하여 전체 로드 및 CDC를 통해 데이터베이스를 마이그레이션하고 동기화하는 방법
   - 메모리 최적화 인스턴스를 사용하여 높은 트래픽을 처리하는 방법

이 부분들을 공부하면, AWS에서 데이터베이스를 마이그레이션하고, 높은 읽기 및 쓰기 트래픽을 처리하며, 데이터 동기화를 유지하는 방법을 더 잘 이해할 수 있습니다.


# 237번
**정답: A. Set up a VPC peering connection between VPC-A and VPC-B.**

**해설:**
- **VPC Peering:**
  - **보안 및 성능:** VPC 피어링은 두 VPC 간의 트래픽을 안전하게 전송할 수 있는 방법입니다. 트래픽이 인터넷을 거치지 않고 AWS 네트워크 내에서 전송되므로 보안이 강화됩니다.
  - **고가용성:** VPC 피어링은 AWS 네트워크 인프라를 통해 고가용성과 높은 대역폭을 제공합니다. 이는 단일 장애 지점 없이 안정적인 연결을 보장합니다.
  - **다중 계정 지원:** VPC 피어링은 동일한 AWS 계정 내의 VPC뿐만 아니라, 서로 다른 AWS 계정 간의 VPC 간에도 설정할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. VPC Gateway Endpoints:** VPC 게이트웨이 엔드포인트는 S3나 DynamoDB와 같은 특정 AWS 서비스에 대한 프라이빗 연결을 제공하는 데 사용됩니다. EC2 인스턴스 간의 연결에는 적합하지 않습니다.
- **C. Virtual Private Gateway:** 가상 프라이빗 게이트웨이는 온프레미스 네트워크와 VPC 간의 연결을 설정하는 데 사용됩니다. VPC 간의 연결에는 적합하지 않습니다.
- **D. Private Virtual Interface (VIF):** 프라이빗 VIF는 Direct Connect를 통해 온프레미스 네트워크와 VPC 간의 연결을 설정하는 데 사용됩니다. VPC 간의 연결에는 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 서로 다른 계정에 있는 두 VPC 간의 안전하고 고가용성 연결을 설정하는 방법을 평가합니다. 특히, VPC 피어링을 사용하여 두 VPC 간의 트래픽을 안전하게 전송하고, 단일 장애 지점 없이 안정적인 연결을 보장하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **VPC Peering:**
   - VPC 피어링의 기본 개념과 사용 사례
   - VPC 피어링을 설정하여 두 VPC 간의 트래픽을 안전하게 전송하는 방법
   - 서로 다른 AWS 계정 간의 VPC 피어링 설정 방법

2. **VPC Gateway Endpoints 및 Virtual Private Gateway:**
   - VPC 게이트웨이 엔드포인트와 가상 프라이빗 게이트웨이의 기본 개념과 사용 사례
   - 이들이 VPC 간의 연결에 적합하지 않은 이유

이 부분들을 공부하면, AWS에서 서로 다른 계정에 있는 두 VPC 간의 안전하고 고가용성 연결을 설정하는 방법을 더 잘 이해할 수 있습니다.


# 238번
**정답: C. Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.**

**해설:**
- **AWS Budgets:**
  - **비용 관리:** AWS Budgets는 비용 및 사용량을 추적하고, 특정 임계값을 초과할 때 알림을 설정할 수 있는 서비스입니다. 이는 비용 초과를 방지하고, 예산을 관리하는 데 유용합니다.
  - **알림 설정:** 예산을 설정하고, 특정 임계값을 초과할 때 Amazon SNS를 통해 알림을 받을 수 있습니다. 이는 비용 초과를 실시간으로 모니터링하고, 즉시 대응할 수 있도록 합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Cost Explorer 일일 보고서:** Cost Explorer를 사용하여 일일 보고서를 생성하고, SES를 통해 알림을 설정하는 것은 가능하지만, AWS Budgets를 사용하는 것이 더 간단하고 비용 효율적입니다.
- **B. Cost Explorer 월간 보고서:** Cost Explorer를 사용하여 월간 보고서를 생성하고, SES를 통해 알림을 설정하는 것은 가능하지만, AWS Budgets를 사용하는 것이 더 간단하고 비용 효율적입니다.
- **D. Cost and Usage Reports와 Athena:** Cost and Usage Reports를 사용하여 시간별 보고서를 생성하고, Athena와 EventBridge를 사용하여 쿼리를 스케줄링하는 것은 가능하지만, 이는 복잡하고 관리 오버헤드가 큽니다. AWS Budgets를 사용하는 것이 더 간단하고 비용 효율적입니다.

**출제 의도:**
이 문제는 AWS에서 비용을 관리하고, 특정 임계값을 초과할 때 알림을 설정하는 방법을 평가합니다. 특히, AWS Budgets를 사용하여 비용을 추적하고, 알림을 설정하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Budgets:**
   - AWS Budgets의 기본 개념과 사용 사례
   - AWS Budgets를 사용하여 비용 및 사용량을 추적하고, 알림을 설정하는 방법

2. **Amazon SNS:**
   - Amazon SNS의 기본 개념과 사용 사례
   - SNS를 사용하여 알림을 설정하고, 알림을 수신하는 방법

이 부분들을 공부하면, AWS에서 비용을 관리하고, 특정 임계값을 초과할 때 알림을 설정하는 방법을 더 잘 이해할 수 있습니다.

# 239번
**정답: B. Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.**

**해설:**
- **Lambda Function URL:**
  - **운영 효율성:** Lambda Function URL은 Lambda 함수에 직접 HTTPS 엔드포인트를 제공하는 기능입니다. 이를 통해 API Gateway를 설정할 필요 없이 Lambda 함수에 직접 접근할 수 있습니다.
  - **IAM 인증:** Lambda Function URL은 AWS_IAM 인증 유형을 지원하므로, IAM을 사용하여 호출을 인증할 수 있습니다. 이는 보안 요구 사항을 충족합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. API Gateway:** API Gateway를 사용하여 Lambda 함수를 호출하는 것은 가능하지만, Lambda Function URL을 사용하는 것이 더 간단하고 운영 오버헤드가 적습니다.
- **C. Lambda@Edge:** Lambda@Edge는 CloudFront와 통합되어 엣지 로케이션에서 코드를 실행하는 데 사용됩니다. 이는 글로벌 콘텐츠 전송에 적합하지만, 단순한 마이크로서비스 호출에는 과도한 설정이 필요합니다.
- **D. CloudFront Functions:** CloudFront Functions는 CloudFront 배포와 통합되어 엣지에서 간단한 JavaScript 함수를 실행하는 데 사용됩니다. 이는 Lambda 함수를 호출하는 데 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 마이크로서비스를 배포하고, HTTPS 엔드포인트와 IAM 인증을 설정하는 방법을 평가합니다. 특히, Lambda Function URL을 사용하여 운영 오버헤드를 최소화하고, IAM 인증을 설정하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Lambda Function URL:**
   - Lambda Function URL의 기본 개념과 사용 사례
   - Lambda Function URL을 사용하여 Lambda 함수에 직접 HTTPS 엔드포인트를 제공하는 방법
   - AWS_IAM 인증 유형을 사용하여 호출을 인증하는 방법

2. **Amazon API Gateway:**
   - API Gateway의 기본 개념과 사용 사례
   - API Gateway를 사용하여 Lambda 함수를 호출하고, IAM 인증을 설정하는 방법

이 부분들을 공부하면, AWS에서 마이크로서비스를 배포하고, HTTPS 엔드포인트와 IAM 인증을 설정하는 방법을 더 잘 이해할 수 있습니다.


# 240번
**정답: D. Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct Connect connection at a location in the same Region.**

**해설:**
- **Direct Connect 비용 절감:**
  - **Direct Connect:** Direct Connect는 AWS와 온프레미스 네트워크 간의 전용 네트워크 연결을 제공하여, 데이터 전송 비용을 절감할 수 있습니다. 특히, 동일한 AWS 리전 내에서 Direct Connect를 사용하는 경우, 인터넷을 통한 데이터 전송보다 비용이 저렴합니다.
  - **동일 리전 내 호스팅:** 시각화 도구를 데이터 웨어하우스와 동일한 AWS 리전에 호스팅하면, 데이터 전송이 AWS 네트워크 내에서 이루어지므로 추가적인 인터넷 데이터 전송 비용이 발생하지 않습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. 인터넷을 통한 쿼리:** 인터넷을 통해 데이터 웨어하우스를 쿼리하면, 데이터 전송 비용이 높아질 수 있습니다. 특히, 50MB 크기의 쿼리 결과를 자주 전송하는 경우, 비용이 크게 증가할 수 있습니다.
- **B. 인터넷을 통한 접근:** 시각화 도구를 AWS 리전에 호스팅하더라도, 인터넷을 통해 접근하면 데이터 전송 비용이 발생합니다. Direct Connect를 사용하는 것이 더 비용 효율적입니다.
- **C. 온프레미스에서 Direct Connect 사용:** 시각화 도구를 온프레미스에 호스팅하고 Direct Connect를 사용하는 것은 가능하지만, 데이터 웨어하우스와 시각화 도구 간의 데이터 전송이 여전히 발생하므로 비용이 더 높아질 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 데이터 전송 비용을 최소화하는 방법을 평가합니다. 특히, Direct Connect를 사용하여 데이터 전송 비용을 절감하고, 동일한 AWS 리전 내에서 시각화 도구와 데이터 웨어하우스를 호스팅하여 추가적인 인터넷 데이터 전송 비용을 피하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Direct Connect:**
   - Direct Connect의 기본 개념과 사용 사례
   - Direct Connect를 사용하여 데이터 전송 비용을 절감하는 방법

2. **데이터 전송 비용:**
   - AWS에서 데이터 전송 비용 구조
   - 인터넷을 통한 데이터 전송과 Direct Connect를 통한 데이터 전송 비용 비교

이 부분들을 공부하면, AWS에서 데이터 전송 비용을 최소화하는 방법을 더 잘 이해할 수 있습니다.


# 241번
**정답: C. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a read replica in another Region.**

**해설:**
- **Amazon RDS Read Replica:**
  - **다중 리전 가용성:** RDS 읽기 복제본을 다른 리전에 생성하면, 데이터가 여러 리전에 걸쳐 복제되어 고가용성을 보장할 수 있습니다. 이는 데이터가 항상 온라인 상태로 유지되도록 합니다.
  - **운영 오버헤드 최소화:** RDS 읽기 복제본은 AWS에서 관리되므로, 수동으로 복제 설정을 관리할 필요가 없습니다. 이는 운영 오버헤드를 최소화합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. PostgreSQL cluster on EC2:** EC2 인스턴스에서 PostgreSQL 클러스터를 운영하는 것은 많은 관리 오버헤드가 발생합니다. 수동으로 복제를 설정하고 관리해야 하므로, 운영 오버헤드가 큽니다.
- **B. RDS Multi-AZ:** Multi-AZ 배포는 동일한 리전 내에서 고가용성을 제공하지만, 다중 리전 가용성을 제공하지 않습니다.
- **D. DB snapshots:** 스냅샷을 다른 리전에 복사하는 것은 데이터 백업을 위한 방법이며, 실시간 데이터 가용성을 보장하지 않습니다. 이는 데이터가 항상 온라인 상태로 유지되도록 하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 다중 리전 가용성을 제공하고, 운영 오버헤드를 최소화하는 방법을 평가합니다. 특히, RDS 읽기 복제본을 사용하여 데이터가 여러 리전에 걸쳐 복제되고, 항상 온라인 상태로 유지되도록 하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon RDS Read Replica:**
   - RDS 읽기 복제본의 기본 개념과 사용 사례
   - RDS 읽기 복제본을 사용하여 다중 리전 가용성을 제공하는 방법

2. **Amazon RDS Multi-AZ:**
   - RDS Multi-AZ의 기본 개념과 사용 사례
   - RDS Multi-AZ가 다중 리전 가용성을 제공하지 않는 이유

이 부분들을 공부하면, AWS에서 다중 리전 가용성을 제공하고, 운영 오버헤드를 최소화하는 방법을 더 잘 이해할 수 있습니다.


# 242번
**정답: C. Multivalue routing policy**

**해설:**
- **Multivalue Routing Policy:**
  - **다중 IP 주소 반환:** Multivalue Answer Routing Policy는 여러 IP 주소를 반환할 수 있습니다. 이는 DNS 쿼리에 대해 여러 EC2 인스턴스의 IP 주소를 반환하여, 트래픽을 분산시킬 수 있습니다.
  - **헬스 체크:** Multivalue Answer Routing Policy는 헬스 체크를 지원하여, 건강한 인스턴스의 IP 주소만 반환할 수 있습니다. 이는 요청이 건강하지 않은 인스턴스로 라우팅되지 않도록 보장합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Simple Routing Policy:** Simple Routing Policy는 단일 IP 주소를 반환합니다. 여러 IP 주소를 반환하는 요구 사항을 충족하지 않습니다.
- **B. Latency Routing Policy:** Latency Routing Policy는 지연 시간을 기준으로 트래픽을 라우팅합니다. 여러 IP 주소를 반환하는 요구 사항을 충족하지 않습니다.
- **D. Geolocation Routing Policy:** Geolocation Routing Policy는 사용자의 지리적 위치를 기준으로 트래픽을 라우팅합니다. 여러 IP 주소를 반환하는 요구 사항을 충족하지 않습니다.

**출제 의도:**
이 문제는 AWS Route 53에서 여러 IP 주소를 반환하고, 헬스 체크를 통해 건강한 인스턴스의 IP 주소만 반환하는 방법을 평가합니다. 특히, Multivalue Answer Routing Policy를 사용하여 이러한 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Route 53 Routing Policies:**
   - Route 53의 다양한 라우팅 정책(예: Simple, Latency, Multivalue, Geolocation)의 기본 개념과 사용 사례
   - Multivalue Answer Routing Policy를 사용하여 여러 IP 주소를 반환하고, 헬스 체크를 통해 건강한 인스턴스의 IP 주소만 반환하는 방법

이 부분들을 공부하면, AWS Route 53에서 여러 IP 주소를 반환하고, 헬스 체크를 통해 건강한 인스턴스의 IP 주소만 반환하는 방법을 더 잘 이해할 수 있습니다.


# 246번
**정답: D. Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route tables for the public subnets with a route to the private subnets.**

**해설:**
- **인터넷 접근:** 인터넷에서 접근할 수 있는 ALB는 퍼블릭 서브넷에 배치되어야 합니다. 퍼블릭 서브넷은 인터넷 게이트웨이와 연결되어 있어야 하며, 이를 통해 인터넷 트래픽을 수신할 수 있습니다.
- **프라이빗 서브넷의 EC2 인스턴스:** EC2 인스턴스는 프라이빗 서브넷에 남아 있어도 됩니다. ALB가 퍼블릭 서브넷에 배치되고, 퍼블릭 서브넷의 라우팅 테이블이 프라이빗 서브넷으로의 경로를 포함하면, ALB는 프라이빗 서브넷의 EC2 인스턴스에 트래픽을 전달할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. NAT 게이트웨이와 Network Load Balancer:** NAT 게이트웨이는 주로 프라이빗 서브넷의 인스턴스가 인터넷에 접근할 수 있도록 하는 데 사용됩니다. 인터넷에서 접근할 수 있는 ALB를 대체하는 데 적합하지 않습니다.
- **B. EC2 인스턴스를 퍼블릭 서브넷으로 이동:** EC2 인스턴스를 퍼블릭 서브넷으로 이동하는 것은 보안 위험이 있습니다. 프라이빗 서브넷에 인스턴스를 유지하면서 ALB를 퍼블릭 서브넷에 배치하는 것이 더 

# 247번
**정답: A. Enable binlog replication on the RDS primary node. C. Allow long-running transactions to complete on the source DB instance.**

**해설:**
- **A. Enable binlog replication on the RDS primary node:**
  - **Binlog 복제:** RDS MySQL에서 읽기 복제본을 생성하려면 바이너리 로그(binlog) 복제가 활성화되어 있어야 합니다. 이는 데이터 변경 사항을 기록하고, 읽기 복제본으로 복제하는 데 필요합니다.

- **C. Allow long-running transactions to complete on the source DB instance:**
  - **긴 트랜잭션 완료:** 읽기 복제본을 생성하기 전에 소스 DB 인스턴스에서 실행 중인 긴 트랜잭션이 완료되도록 해야 합니다. 이는 데이터 일관성을 보장하고, 복제본 생성 중에 데이터 손실을 방지합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. Choose a failover priority for the source DB instance:** 읽기 복제본을 생성하는 것과는 직접적인 관련이 없습니다. 이는 고가용성을 위한 설정입니다.
- **D. Create a global table and specify the AWS Regions where the table will be available:** 글로벌 테이블은 DynamoDB와 관련된 기능이며, RDS MySQL과는 관련이 없습니다.
- **E. Enable automatic backups on the source instance by setting the backup retention period to a value other than 0:** 자동 백업을 활성화하는 것은 좋은 관행이지만, 읽기 복제본을 생성하는 데 필수적인 단계는 아닙니다.

**출제 의도:**
이 문제는 AWS RDS MySQL에서 읽기 복제본을 생성하기 전에 수행해야 할 필수 작업을 평가합니다. 특히, binlog 복제를 활성화하고, 긴 트랜잭션이 완료되도록 하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **RDS MySQL Read Replica:**
   - 읽기 복제본의 기본 개념과 사용 사례
   - 읽기 복제본을 생성하기 전에 수행해야 할 필수 작업

2. **Binlog Replication:**
   - 바이너리 로그(binlog) 복제의 기본 개념과 사용 사례
   - binlog 복제를 활성화하는 방법

이 부분들을 공부하면, AWS RDS MySQL에서 읽기 복제본을 생성하기 전에 수행해야 할 필수 작업을 더 잘 이해할 수 있습니다.


# 263번
**정답: A. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster. D. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.**

**해설:**
- **Amazon ECS와 Fargate:**
  - **ECS 클러스터:** Amazon ECS는 AWS에서 컨테이너를 관리하는 서비스로, 클러스터를 사용하여 여러 컨테이너를 관리할 수 있습니다. ECS 클러스터를 사용하면 컨테이너 오케스트레이션을 쉽게 수행할 수 있습니다.
  - **Fargate 런치 타입:** Fargate는 서버리스 컨테이너 실행 환경으로, 인프라를 관리할 필요 없이 컨테이너를 실행할 수 있습니다. Fargate를 사용하면 EC2 인스턴스를 관리할 필요가 없으므로, 유지 관리와 확장에 대한 노력이 최소화됩니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. Kubernetes control plane on EC2:** Kubernetes 제어 평면을 EC2 인스턴스에 배포하면, 추가 인프라를 관리해야 하므로 유지 관리 노력이 증가합니다.
- **C. ECS with EC2 launch type:** EC2 런치 타입을 사용하면 EC2 인스턴스를 관리해야 하므로, 추가 인프라를 관리해야 합니다. 이는 유지 관리 노력을 증가시킵니다.
- **E. Kubernetes worker nodes on EC2:** Kubernetes 워커 노드를 EC2 인스턴스에 배포하면, 추가 인프라를 관리해야 하므로 유지 관리 노력이 증가합니다.

**출제 의도:**
이 문제는 AWS에서 컨테이너 기술을 사용하여 마이크로서비스를 배포하고, 유지 관리와 확장에 대한 노력을 최소화하는 방법을 평가합니다. 특히, Amazon ECS와 Fargate를 사용하여 서버리스 컨테이너 실행 환경을 구성하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon ECS:**
   - ECS의 기본 개념과 사용 사례
   - ECS 클러스터를 사용하여 컨테이너를 관리하는 방법

2. **AWS Fargate:**
   - Fargate의 기본 개념과 사용 사례
   - Fargate를 사용하여 서버리스 컨테이너 실행 환경을 구성하는 방법

이 부분들을 공부하면, AWS에서 컨테이너 기술을 사용하여 마이크로서비스를 배포하고, 유지 관리와 확장에 대한 노력을 최소화하는 방법을 더 잘 이해할 수 있습니다.


# 276번
**정답: A. Configure storage Auto Scaling on the RDS for Oracle instance. D. Configure the Auto Scaling group to use the average CPU as the scaling metric.**

**해설:**
- **A. Configure storage Auto Scaling on the RDS for Oracle instance:**
  - **자동 스토리지 확장:** RDS for Oracle 인스턴스에 스토리지 자동 확장을 구성하면, 스토리지 용량이 부족할 때 자동으로 스토리지를 확장할 수 있습니다. 이는 RDS 인스턴스가 스토리지 부족으로 인해 중단되지 않도록 보장합니다.

- **D. Configure the Auto Scaling group to use the average CPU as the scaling metric:**
  - **CPU 기반 스케일링:** Auto Scaling 그룹에서 평균 CPU 사용률을 스케일링 메트릭으로 사용하면, 트래픽 증가에 따라 EC2 인스턴스를 자동으로 추가하거나 제거할 수 있습니다. 이는 EC2 인스턴스가 과부하되지 않도록 보장합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. Migrate the database to Amazon Aurora:** Amazon Aurora로 데이터베이스를 마이그레이션하는 것은 Oracle-specific PL/SQL 기능을 사용할 수 없게 만들 수 있으며, 이는 애플리케이션에 큰 변화를 요구할 수 있습니다.
- **C. Configure an alarm on the RDS for Oracle instance for low free storage space:** 알람을 구성하는 것은 스토리지 부족을 모니터링하는 데 도움이 되지만, 자동으로 스토리지를 확장하지는 않습니다.
- **E. Configure the Auto Scaling group to use the average free memory as the scaling metric:** 메모리 사용률을 스케일링 메트릭으로 사용하는 것은 가능하지만, 일반적으로 CPU 사용률이 더 적합한 스케일링 메트릭입니다. CPU 사용률은 트래픽 증가에 더 직접적으로 반응합니다.

**출제 의도:**
이 문제는 AWS에서 트래픽 증가에 따라 자동으로 확장할 수 있는 시스템을 설계하는 방법을 평가합니다. 특히, RDS 인스턴스의 스토리지 자동 확장과 Auto Scaling 그룹의 CPU 기반 스케일링을 구성하여 시스템이 자동으로 확장할 수 있도록 하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon RDS Auto Scaling:**
   - RDS의 스토리지 자동 확장의 기본 개념과 사용 사례
   - 스토리지 자동 확장을 구성하여 RDS 인스턴스가 스토리지 부족으로 인해 중단되지 않도록 하는 방법

2. **Auto Scaling Metrics:**
   - Auto Scaling 메트릭의 기본 개념과 사용 사례
   - CPU 사용률을 스케일링 메트릭으로 사용하여 트래픽 증가에 따라 EC2 인스턴스를 자동으로 추가하거나 제거하는 방법

이 부분들을 공부하면, AWS에서 트래픽 증가에 따라 자동으로 확장할 수 있는 시스템을 설계하는 방법을 더 잘 이해할 수 있습니다.

# 277번
**정답: D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.**

**해설:**
- **Amazon S3:**
  - **비용 효율성:** Amazon S3는 대용량 데이터를 저장하는 데 매우 비용 효율적입니다. 특히, 비디오 콘텐츠와 같은 대용량 파일을 저장하는 데 적합합니다.
  - **내구성 및 가용성:** S3는 높은 내구성과 가용성을 제공하여 데이터를 안전하게 저장할 수 있습니다.

- **Amazon EBS:**
  - **임시 스토리지:** 비디오 콘텐츠를 처리할 때만 EBS 볼륨을 사용하여 임시 스토리지로 활용할 수 있습니다. 이는 EFS를 계속 사용하는 것보다 비용을 절감할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. AWS Storage Gateway for files:** Storage Gateway는 온프레미스와 클라우드 간의 하이브리드 스토리지 솔루션으로, 이 시나리오에서는 적합하지 않습니다.
- **B. AWS Storage Gateway for volumes:** 마찬가지로, Storage Gateway for volumes는 온프레미스와 클라우드 간의 하이브리드 스토리지 솔루션으로, 이 시나리오에서는 적합하지 않습니다.
- **C. Amazon EFS와 EBS:** EFS를 계속 사용하고, 처리 후에 EBS로 전송하는 것은 비용 절감에 큰 도움이 되지 않습니다. S3를 사용하는 것이 더 비용 효율적입니다.

**출제 의도:**
이 문제는 AWS에서 비디오 콘텐츠를 저장하고 처리하는 데 가장 비용 효율적인 스토리지 솔루션을 선택하는 방법을 평가합니다. 특히, Amazon S3와 EBS를 사용하여 비용을 절감하고, 비디오 콘텐츠를 효율적으로 처리하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon S3:**
   - S3의 기본 개념과 사용 사례
   - S3를 사용하여 대용량 데이터를 비용 효율적으로 저장하는 방법

2. **Amazon EBS:**
   - EBS의 기본 개념과 사용 사례
   - EBS를 임시 스토리지로 활용하여 데이터를 처리하는 방법

이 부분들을 공부하면, AWS에서 비디오 콘텐츠를 저장하고 처리하는 데 가장 비용 효율적인 스토리지 솔루션을 선택하는 방법을 더 잘 이해할 수 있습니다.

# 285번
**정답: B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).**

**해설:**
- **Amazon API Gateway와 AWS Lambda:**
  - **서버리스 아키텍처:** API Gateway와 Lambda를 사용하면 서버를 관리할 필요 없이 동적 서버 측 컴포넌트를 구현할 수 있습니다. 이는 비용 효율적이며, 특히 트래픽이 적은 경우에 적합합니다.
  - **비용 효율성:** Lambda는 사용한 만큼만 비용을 지불하는 서버리스 컴퓨팅 서비스로, 월간 방문자가 100명 이하인 경우 매우 저렴하게 운영할 수 있습니다.
  - **Amazon SES 통합:** Lambda 함수에서 Amazon SES를 호출하여 이메일을 전송할 수 있습니다. 이는 이메일 전송을 간단하고 효율적으로 처리할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Amazon ECS:** ECS는 컨테이너 오케스트레이션 서비스로, 소규모 트래픽을 처리하는 데는 과도한 솔루션입니다. 비용이 더 많이 들 수 있습니다.
- **C. Amazon Lightsail:** Lightsail은 간단한 서버 호스팅 솔루션이지만, 서버를 관리해야 하므로 Lambda와 API Gateway를 사용하는 것보다 비용 효율적이지 않습니다.
- **D. Amazon EC2:** EC2 인스턴스를 사용하여 LAMP 스택을 배포하는 것은 소규모 트래픽을 처리하는 데 과도한 솔루션입니다. 서버를 관리해야 하므로 비용이 더 많이 들 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 소규모 트래픽을 처리하는 동적 서버 측 컴포넌트를 구현하고, 비용 효율적으로 운영하는 방법을 평가합니다. 특히, Amazon API Gateway와 AWS Lambda를 사용하여 서버리스 아키텍처를 구현하고, Amazon SES를 사용하여 이메일을 전송하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon API Gateway:**
   - API Gateway의 기본 개념과 사용 사례
   - API Gateway를 사용하여 엔드포인트를 생성하고, Lambda와 통합하는 방법

2. **AWS Lambda:**
   - Lambda의 기본 개념과 사용 사례
   - Lambda 함수를 작성하고, API Gateway와 통합하여 서버리스 아키텍처를 구현하는 방법

3. **Amazon SES:**
   - SES의 기본 개념과 사용 사례
   - SES를 사용하여 이메일을 전송하는 방법

이 부분들을 공부하면, AWS에서 소규모 트래픽을 처리하는 동적 서버 측 컴포넌트를 구현하고, 비용 효율적으로 운영하는 방법을 더 잘 이해할 수 있습니다.

# 291번
**정답: B. Signed URLs, A. Signed cookies**

**해설:**
- **Signed URLs:**
  - **쿠키 미지원 클라이언트:** 일부 사용자가 쿠키를 지원하지 않는 커스텀 HTTP 클라이언트를 사용하고 있으므로, Signed URLs는 이러한 사용자를 지원하는 데 적합합니다. Signed URLs는 URL 자체에 만료 시간과 서명을 포함하여, 특정 시간 동안만 접근을 허용합니다.
  - **하드코딩된 URL:** 일부 사용자가 하드코딩된 URL을 변경할 수 없으므로, Signed URLs를 사용하면 기존 URL을 변경하지 않고도 접근 제어를 구현할 수 있습니다.

- **Signed Cookies:**
  - **쿠키 지원 클라이언트:** 쿠키를 지원하는 클라이언트의 경우, Signed Cookies를 사용하여 접근을 제어할 수 있습니다. 이는 여러 파일에 대한 접근을 제어하는 데 유용합니다.
  - **기존 URL 유지:** Signed Cookies를 사용하면 URL을 변경할 필요가 없으므로, 사용자에게 미치는 영향을 최소화할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **C. AWS AppSync:** AppSync는 GraphQL API를 제공하는 서비스로, 이 시나리오에서는 적합하지 않습니다.
- **D. JSON Web Token (JWT):** JWT는 인증 및 권한 부여를 위한 토큰 기반 솔루션이지만, CloudFront와 S3의 접근 제어를 직접적으로 지원하지 않습니다.
- **E. AWS Secrets Manager:** Secrets Manager는 비밀 관리 서비스로, 이 시나리오에서는 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 CloudFront와 S3를 사용하여 공개적으로 제공되는 비디오 콘텐츠에 대한 접근을 제어하는 방법을 평가합니다. 특히, Signed URLs와 Signed Cookies를 사용하여 사용자에게 미치는 영향을 최소화하면서 접근을 제어하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Signed URLs and Signed Cookies:**
   - Signed URLs와 Signed Cookies의 기본 개념과 사용 사례
   - Signed URLs와 Signed Cookies를 사용하여 CloudFront와 S3의 접근을 제어하는 방법

2. **AWS CloudFront and S3:**
   - CloudFront와 S3의 기본 개념과 사용 사례
   - CloudFront와 S3를 사용하여 비디오 콘텐츠를 제공하고, 접근을 제어하는 방법

이 부분들을 공부하면, AWS에서 CloudFront와 S3를 사용하여 공개적으로 제공되는 비디오 콘텐츠에 대한 접근을 제어하는 방법을 더 잘 이해할 수 있습니다.


# 293번
**정답: D. Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.**

**해설:**
- **AWS Storage Gateway (Stored Volume Gateway):**
  - **로컬 액세스:** Stored Volume Gateway는 온프레미스에서 데이터를 저장하고, 백업을 위해 AWS로 데이터를 전송합니다. 온프레미스 시스템은 로컬 스토리지에 직접 액세스할 수 있습니다.
  - **자동 및 안전한 전송:** Storage Gateway는 데이터를 자동으로 AWS로 전송하며, 전송 중에 데이터를 암호화하여 안전하게 보호합니다.
  - **백업 및 복구:** Stored Volume Gateway는 온프레미스 데이터의 백업 및 복구를 간편하게 관리할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. AWS Snowball:** Snowball은 대량의 데이터를 물리적으로 AWS로 전송하는 데 사용되며, 지속적인 로컬 액세스를 제공하지 않습니다.
- **B. AWS Snowball Edge:** Snowball Edge는 일시적인 데이터 전송 및 로컬 컴퓨팅을 제공하지만, 지속적인 백업 솔루션으로는 적합하지 않습니다.
- **C. Cached Volume Gateway:** Cached Volume Gateway는 주로 AWS에 데이터를 저장하고, 자주 액세스하는 데이터를 로컬에 캐시합니다. 온프레미스에서 모든 데이터를 로컬로 유지하려는 요구 사항을 충족하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 온프레미스 데이터의 백업 솔루션을 설계하고, 로컬 액세스를 유지하면서 데이터를 안전하게 전송하는 방법을 평가합니다. 특히, AWS Storage Gateway의 Stored Volume Gateway를 사용하여 이러한 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Storage Gateway:**
   - Storage Gateway의 기본 개념과 사용 사례
   - Stored Volume Gateway와 Cached Volume Gateway의 차이점
   - Stored Volume Gateway를 사용하여 온프레미스 데이터의 백업 및 로컬 액세스를 제공하는 방법

2. **AWS Snowball 및 Snowball Edge:**
   - Snowball과 Snowball Edge의 기본 개념과 사용 사례
   - Snowball과 Snowball Edge가 지속적인 백업 솔루션으로 적합하지 않은 이유

이 부분들을 공부하면, AWS에서 온프레미스 데이터의 백업 솔루션을 설계하고, 로컬 액세스를 유지하면서 데이터를 안전하게 전송하는 방법을 더 잘 이해할 수 있습니다.

# 296번
**정답: D. 10.0.1.0/24**

**해설:**
- **VPC Peering 조건:**
  - **CIDR 블록 중복 불가:** VPC 피어링 연결을 설정하려면 두 VPC의 CIDR 블록이 중복되지 않아야 합니다. 즉, 두 VPC의 IP 주소 범위가 겹치지 않아야 합니다.
  - **CIDR 블록 크기:** CIDR 블록의 크기는 /24, /16 등 다양한 크기로 설정할 수 있지만, 피어링 연결을 위해서는 중복되지 않는 범위 내에서 설정해야 합니다.

- **옵션 분석:**
  - **A. 10.0.1.0/32:** /32는 단일 IP 주소를 나타내며, VPC의 CIDR 블록으로는 너무 작습니다.
  - **B. 192.168.0.0/24:** 이 CIDR 블록은 이미 개발 VPC에서 사용 중이므로 중복됩니다.
  - **C. 192.168.1.0/32:** /32는 단일 IP 주소를 나타내며, VPC의 CIDR 블록으로는 너무 작습니다.
  - **D. 10.0.1.0/24:** 이 CIDR 블록은 개발 VPC의 CIDR 블록과 중복되지 않으며, VPC 피어링 연결을 설정하는 데 적합합니다.

**출제 의도:**
이 문제는 AWS에서 VPC 피어링 연결을 설정할 때 CIDR 블록을 올바르게 선택하는 방법을 평가합니다. 특히, 중복되지 않는 CIDR 블록을 선택하여 VPC 피어링 연결을 설정하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **VPC Peering:**
   - VPC 피어링의 기본 개념과 사용 사례
   - VPC 피어링 연결을 설정할 때 CIDR 블록이 중복되지 않도록 하는 방법

2. **CIDR 블록:**
   - CIDR 블록의 기본 개념과 사용 사례
   - 다양한 크기의 CIDR 블록을 이해하고, VPC에 적합한 CIDR 블록을 선택하는 방법

이 부분들을 공부하면, AWS에서 VPC 피어링 연결을 설정할 때 CIDR 블록을 올바르게 선택하는 방법을 더 잘 이해할 수 있습니다.

# 297번
**정답: B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.**

**해설:**
- **EC2 Auto Scaling Group:**
  - **자동 스케일링:** Auto Scaling 그룹을 사용하면 애플리케이션의 트래픽 변화에 따라 EC2 인스턴스의 수를 자동으로 조정할 수 있습니다. 이는 비용을 최적화하고, CPU 리소스가 필요한 경우 자동으로 확장할 수 있습니다.
  - **타겟 트래킹 스케일링 정책:** ASGAverageCPUUtilization 메트릭을 기반으로 하는 타겟 트래킹 스케일링 정책을 설정하면, CPU 사용률이 특정 값(예: 50%)을 목표로 자동으로 인스턴스를 추가하거나 제거할 수 있습니다. 이는 CPU 사용률이 낮을 때 비용을 절감하고, 사용률이 높을 때 성능을 보장합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. CloudWatch 알람과 Lambda:** CPU 사용률이 낮을 때 인스턴스를 종료하는 것은 비용 절감에 도움이 될 수 있지만, CPU 사용률이 높을 때 자동으로 인스턴스를 추가하지 않으므로 성능을 보장하지 못합니다.
- **C. Auto Scaling 그룹 (스케일링 정책 없음):** Auto Scaling 그룹을 설정하더라도 스케일링 정책이 없으면, 트래픽 변화에 따라 자동으로 인스턴스를 조정할 수 없습니다.
- **D. CloudWatch 알람과 SNS:** 알람을 설정하고 이메일을 통해 수동으로 인스턴스를 조정하는 것은 자동화되지 않으며, 운영 오버헤드가 큽니다.

**출제 의도:**
이 문제는 AWS에서 애플리케이션의 자동 스케일링을 설정하여 비용을 최적화하고 성능을 보장하는 방법을 평가합니다. 특히, EC2 Auto Scaling 그룹과 타겟 트래킹 스케일링 정책을 사용하여 CPU 사용률에 따라 인스턴스를 자동으로 조정하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **EC2 Auto Scaling:**
   - Auto Scaling 그룹의 기본 개념과 사용 사례
   - 타겟 트래킹 스케일링 정책을 설정하여 CPU 사용률에 따라 인스턴스를 자동으로 조정하는 방법

2. **CloudWatch Metrics:**
   - CloudWatch 메트릭의 기본 개념과 사용 사례
   - ASGAverageCPUUtilization 메트릭을 사용하여 스케일링 정책을 설정하는 방법

이 부분들을 공부하면, AWS에서 애플리케이션의 자동 스케일링을 설정하여 비용을 최적화하고 성능을 보장하는 방법을 더 잘 이해할 수 있습니다.

 
# 298번
**정답: C. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.**

**해설:**
- **다중 가용 영역 (Multi-AZ) 배포:**
  - **EC2 인스턴스:** Auto Scaling 그룹을 사용하여 EC2 인스턴스를 여러 가용 영역에 분산 배치하면, 한 가용 영역에 장애가 발생하더라도 애플리케이션이 계속 작동할 수 있습니다.
  - **RDS Multi-AZ:** RDS 인스턴스를 Multi-AZ 배포로 구성하면, 기본 인스턴스와 동기화된 스탠바이 인스턴스가 다른 가용 영역에 배치됩니다. 이는 데이터베이스의 고가용성을 보장합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. DB 인스턴스의 네트워크 연결:** RDS 인스턴스는 Multi-AZ 배포를 통해 고가용성을 제공해야 하며, 단순히 네트워크 연결을 설정하는 것으로는 충분하지 않습니다.
- **B. 두 개의 서브넷이 두 가용 영역에 걸쳐 있음:** 서브넷은 단일 가용 영역 내에 있어야 하며, 두 가용 영역에 걸쳐 있을 수 없습니다.
- **D. 서브넷이 두 가용 영역에 걸쳐 있음:** 서브넷은 단일 가용 영역 내에 있어야 하며, 두 가용 영역에 걸쳐 있을 수 없습니다.

**출제 의도:**
이 문제는 AWS에서 고가용성을 보장하기 위해 EC2 인스턴스와 RDS 인스턴스를 여러 가용 영역에 분산 배치하는 방법을 평가합니다. 특히, Auto Scaling 그룹과 RDS Multi-AZ 배포를 사용하여 애플리케이션의 고가용성을 보장하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Auto Scaling Group:**
   - Auto Scaling 그룹의 기본 개념과 사용 사례
   - 여러 가용 영역에 EC2 인스턴스를 분산 배치하는 방법

2. **RDS Multi-AZ 배포:**
   - RDS Multi-AZ 배포의 기본 개념과 사용 사례
   - Multi-AZ 배포를 통해 데이터베이스의 고가용성을 보장하는 방법

이 부분들을 공부하면, AWS에서 고가용성을 보장하기 위해 EC2 인스턴스와 RDS 인스턴스를 여러 가용 영역에 분산 배치하는 방법을 더 잘 이해할 수 있습니다.

# 299번
**정답: B. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.**

**해설:**
- **Amazon FSx for Lustre:**
  - **고성능:** FSx for Lustre는 고성능 파일 시스템으로, 특히 고성능 컴퓨팅(HPC) 워크로드에 적합합니다. SSD 스토리지를 사용하면 서브 밀리초 지연 시간과 높은 처리량을 제공합니다.
  - **데이터 통합:** S3와 통합하여 데이터를 가져오고 내보낼 수 있습니다. 이는 대규모 데이터 세트를 처리하는 데 유용합니다.

- **옵션 분석:**
  - **A. FSx for NetApp ONTAP (Tiering Policy: ALL):** NetApp ONTAP는 고성능 파일 시스템이지만, Lustre가 HPC 워크로드에 더 적합합니다. 또한, Tiering Policy를 ALL로 설정하면 성능이 저하될 수 있습니다.
  - **C. FSx for Lustre (Persistent HDD):** HDD 스토리지는 SSD보다 지연 시간이 길고 처리량이 낮습니다. 서브 밀리초 지연 시간과 높은 처리량 요구 사항을 충족하지 못합니다.
  - **D. FSx for NetApp ONTAP (Tiering Policy: NONE):** NetApp ONTAP는 고성능 파일 시스템이지만, Lustre가 HPC 워크로드에 더 적합합니다.

**출제 의도:**
이 문제는 AWS에서 고성능 컴퓨팅 워크로드를 처리하기 위해 적합한 스토리지 솔루션을 선택하는 방법을 평가합니다. 특히, FSx for Lustre를 사용하여 서브 밀리초 지연 시간과 높은 처리량 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon FSx for Lustre:**
   - FSx for Lustre의 기본 개념과 사용 사례
   - FSx for Lustre를 사용하여 고성능 컴퓨팅 워크로드를 처리하는 방법

2. **스토리지 성능:**
   - SSD와 HDD 스토리지의 차이점
   - 고성능 스토리지 솔루션을 선택하는 방법

이 부분들을 공부하면, AWS에서 고성능 컴퓨팅 워크로드를 처리하기 위해 적합한 스토리지 솔루션을 선택하는 방법을 더 잘 이해할 수 있습니다.


# 300번
**정답: C. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.**

**해설:**
- **Amazon EC2 Reserved Instances:**
  - **비용 절감:** Reserved Instances는 장기적인 약정을 통해 On-Demand Instances보다 비용을 절감할 수 있습니다. 애플리케이션이 24시간, 7일 내내 실행되므로 Reserved Instances가 비용 효율적입니다.
  
- **Amazon Aurora Reserved Instances:**
  - **확장성:** Amazon Aurora는 고성능, 고가용성, 자동 확장 기능을 제공하는 관계형 데이터베이스 서비스입니다. 데이터베이스 스토리지가 시간이 지남에 따라 계속 증가하는 요구 사항을 충족할 수 있습니다.
  - **비용 절감:** Aurora Reserved Instances를 사용하면 On-Demand Instances보다 비용을 절감할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. EC2 Spot Instances와 S3:** Spot Instances는 언제든지 회수될 수 있으므로, 24/7 실행되는 애플리케이션에는 적합하지 않습니다. 또한, S3는 객체 스토리지로, 관계형 데이터베이스 요구 사항을 충족하지 않습니다.
- **B. EC2 Reserved Instances와 RDS On-Demand Instances:** RDS On-Demand Instances는 장기적인 비용 절감 측면에서 Reserved Instances보다 덜 효율적입니다.
- **D. EC2 On-Demand Instances와 RDS Reserved Instances:** EC2 On-Demand Instances는 장기적인 비용 절감 측면에서 Reserved Instances보다 덜 효율적입니다.

**출제 의도:**
이 문제는 AWS에서 24/7 실행되는 애플리케이션과 데이터베이스를 비용 효율적으로 마이그레이션하는 방법을 평가합니다. 특히, EC2 Reserved Instances와 Amazon Aurora Reserved Instances를 사용하여 비용을 절감하고, 데이터베이스 스토리지 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon EC2 Reserved Instances:**
   - Reserved Instances의 기본 개념과 사용 사례
   - Reserved Instances를 사용하여 비용을 절감하는 방법

2. **Amazon Aurora:**
   - Aurora의 기본 개념과 사용 사례
   - Aurora Reserved Instances를 사용하여 비용을 절감하고, 데이터베이스 스토리지 요구 사항을 충족하는 방법

이 부분들을 공부하면, AWS에서 24/7 실행되는 애플리케이션과 데이터베이스를 비용 효율적으로 마이그레이션하는 방법을 더 잘 이해할 수 있습니다.

# 308번
**정답: B. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time. C. Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.**

**해설:**
- **B. Trusted Advisor recommendations from the consolidated billing account:**
  - **통합된 뷰:** 통합 결제 계정을 사용하면 모든 연결된 계정의 Trusted Advisor 권장 사항을 한 번에 볼 수 있습니다. 이는 여러 계정에서 실행 중인 RDS 인스턴스를 효율적으로 검토하는 데 도움이 됩니다.

- **C. Trusted Advisor check for Amazon RDS Reserved Instance Optimization:**
  - **비용 절감:** Reserved Instance Optimization 체크는 온디맨드 인스턴스를 예약 인스턴스로 전환하여 비용을 절감할 수 있는 기회를 제공합니다. 이는 RDS 비용을 줄이는 데 중요한 역할을 합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Trusted Advisor recommendations from the account where the RDS instances are running:** 개별 계정에서 권장 사항을 확인하는 것은 가능하지만, 통합 결제 계정을 사용하면 모든 계정의 권장 사항을 한 번에 볼 수 있어 더 효율적입니다.
- **D. Trusted Advisor check for Amazon RDS Idle DB Instances:** 이 체크는 사용되지 않는 RDS 인스턴스를 식별하는 데 유용하지만, 고성능 RDS 인스턴스가 활성 상태로 실행 중이므로 이 체크는 덜 관련이 있습니다.
- **E. Trusted Advisor check for Amazon Redshift Reserved Node Optimization:** 이 체크는 Redshift에 대한 것이며, RDS 비용 절감과는 관련이 없습니다.

**출제 의도:**
이 문제는 AWS Trusted Advisor를 사용하여 여러 계정에서 실행 중인 RDS 인스턴스의 비용을 절감하는 방법을 평가합니다. 특히, 통합 결제 계정을 사용하여 모든 계정의 권장 사항을 한 번에 보고, RDS Reserved Instance Optimization 체크를 통해 비용 절감 기회를 식별하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Trusted Advisor:**
   - Trusted Advisor의 기본 개념과 사용 사례
   - 통합 결제 계정을 사용하여 여러 계정의 권장 사항을 확인하는 방법

2. **RDS 비용 최적화:**
   - RDS Reserved Instance Optimization 체크의 기본 개념과 사용 사례
   - RDS 비용을 절감하는 방법

이 부분들을 공부하면, AWS Trusted Advisor를 사용하여 여러 계정에서 실행 중인 RDS 인스턴스의 비용을 절감하는 방법을 더 잘 이해할 수 있습니다.


# 309번
### 정답
A. Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.

### 해설
Amazon S3 Storage Lens는 S3 스토리지 사용량 및 활동에 대한 포괄적인 메트릭과 인사이트를 제공하는 도구입니다. S3 Storage Lens 대시보드를 사용하면 S3 버킷의 액세스 패턴을 쉽게 분석할 수 있으며, 이는 운영 오버헤드를 최소화하면서 스토리지 비용을 최적화하는 데 도움이 됩니다.

각 옵션을 자세히 살펴보면:

- **A. Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.**
  - S3 Storage Lens는 S3 스토리지 사용량 및 활동에 대한 포괄적인 메트릭과 인사이트를 제공하며, 운영 오버헤드를 최소화하면서 S3 버킷의 액세스 패턴을 분석할 수 있습니다. 이는 가장 적은 운영 오버헤드로 목표를 달성할 수 있는 솔루션입니다.

- **B. Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.**
  - S3 대시보드는 기본적인 스토리지 정보를 제공하지만, S3 Storage Lens와 같은 고급 활동 메트릭을 제공하지 않습니다.

- **C. Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access patterns by using the metrics data with Amazon Athena.**
  - CloudWatch 메트릭과 Athena를 사용하여 데이터를 분석하는 것은 가능하지만, 설정 및 관리가 복잡할 수 있으며 운영 오버헤드가 증가할 수 있습니다.

- **D. Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using CloudTrail logs that are integrated with Amazon CloudWatch Logs.**
  - CloudTrail 로그를 사용하여 S3 객체 모니터링을 설정하고 CloudWatch Logs와 통합하여 데이터를 분석하는 것은 가능하지만, 설정 및 관리가 복잡할 수 있으며 운영 오버헤드가 증가할 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Amazon S3 Storage Lens**
   - S3 Storage Lens는 S3 스토리지 사용량 및 활동에 대한 포괄적인 메트릭과 인사이트를 제공하는 도구입니다.
   - S3 버킷의 액세스 패턴을 쉽게 분석할 수 있습니다.

2. **Amazon S3 대시보드**
   - S3 대시보드는 기본적인 스토리지 정보를 제공하지만, 고급 활동 메트릭을 제공하지 않습니다.

3. **Amazon CloudWatch 및 Amazon Athena**
   - CloudWatch는 AWS 리소스 및 애플리케이션을 모니터링하는 서비스입니다.
   - Athena는 S3 데이터를 쿼리할 수 있는 서버리스 대화형 쿼리 서비스입니다.
   - 두 서비스를 결합하여 데이터를 분석할 수 있지만, 설정 및 관리가 복잡할 수 있습니다.

4. **AWS CloudTrail 및 CloudWatch Logs**
   - CloudTrail은 AWS 계정의 API 호출을 기록하는 서비스입니다.
   - CloudWatch Logs는 로그 데이터를 모니터링하고 분석할 수 있는 서비스입니다.
   - 두 서비스를 결합하여 데이터를 분석할 수 있지만, 설정 및 관리가 복잡할 수 있습니다.

### 추가 학습 자료
- [Amazon S3 Storage Lens 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/storage-lens.html)
- [Amazon CloudWatch 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html)
- [Amazon Athena 공식 문서](https://docs.aws.amazon.com/ko_kr/athena/latest/ug/what-is.html)
- [AWS CloudTrail 공식 문서](https://docs.aws.amazon.com/ko_kr/awscloudtrail/latest/userguide/cloudtrail-user-guide.html)

이 자료들을 통해 S3 Storage Lens, CloudWatch, Athena, CloudTrail의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 312번
**정답: B. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EC2 instances as resources.**

**해설:**
- **AWS Backup:**
  - **운영 효율성:** AWS Backup은 중앙 집중식 관리 콘솔을 통해 백업 작업을 자동화하고 관리할 수 있는 서비스입니다. 이를 통해 운영 오버헤드를 최소화할 수 있습니다.
  - **리소스 추가:** AWS Backup을 사용하면 EC2 인스턴스와 EBS 볼륨을 리소스로 추가하여 백업 계획을 설정할 수 있습니다. 이는 EC2 인스턴스의 구성과 데이터를 모두 백업할 수 있음을 의미합니다.
  - **Cross-Region 복사:** AWS Backup은 백업을 다른 리전으로 자동으로 복사할 수 있는 기능을 제공하여, 재해 복구 요구 사항을 충족할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. AWS Lambda와 스냅샷:** Lambda 함수를 사용하여 스냅샷을 예약하고 다른 리전으로 복사할 수 있지만, 이는 추가적인 스크립팅과 관리 오버헤드를 요구합니다.
- **C. AWS Backup과 EBS 볼륨:** EBS 볼륨만을 리소스로 추가하면, EC2 인스턴스의 구성은 백업되지 않습니다. EC2 인스턴스 전체를 백업하는 것이 더 효율적입니다.
- **D. AWS Lambda와 스냅샷 (가용 영역):** 다른 가용 영역으로 스냅샷을 복사하는 것은 재해 복구 요구 사항을 충족하지 못합니다. 다른 리전으로 복사하는 것이 필요합니다.

**출제 의도:**
이 문제는 AWS에서 EC2 인스턴스와 EBS 볼륨을 효율적으로 백업하고, 재해 복구를 위해 다른 리전으로 복사하는 방법을 평가합니다. 특히, AWS Backup을 사용하여 운영 오버헤드를 최소화하면서 이러한 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Backup:**
   - AWS Backup의 기본 개념과 사용 사례
   - AWS Backup을 사용하여 EC2 인스턴스와 EBS 볼륨을 백업하고, 다른 리전으로 복사하는 방법

2. **재해 복구 (Disaster Recovery):**
   - 재해 복구 계획의 기본 개념과 사용 사례
   - Cross-Region 복사의 중요성과 구현 방법

이 부분들을 공부하면, AWS에서 EC2 인스턴스와 EBS 볼륨을 효율적으로 백업하고, 재해 복구를 위해 다른 리전으로 복사하는 방법을 더 잘 이해할 수 있습니다.


# 316번
**정답: C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.**

**해설:**
- **AWS Lambda:**
  - **비용 절감:** Lambda는 서버리스 컴퓨팅 서비스로, 사용한 만큼만 비용을 지불합니다. EC2 인스턴스를 계속 실행하는 것보다 비용 효율적입니다.
  - **자동 확장:** Lambda는 자동으로 확장되므로, SQS 큐에 추가되는 메시지 수가 증가해도 처리할 수 있습니다. 이는 메시지 처리량이 증가할 때 유연하게 대응할 수 있습니다.
  - **운영 오버헤드 감소:** Lambda를 사용하면 인프라를 관리할 필요가 없으므로 운영 오버헤드가 줄어듭니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Increase the size of the EC2 instance:** EC2 인스턴스의 크기를 늘리는 것은 비용을 증가시키며, 메시지 수가 증가할 때 유연하게 대응하기 어렵습니다.
- **B. Use Amazon EventBridge:** EventBridge를 사용하여 EC2 인스턴스를 끄는 것은 비용 절감에 도움이 될 수 있지만, 메시지 처리량이 증가할 때 유연하게 대응하기 어렵습니다.
- **D. Use AWS Systems Manager Run Command:** Systems Manager Run Command를 사용하여 스크립트를 온디맨드로 실행하는 것은 메시지 처리량이 증가할 때 유연하게 대응하기 어렵습니다.

**출제 의도:**
이 문제는 AWS에서 SQS 큐의 메시지를 효율적으로 처리하고, 비용을 절감하며, 메시지 처리량이 증가할 때 유연하게 대응하는 방법을 평가합니다. 특히, AWS Lambda를 사용하여 이러한 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Lambda:**
   - Lambda의 기본 개념과 사용 사례
   - Lambda를 사용하여 SQS 큐의 메시지를 처리하는 방법

2. **비용 절감 및 자동 확장:**
   - 서버리스 컴퓨팅의 비용 절감 효과
   - 자동 확장을 통해 메시지 처리량 증가에 대응하는 방법

이 부분들을 공부하면, AWS에서 SQS 큐의 메시지를 효율적으로 처리하고, 비용을 절감하며, 메시지 처리량이 증가할 때 유연하게 대응하는 방법을 더 잘 이해할 수 있습니다.

# 318번
**정답: A. Enable AWS CloudTrail and use it for auditing. D. Enable AWS Config and create rules for auditing and compliance purposes.**

**해설:**
- **AWS CloudTrail:**
  - **감사 및 추적:** CloudTrail은 AWS 계정에서 API 호출을 기록하여, 누가 어떤 작업을 수행했는지 추적할 수 있습니다. 이를 통해 EC2 인스턴스 프로비저닝 및 보안 그룹 규칙 변경과 같은 활동을 감사할 수 있습니다.

- **AWS Config:**
  - **구성 관리 및 규정 준수:** AWS Config는 AWS 리소스의 구성을 평가, 감사 및 평가할 수 있는 서비스입니다. Config 규칙을 사용하여 리소스가 특정 규정 준수 요구 사항을 충족하는지 확인할 수 있습니다. 이를 통해 EC2 인스턴스 크기 및 보안 그룹 규칙 변경을 모니터링하고 규정 준수 상태를 유지할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **B. 데이터 수명 주기 정책:** 데이터 수명 주기 정책은 주로 데이터 관리와 관련이 있으며, EC2 인스턴스 프로비저닝 및 보안 그룹 규칙 변경을 추적하는 데 적합하지 않습니다.
- **C. AWS Trusted Advisor:** Trusted Advisor는 비용 최적화, 보안, 성능, 내결함성 및 서비스 한도에 대한 권장 사항을 제공하지만, 실시간 구성 변경 추적 및 감사 기능을 제공하지 않습니다.
- **E. AWS CloudFormation:** CloudFormation은 인프라를 코드로 관리하는 도구로, 이전 리소스 구성을 복원할 수 있지만, 실시간 구성 변경 추적 및 감사 기능을 제공하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 리소스 프로비저닝 및 구성 변경을 추적하고 감사하는 방법을 평가합니다. 특히, AWS CloudTrail과 AWS Config를 사용하여 이러한 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS CloudTrail:**
   - CloudTrail의 기본 개념과 사용 사례
   - CloudTrail을 사용하여 API 호출을 기록하고 감사하는 방법

2. **AWS Config:**
   - Config의 기본 개념과 사용 사례
   - Config 규칙을 사용하여 리소스 구성을 평가하고 규정 준수 상태를 유지하는 방법

이 부분들을 공부하면, AWS에서 리소스 프로비저닝 및 구성 변경을 추적하고 감사하는 방법을 더 잘 이해할 수 있습니다.


# 324번
**정답: D. Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.**

**해설:**
- **AWS Storage Gateway Volume Gateway (Stored Volumes):**
  - **기존 인프라 변경 최소화:** Stored Volumes는 온프레미스 파일 서버에 iSCSI를 통해 마운트되므로, 기존 인프라를 크게 변경할 필요가 없습니다.
  - **로컬 데이터 접근:** Stored Volumes는 모든 데이터를 로컬에 저장하고, 백업을 위해 AWS로 복제합니다. 이는 사용자가 로컬에서 데이터를 즉시 접근할 수 있도록 하여 지연 시간을 최소화합니다.
  - **재해 복구:** 스냅샷을 사용하여 데이터를 Amazon EBS 볼륨으로 복원하고, 이를 Amazon EC2 인스턴스에 연결하여 재해 복구를 수행할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Amazon S3 File Gateway:** S3 File Gateway는 NFS 프로토콜을 사용하므로, 기존 애플리케이션을 수정해야 합니다. 또한, 로컬 캐시가 제한적이므로 지연 시간이 발생할 수 있습니다.
- **B. AWS Storage Gateway Tape Gateway:** Tape Gateway는 주로 백업 및 아카이브 용도로 사용되며, 실시간 데이터 접근을 제공하지 않습니다.
- **C. AWS Storage Gateway Volume Gateway (Cached Volumes):** Cached Volumes는 자주 액세스되는 데이터를 로컬에 캐시하지만, 모든 데이터를 로컬에 저장하지 않으므로 지연 시간이 발생할 수 있습니다.

**출제 의도:**
이 문제는 AWS에서 온프레미스 파일 스토리지의 재해 복구 계획을 구현하고, 사용자가 지연 시간 없이 데이터를 접근할 수 있도록 하는 방법을 평가합니다. 특히, AWS Storage Gateway Volume Gateway (Stored Volumes)를 사용하여 이러한 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **AWS Storage Gateway:**
   - Storage Gateway의 기본 개념과 사용 사례
   - Volume Gateway (Stored Volumes)와 Volume Gateway (Cached Volumes)의 차이점

2. **재해 복구 계획:**
   - 재해 복구 계획의 기본 개념과 사용 사례
   - 스냅샷을 사용하여 데이터를 복원하고, Amazon EBS 볼륨으로 복원하는 방법

이 부분들을 공부하면, AWS에서 온프레미스 파일 스토리지의 재해 복구 계획을 구현하고, 사용자가 지연 시간 없이 데이터를 접근할 수 있도록 하는 방법을 더 잘 이해할 수 있습니다.

# 326번
**정답: A. Move assets to S3 Intelligent-Tiering after 30 days. B. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.**

**해설:**
- **A. S3 Intelligent-Tiering:**
  - **비용 최적화:** S3 Intelligent-Tiering은 객체의 접근 패턴에 따라 자동으로 비용을 최적화합니다. 자주 접근되는 객체는 빈번한 접근 계층에, 덜 자주 접근되는 객체는 드문 접근 계층에 저장됩니다. 이는 접근 패턴이 불규칙한 객체에 적합합니다.
  - **높은 가용성 및 내구성:** S3 Intelligent-Tiering은 S3 Standard와 동일한 높은 가용성과 내구성을 제공합니다.

- **B. Incomplete Multipart Uploads 정리:**
  - **비용 절감:** 불완전한 멀티파트 업로드는 스토리지 비용을 증가시킬 수 있습니다. S3 Lifecycle 정책을 사용하여 불완전한 멀티파트 업로드를 정리하면 비용을 절감할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **C. Expired Object Delete Markers 정리:** 이 정책은 버전 관리를 사용하는 경우에만 유용하며, 비용 최적화와 직접적인 관련이 없습니다.
- **D. S3 Standard-IA:** S3 Standard-IA는 자주 접근되지 않는 객체에 적합하지만, 접근 패턴이 불규칙한 경우 비용이 증가할 수 있습니다.
- **E. S3 One Zone-IA:** S3 One Zone-IA는 단일 가용 영역에만 데이터를 저장하므로, 높은 가용성과 내구성을 요구하는 경우 적합하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 S3 스토리지 비용을 최적화하면서 높은 가용성과 내구성을 유지하는 방법을 평가합니다. 특히, S3 Intelligent-Tiering과 S3 Lifecycle 정책을 사용하여 이러한 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon S3 Storage Classes:**
   - S3 Intelligent-Tiering, S3 Standard-IA, S3 One Zone-IA의 기본 개념과 사용 사례
   - 각 스토리지 클래스의 비용 최적화 및 가용성/내구성 특성

2. **S3 Lifecycle Policies:**
   - S3 Lifecycle 정책의 기본 개념과 사용 사례
   - 불완전한 멀티파트 업로드 및 만료된 객체 삭제 마커를 정리하는 방법

이 부분들을 공부하면, AWS에서 S3 스토리지 비용을 최적화하면서 높은 가용성과 내구성을 유지하는 방법을 더 잘 이해할 수 있습니다.


# 329번
**정답: D. Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.**

**해설:**
- **Amazon Inspector:**
  - **취약점 스캔:** Amazon Inspector는 EC2 인스턴스에서 실행되는 애플리케이션의 보안 취약점을 자동으로 스캔하는 서비스입니다. 이를 통해 정기적으로 보안 스캔을 수행할 수 있습니다.
  - **보고서 생성:** Inspector는 각 인스턴스의 취약점 상태에 대한 보고서를 제공합니다.

- **AWS Systems Manager Patch Manager:**
  - **자동 패치:** Patch Manager는 EC2 인스턴스를 정기적으로 패치할 수 있는 기능을 제공합니다. 이를 통해 인스턴스가 최신 보안 패치를 적용받도록 할 수 있습니다.
  - **상태 보고:** Patch Manager는 각 인스턴스의 패치 상태에 대한 보고서를 생성할 수 있습니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Amazon Macie:** Macie는 데이터 보호 및 개인정보 식별 서비스로, 소프트웨어 취약점 스캔을 수행하지 않습니다. 또한, 각 인스턴스에 cron job을 설정하는 것은 관리 오버헤드가 큽니다.
- **B. Amazon GuardDuty:** GuardDuty는 AWS 계정과 리소스에 대한 위협 탐지 서비스로, 소프트웨어 취약점 스캔을 수행하지 않습니다. Session Manager는 패치 관리 기능을 제공하지 않습니다.
- **C. Amazon Detective:** Detective는 보안 조사 및 분석 서비스로, 소프트웨어 취약점 스캔을 수행하지 않습니다. EventBridge는 패치 관리 기능을 제공하지 않습니다.

**출제 의도:**
이 문제는 AWS에서 EC2 인스턴스의 보안 취약점을 스캔하고, 정기적으로 패치를 적용하며, 각 인스턴스의 패치 상태를 보고하는 방법을 평가합니다. 특히, Amazon Inspector와 AWS Systems Manager Patch Manager를 사용하여 이러한 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon Inspector:**
   - Inspector의 기본 개념과 사용 사례
   - Inspector를 사용하여 EC2 인스턴스의 보안 취약점을 스캔하는 방법

2. **AWS Systems Manager Patch Manager:**
   - Patch Manager의 기본 개념과 사용 사례
   - Patch Manager를 사용하여 EC2 인스턴스를 정기적으로 패치하고, 패치 상태를 보고하는 방법

이 부분들을 공부하면, AWS에서 EC2 인스턴스의 보안 취약점을 스캔하고, 정기적으로 패치를 적용하며, 각 인스턴스의 패치 상태를 보고하는 방법을 더 잘 이해할 수 있습니다.


# 333번
**정답: C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.**

**해설:**
- **EC2 Auto Scaling Scheduled Scaling Policy:**
  - **예측 가능한 트래픽 패턴:** 매월 첫날 자정에 월말 재무 계산 배치 작업이 실행될 때 애플리케이션의 CPU 사용률이 급증하는 패턴이 예측 가능합니다. 이러한 예측 가능한 트래픽 패턴에 대해 EC2 Auto Scaling의 예약된 스케일링 정책을 사용하여 미리 인스턴스를 추가할 수 있습니다.
  - **사전 준비:** 예약된 스케일링 정책을 사용하면 예상되는 트래픽 증가 전에 인스턴스를 추가하여 애플리케이션이 부하를 처리할 수 있도록 준비할 수 있습니다. 이는 CPU 사용률이 100%에 도달하여 애플리케이션이 중단되는 것을 방지합니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. Amazon CloudFront:** CloudFront는 주로 정적 콘텐츠를 캐싱하여 성능을 향상시키는 데 사용됩니다. CPU 사용률 급증 문제를 해결하지 못합니다.
- **B. EC2 Auto Scaling Simple Scaling Policy:** 단순 스케일링 정책은 CPU 사용률이 이미 높은 상태에서 인스턴스를 추가하므로, 트래픽 급증에 대한 사전 준비가 부족할 수 있습니다.
- **D. Amazon ElastiCache:** ElastiCache는 데이터베이스 캐싱을 통해 성능을 향상시킬 수 있지만, CPU 사용률 급증 문제를 직접 해결하지는 못합니다.

**출제 의도:**
이 문제는 AWS에서 예측 가능한 트래픽 패턴에 대해 애플리케이션의 성능을 최적화하고, 중단을 방지하는 방법을 평가합니다. 특히, EC2 Auto Scaling의 예약된 스케일링 정책을 사용하여 이러한 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **EC2 Auto Scaling:**
   - Auto Scaling의 기본 개념과 사용 사례
   - 예약된 스케일링 정책을 사용하여 예측 가능한 트래픽 패턴에 대응하는 방법

2. **트래픽 관리 및 성능 최적화:**
   - 예측 가능한 트래픽 패턴에 대해 애플리케이션의 성능을 최적화하는 방법
   - CPU 사용률 급증 문제를 해결하는 방법

이 부분들을 공부하면, AWS에서 예측 가능한 트래픽 패턴에 대해 애플리케이션의 성능을 최적화하고, 중단을 방지하는 방법을 더 잘 이해할 수 있습니다.

# 335번
**정답: B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.**

**해설:**
- **Amazon EBS Fast Snapshot Restore:**
  - **최소 초기화 지연:** EBS Fast Snapshot Restore(FSR)는 스냅샷에서 프로비저닝된 볼륨의 초기화 시간을 줄여줍니다. 이를 통해 EC2 인스턴스가 더 빠르게 시작될 수 있습니다.
  - **AMI 프로비저닝:** FSR을 사용하여 스냅샷에서 AMI를 프로비저닝하면, Auto Scaling 그룹에서 인스턴스를 시작할 때 초기화 지연이 최소화됩니다.

**다른 보기가 정답이 될 수 없는 이유:**
- **A. aws ec2 register-image:** 이 방법은 AMI를 생성하는 데 사용되지만, 초기화 지연을 줄이는 데 직접적인 도움이 되지 않습니다.
- **C. Amazon Data Lifecycle Manager (DLM):** DLM은 AMI와 스냅샷의 라이프사이클을 관리하는 데 유용하지만, 초기화 지연을 줄이는 데 직접적인 도움이 되지 않습니다.
- **D. AWS Backup lifecycle policies:** AWS Backup은 백업 및 복구를 관리하는 데 사용되며, 초기화 지연을 줄이는 데 직접적인 도움이 되지 않습니다.

**출제 의도:**
이 문제는 AWS에서 EC2 인스턴스를 빠르게 프로비저닝하고, 초기화 지연을 최소화하는 방법을 평가합니다. 특히, Amazon EBS Fast Snapshot Restore를 사용하여 이러한 요구 사항을 충족하는 방법을 이해하는 것이 목적입니다.

**공부해야 하는 부분:**
1. **Amazon EBS Fast Snapshot Restore:**
   - Fast Snapshot Restore의 기본 개념과 사용 사례
   - Fast Snapshot Restore를 사용하여 초기화 지연을 줄이는 방법

2. **Auto Scaling 그룹:**
   - Auto Scaling 그룹의 기본 개념과 사용 사례
   - Auto Scaling 그룹에서 AMI를 교체하는 방법

이 부분들을 공부하면, AWS에서 EC2 인스턴스를 빠르게 프로비저닝하고, 초기화 지연을 최소화하는 방법을 더 잘 이해할 수 있습니다.


# 338번
### 정답
D. Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region.

### 해설
Amazon Aurora 글로벌 데이터베이스는 여러 AWS 리전에 걸쳐 데이터베이스를 복제할 수 있는 기능을 제공합니다. 이는 고가용성과 재해 복구를 위한 최적의 솔루션입니다. 글로벌 데이터베이스를 설정하면 데이터가 자동으로 여러 리전에 복제되며, 재해 발생 시 빠르게 장애 조치를 수행할 수 있습니다. 최소 하나의 DB 인스턴스를 보조 리전에 지정하면 데이터 복제가 지속적으로 이루어지며, 재해 복구 시 빠르게 활성화할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB instance for the Aurora cluster in the secondary Region.**
  - MySQL 바이너리 로그 복제는 Aurora 글로벌 데이터베이스에 비해 설정 및 관리가 복잡할 수 있으며, 비용 효율적이지 않을 수 있습니다.

- **B. Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.**
  - 보조 리전에서 DB 인스턴스를 제거하면 데이터 복제가 중단되며, 재해 복구 시 데이터 손실이 발생할 수 있습니다.

- **C. Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora cluster in the secondary Region. Remove the DB instance from the secondary Region.**
  - AWS DMS는 데이터 마이그레이션에 적합하지만, 지속적인 데이터 복제 및 재해 복구를 위한 최적의 솔루션은 아닙니다. 또한, 보조 리전에서 DB 인스턴스를 제거하면 데이터 복제가 중단됩니다.

- **D. Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region.**
  - Aurora 글로벌 데이터베이스는 여러 리전에 걸쳐 데이터베이스를 복제할 수 있는 기능을 제공하며, 최소 하나의 DB 인스턴스를 보조 리전에 지정하면 데이터 복제가 지속적으로 이루어집니다. 이는 고가용성과 재해 복구를 위한 최적의 솔루션입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **Amazon Aurora 글로벌 데이터베이스**
   - Aurora 글로벌 데이터베이스는 여러 AWS 리전에 걸쳐 데이터베이스를 복제할 수 있는 기능을 제공합니다.
   - 고가용성과 재해 복구를 위한 최적의 솔루션입니다.

2. **MySQL 바이너리 로그 복제**
   - MySQL 바이너리 로그 복제는 데이터베이스 복제를 위한 방법 중 하나입니다.
   - 설정 및 관리가 복잡할 수 있으며, 비용 효율적이지 않을 수 있습니다.

3. **AWS Database Migration Service (DMS)**
   - DMS는 데이터베이스를 AWS로 마이그레이션하거나 AWS 내에서 데이터베이스를 복제하는 데 사용됩니다.
   - 지속적인 데이터 복제 및 재해 복구를 위한 최적의 솔루션은 아닙니다.

### 추가 학습 자료
- [Amazon Aurora 글로벌 데이터베이스 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html)
- [MySQL 바이너리 로그 복제 공식 문서](https://dev.mysql.com/doc/refman/8.0/en/replication.html)
- [AWS Database Migration Service (DMS) 공식 문서](https://docs.aws.amazon.com/ko_kr/dms/latest/userguide/Welcome.html)

이 자료들을 통해 Aurora 글로벌 데이터베이스, MySQL 바이너리 로그 복제, DMS의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 348번
### 정답
A. Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA). Reserve capacity for the forecasted workload.

### 해설
회사는 일정하고 예측 가능한 데이터 워크로드를 가지고 있으며, DynamoDB 비용을 예산 내에서 유지하고자 합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하기 위해서는 프로비저닝된 모드와 DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA)를 사용하는 것이 적절합니다. 프로비저닝된 모드는 일정한 워크로드에 대해 예측 가능한 비용을 제공하며, 예약 용량을 사용하면 비용을 더욱 절감할 수 있습니다. DynamoDB Standard-IA는 자주 액세스되지 않는 데이터를 저장하는 데 비용 효율적입니다.

각 옵션을 자세히 살펴보면:

- **A. Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA). Reserve capacity for the forecasted workload.**
  - 프로비저닝된 모드와 DynamoDB Standard-IA를 사용하면 일정한 워크로드에 대해 예측 가능한 비용을 제공하며, 예약 용량을 통해 비용을 절감할 수 있습니다. 이는 회사의 요구 사항을 가장 비용 효율적으로 충족합니다.

- **B. Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).**
  - 프로비저닝된 모드를 사용하면 일정한 워크로드에 대해 예측 가능한 비용을 제공하지만, DynamoDB Standard-IA를 사용하지 않으면 비용 절감 효과가 떨어질 수 있습니다.

- **C. Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high enough to accommodate changes in the workload.**
  - 온디맨드 모드는 워크로드가 일정하지 않거나 예측하기 어려운 경우에 적합하지만, 일정한 워크로드에 대해서는 비용이 더 많이 들 수 있습니다.

- **D. Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs) with reserved capacity.**
  - 온디맨드 모드에서는 예약 용량을 사용할 수 없습니다. 이는 잘못된 옵션입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Amazon DynamoDB 프로비저닝된 모드**
   - 프로비저닝된 모드는 일정한 워크로드에 대해 예측 가능한 비용을 제공합니다.
   - 읽기 용량 단위(RCU)와 쓰기 용량 단위(WCU)를 지정하여 용량을 예약할 수 있습니다.

2. **DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA)**
   - DynamoDB Standard-IA는 자주 액세스되지 않는 데이터를 저장하는 데 비용 효율적입니다.
   - 프로비저닝된 모드와 함께 사용할 수 있습니다.

3. **Amazon DynamoDB 온디맨드 모드**
   - 온디맨드 모드는 워크로드가 일정하지 않거나 예측하기 어려운 경우에 적합합니다.
   - 사용한 만큼만 비용을 지불하지만, 일정한 워크로드에 대해서는 비용이 더 많이 들 수 있습니다.

### 추가 학습 자료
- [Amazon DynamoDB 프로비저닝된 모드 공식 문서](https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/HowItWorks.Provisioned.html)
- [DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA) 공식 문서](https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/HowItWorks.StorageClasses.html)
- [Amazon DynamoDB 온디맨드 모드 공식 문서](https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/HowItWorks.OnDemand.html)

이 자료들을 통해 DynamoDB의 다양한 모드와 기능을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 358번
### 정답: **C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.**

### 해설:

이 문제는 **이미지 동적 처리**와 **적절한 형식으로 클라이언트에게 제공**하는 요구 사항을 충족하는 솔루션을 찾는 것입니다. 회사는 **Amazon S3**에 저장된 이미지를 **동적으로 크기 조정**하고, 적절한 형식으로 클라이언트에게 제공하려고 합니다.

#### 각 옵션 분석:

- **A. Install an external image management library on an EC2 instance. Use the image management library to process the images.**
  - 이 옵션은 이미지를 **EC2 인스턴스에서 처리**하는 방법입니다. 그러나, EC2 인스턴스를 사용하면 **운영 오버헤드**가 커지며, 특히 대규모로 이미지를 처리하고 관리해야 하는 상황에서는 비효율적입니다. 서버 관리와 유지 보수 비용이 많이 들 수 있습니다.

- **B. Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.**
  - **CloudFront의 오리진 요청 정책**은 HTTP 요청에 따라 CloudFront가 오리진에 대한 요청을 어떻게 처리할지를 정의하지만, **이미지 크기 조정 및 형식 변환**을 자동으로 처리하지 않습니다. 따라서 이 옵션은 이미지를 처리하는 요구 사항을 충족하지 못합니다.

- **C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.**
  - **Lambda@Edge**는 **CloudFront** 배포와 연동하여 요청 및 응답을 처리할 수 있습니다. 이 솔루션은 **이미지 관리 라이브러리**를 Lambda@Edge 함수에 배포하여 이미지 크기 조정 및 형식 변환을 **클라우드 엣지에서 처리**할 수 있게 합니다. 이렇게 하면 **운영 오버헤드가 적고, 전 세계에 분산된 클라이언트들에게 빠르게 서비스를 제공**할 수 있습니다.
  - 이 방법은 EC2 인스턴스를 사용하지 않고도 이미지의 동적 처리를 **효율적으로** 할 수 있는 솔루션입니다.

- **D. Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.**
  - **CloudFront 응답 헤더 정책**은 응답 헤더를 관리하는 데 사용됩니다. 그러나 이미지를 **동적으로 크기 조정하거나 형식을 변환**하는 기능은 지원하지 않으므로 이 옵션은 요구 사항을 충족하지 않습니다.

### 결론:
**Lambda@Edge**를 사용하여 이미지 처리를 엣지에서 처리하는 것이 가장 효율적이며, 운영 오버헤드를 최소화하면서 요구 사항을 충족할 수 있습니다. **C** 옵션이 가장 적합한 솔루션입니다.


# 351번
### 정답
D. Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.

### 해설
회사는 이벤트 기반 아키텍처로 전환하고, 서버리스 개념을 사용하여 워크플로우의 다양한 측면을 수행하면서 운영 오버헤드를 최소화하려고 합니다. AWS Step Functions는 서버리스 워크플로우 서비스로, 상태 머신을 사용하여 여러 AWS Lambda 함수를 조정하고 관리할 수 있습니다. 이를 통해 분산된 이벤트 기반 아키텍처를 구축하고, 운영 오버헤드를 최소화할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workflow steps.**
  - AWS Glue는 데이터 통합 서비스로, ETL(Extract, Transform, Load) 작업에 적합합니다. 그러나 일반적인 워크플로우 관리에는 적합하지 않습니다.

- **B. Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workflow steps on the EC2 instances.**
  - EC2 인스턴스를 사용하면 서버를 관리해야 하므로 운영 오버헤드가 증가합니다. 서버리스 개념을 사용하려는 요구 사항을 충족하지 않습니다.

- **C. Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a schedule to process the workflow steps.**
  - EventBridge는 이벤트 기반 아키텍처를 구축하는 데 유용하지만, 복잡한 워크플로우 관리에는 적합하지 않습니다. Step Functions가 더 적합한 솔루션입니다.

- **D. Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.**
  - Step Functions는 상태 머신을 사용하여 여러 Lambda 함수를 조정하고 관리할 수 있습니다. 이를 통해 분산된 이벤트 기반 아키텍처를 구축하고, 운영 오버헤드를 최소화할 수 있습니다. 이는 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **AWS Step Functions**
   - Step Functions는 서버리스 워크플로우 서비스로, 상태 머신을 사용하여 여러 AWS Lambda 함수를 조정하고 관리할 수 있습니다.
   - 복잡한 워크플로우를 시각적으로 설계하고, 상태 전환을 관리할 수 있습니다.

2. **AWS Lambda**
   - Lambda는 서버리스 컴퓨팅 서비스로, 이벤트에 응답하여 코드를 실행할 수 있습니다.
   - Step Functions와 통합하여 워크플로우의 각 단계를 처리할 수 있습니다.

3. **Amazon EventBridge**
   - EventBridge는 이벤트 기반 아키텍처를 구축할 수 있는 서비스입니다.
   - 이벤트를 기반으로 Lambda 함수를 호출할 수 있지만, 복잡한 워크플로우 관리에는 적합하지 않습니다.

4. **AWS Glue**
   - Glue는 데이터 통합 서비스로, ETL 작업에 적합합니다.
   - 일반적인 워크플로우 관리에는 적합하지 않습니다.

### 추가 학습 자료
- [AWS Step Functions 공식 문서](https://docs.aws.amazon.com/ko_kr/step-functions/latest/dg/welcome.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)
- [Amazon EventBridge 공식 문서](https://docs.aws.amazon.com/ko_kr/eventbridge/latest/userguide/what-is-amazon-eventbridge.html)
- [AWS Glue 공식 문서](https://docs.aws.amazon.com/ko_kr/glue/latest/dg/what-is-glue.html)

이 자료들을 통해 Step Functions, Lambda, EventBridge, Glue의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 353번
### 정답
B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.

### 해설
회사는 데이터베이스 계층을 완전 관리형 솔루션으로 이전하고, 고가용성과 장애 내성을 유지하면서 비용을 절감하고 성능을 안정화하려고 합니다. Amazon RDS는 완전 관리형 데이터베이스 서비스로, Multi-AZ 배포를 통해 고가용성과 장애 내성을 제공합니다. General Purpose SSD (gp2) EBS 볼륨은 비용 효율적이며, 1,000 IOPS의 읽기 및 쓰기 트래픽을 처리할 수 있습니다. 이는 회사의 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.**
  - io2 Block Express EBS 볼륨은 높은 성능을 제공하지만, 비용이 더 많이 들 수 있습니다. 회사의 요구 사항을 충족할 수 있지만, 비용 효율적이지 않습니다.

- **B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.**
  - gp2 EBS 볼륨은 비용 효율적이며, 1,000 IOPS의 읽기 및 쓰기 트래픽을 처리할 수 있습니다. Multi-AZ 배포를 통해 고가용성과 장애 내성을 제공하며, 회사의 요구 사항을 가장 비용 효율적으로 충족할 수 있습니다.

- **C. Use Amazon S3 Intelligent-Tiering access tiers.**
  - S3 Intelligent-Tiering은 객체 스토리지 솔루션으로, 데이터베이스 워크로드에 적합하지 않습니다.

- **D. Use two large EC2 instances to host the database in active-passive mode.**
  - 두 개의 EC2 인스턴스를 사용하여 데이터베이스를 호스팅하는 것은 관리 오버헤드가 증가하며, 비용 효율적이지 않습니다. 또한, 완전 관리형 솔루션이 아닙니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **Amazon RDS (Relational Database Service)**
   - RDS는 완전 관리형 관계형 데이터베이스 서비스로, 다양한 데이터베이스 엔진을 지원합니다.
   - Multi-AZ 배포를 통해 고가용성과 장애 내성을 제공합니다.

2. **EBS 볼륨 유형**
   - **General Purpose SSD (gp2)**: 비용 효율적이며, 다양한 워크로드에 적합합니다.
   - **Provisioned IOPS SSD (io2)**: 높은 성능을 제공하지만, 비용이 더 많이 들 수 있습니다.

3. **Amazon S3 Intelligent-Tiering**
   - S3 Intelligent-Tiering은 객체 스토리지 솔루션으로, 데이터베이스 워크로드에 적합하지 않습니다.

### 추가 학습 자료
- [Amazon RDS 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Welcome.html)
- [Amazon EBS 볼륨 유형 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ebs-volume-types.html)
- [Amazon S3 Intelligent-Tiering 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/storage-class-intro.html#sc-dynamic-data-access)

이 자료들을 통해 RDS, EBS 볼륨 유형, S3 Intelligent-Tiering의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 359번
### 정답
C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

### 해설
병원의 요구 사항은 PHI(Protected Health Information)를 전송 중 및 저장 중에 암호화하고, 데이터 저장 시 암호화 키를 관리하는 것입니다. 이를 위해 S3 버킷 정책에서 `aws:SecureTransport` 조건을 사용하여 HTTPS(TLS)를 통한 암호화된 연결만 허용하고, 서버 측 암호화(SSE-KMS)를 사용하여 AWS KMS 키로 데이터를 암호화해야 합니다. KMS 키 관리를 병원의 컴플라이언스 팀에 할당하면 요구 사항을 충족할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.**
  - ACM에서 SSL/TLS 인증서를 생성하고 S3에 연결하는 것은 적절하지 않습니다. S3 버킷 정책에서 `aws:SecureTransport` 조건을 사용하는 것이 더 적절합니다.

- **B. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.**
  - SSE-S3는 S3 관리형 키를 사용하므로, 컴플라이언스 팀이 키를 관리할 수 없습니다.

- **C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.**
  - `aws:SecureTransport` 조건을 사용하여 HTTPS(TLS)를 통한 암호화된 연결만 허용하고, SSE-KMS를 사용하여 KMS 키로 데이터를 암호화하면 요구 사항을 충족할 수 있습니다. KMS 키 관리를 컴플라이언스 팀에 할당할 수 있습니다.

- **D. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie.**
  - Amazon Macie는 민감한 데이터를 보호하는 데 유용하지만, 데이터 저장 시 암호화 키를 관리하는 데는 적합하지 않습니다.

따라서, 병원의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Amazon S3 버킷 정책**
   - S3 버킷 정책을 사용하여 버킷에 대한 접근을 제어할 수 있습니다.
   - `aws:SecureTransport` 조건을 사용하여 HTTPS(TLS)를 통한 암호화된 연결만 허용할 수 있습니다.

2. **AWS KMS (Key Management Service)**
   - KMS는 암호화 키를 생성하고 관리할 수 있는 서비스입니다.
   - SSE-KMS를 사용하여 데이터를 암호화하고, KMS 키 관리를 특정 사용자나 그룹에 할당할 수 있습니다.

3. **서버 측 암호화 (SSE)**
   - SSE는 S3에서 데이터를 저장할 때 자동으로 암호화하는 기능입니다.
   - SSE-KMS는 KMS 키를 사용하여 데이터를 암호화합니다.

### 추가 학습 자료
- [Amazon S3 버킷 정책 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/bucket-policies.html)
- [AWS KMS 공식 문서](https://docs.aws.amazon.com/ko_kr/kms/latest/developerguide/overview.html)
- [Amazon S3 서버 측 암호화 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/serv-side-encryption.html)

이 자료들을 통해 S3 버킷 정책, KMS, 서버 측 암호화의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 362번
### 정답
B. Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.
E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.

### 해설
메시지가 전송된 순서대로 수신되어야 하는 경우, Amazon Kinesis 데이터 스트림과 Amazon SQS FIFO 큐는 모두 메시지 순서를 보장하는 데 적합한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Write the messages to an Amazon DynamoDB table with the payment ID as the partition key.**
  - DynamoDB는 메시지 순서를 보장하지 않습니다. 따라서 이 옵션은 적절하지 않습니다.

- **B. Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.**
  - Kinesis 데이터 스트림은 파티션 키를 사용하여 메시지를 분산시키며, 동일한 파티션 키를 가진 메시지의 순서를 보장합니다. 따라서 이 옵션은 적절합니다.

- **C. Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key.**
  - ElastiCache for Memcached는 메시지 순서를 보장하지 않습니다. 따라서 이 옵션은 적절하지 않습니다.

- **D. Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the message attribute to use the payment ID.**
  - SQS 표준 큐는 메시지 순서를 보장하지 않습니다. 따라서 이 옵션은 적절하지 않습니다.

- **E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.**
  - SQS FIFO 큐는 메시지 그룹 ID를 사용하여 메시지의 순서를 보장합니다. 따라서 이 옵션은 적절합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B와 E입니다.

### 필요한 학습 내용

1. **Amazon Kinesis 데이터 스트림**
   - Kinesis 데이터 스트림은 실시간으로 데이터를 수집하고 처리할 수 있는 서비스입니다.
   - 파티션 키를 사용하여 메시지를 분산시키며, 동일한 파티션 키를 가진 메시지의 순서를 보장합니다.

2. **Amazon SQS FIFO 큐**
   - SQS FIFO 큐는 메시지의 순서를 보장하며, 메시지 그룹 ID를 사용하여 메시지의 순서를 제어할 수 있습니다.
   - 메시지의 중복을 방지하고, 정확히 한 번만 처리되도록 보장합니다.

### 추가 학습 자료
- [Amazon Kinesis 데이터 스트림 공식 문서](https://docs.aws.amazon.com/ko_kr/streams/latest/dev/introduction.html)
- [Amazon SQS FIFO 큐 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html)

이 자료들을 통해 Kinesis 데이터 스트림과 SQS FIFO 큐의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 363번
### 정답
B. Amazon Simple Notification Service (Amazon SNS) FIFO topics

### 해설
Amazon SNS FIFO(First-In-First-Out) 주제는 메시지의 순서를 보장하면서 여러 구독자에게 메시지를 동시에 전달할 수 있는 기능을 제공합니다. 게임 시스템에서 리더보드, 매치메이킹, 인증 서비스와 같은 여러 서비스에 이벤트를 동시에 보내면서 이벤트의 순서를 보장해야 하는 경우, SNS FIFO 주제가 적합한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Amazon EventBridge event bus**
  - EventBridge는 이벤트 기반 아키텍처를 구축할 수 있는 서비스로, 이벤트의 순서를 보장하지 않습니다.

- **B. Amazon Simple Notification Service (Amazon SNS) FIFO topics**
  - SNS FIFO 주제는 메시지의 순서를 보장하면서 여러 구독자에게 메시지를 동시에 전달할 수 있습니다. 이는 게임 시스템의 요구 사항을 충족하는 가장 적절한 솔루션입니다.

- **C. Amazon Simple Notification Service (Amazon SNS) standard topics**
  - SNS 표준 주제는 메시지의 순서를 보장하지 않습니다.

- **D. Amazon Simple Queue Service (Amazon SQS) FIFO queues**
  - SQS FIFO 큐는 메시지의 순서를 보장하지만, 메시지를 여러 구독자에게 동시에 전달하는 데 적합하지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **Amazon SNS FIFO 주제**
   - SNS FIFO 주제는 메시지의 순서를 보장하면서 여러 구독자에게 메시지를 동시에 전달할 수 있습니다.
   - 메시지 그룹 ID를 사용하여 메시지의 순서를 제어할 수 있습니다.

2. **Amazon EventBridge**
   - EventBridge는 이벤트 기반 아키텍처를 구축할 수 있는 서비스입니다.
   - 이벤트의 순서를 보장하지 않습니다.

3. **Amazon SNS 표준 주제**
   - SNS 표준 주제는 메시지를 여러 구독자에게 동시에 전달할 수 있지만, 메시지의 순서를 보장하지 않습니다.

4. **Amazon SQS FIFO 큐**
   - SQS FIFO 큐는 메시지의 순서를 보장하지만, 메시지를 여러 구독자에게 동시에 전달하는 데 적합하지 않습니다.

### 추가 학습 자료
- [Amazon SNS FIFO 주제 공식 문서](https://docs.aws.amazon.com/ko_kr/sns/latest/dg/fifo-topics.html)
- [Amazon EventBridge 공식 문서](https://docs.aws.amazon.com/ko_kr/eventbridge/latest/userguide/what-is-amazon-eventbridge.html)
- [Amazon SNS 표준 주제 공식 문서](https://docs.aws.amazon.com/ko_kr/sns/latest/dg/welcome.html)
- [Amazon SQS FIFO 큐 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html)

이 자료들을 통해 SNS FIFO 주제, EventBridge, SNS 표준 주제, SQS FIFO 큐의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 364번
### 정답
B. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.
D. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.

### 해설
병원의 요구 사항은 데이터가 전송 중 및 저장 중에 암호화되고, 병원의 승인된 인원만 데이터에 접근할 수 있도록 하는 것입니다. 이를 위해 SQS와 SNS 모두에서 서버 측 암호화를 활성화하고, AWS KMS 고객 관리 키를 사용하여 키 정책을 적용해야 합니다. 또한, TLS를 통해 암호화된 연결만 허용하도록 조건을 설정해야 합니다.

각 옵션을 자세히 살펴보면:

- **A. Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.**
  - SQS 구성 요소에 서버 측 암호화를 활성화하고 기본 키 정책을 업데이트하는 것은 적절하지만, TLS를 통한 암호화된 연결을 설정하는 조건이 포함되어 있지 않습니다.

- **B. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.**
  - SNS 구성 요소에 서버 측 암호화를 활성화하고, KMS 고객 관리 키를 사용하여 키 정책을 적용하는 것은 적절합니다.

- **C. Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.**
  - SNS 구성 요소에 암호화를 활성화하고, 기본 키 정책을 업데이트하며, TLS를 통한 암호화된 연결을 설정하는 조건을 추가하는 것은 적절하지만, SQS 구성 요소에 대한 설정이 포함되어 있지 않습니다.

- **D. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.**
  - SQS 구성 요소에 서버 측 암호화를 활성화하고, KMS 고객 관리 키를 사용하여 키 정책을 적용하며, TLS를 통한 암호화된 연결을 설정하는 조건을 추가하는 것은 적절합니다.

- **E. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.**
  - SQS 구성 요소에 서버 측 암호화를 활성화하고, KMS 고객 관리 키를 사용하며, IAM 정책을 적용하는 것은 적절하지만, SNS 구성 요소에 대한 설정이 포함되어 있지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B와 D입니다.

### 필요한 학습 내용

1. **Amazon SQS**
   - SQS는 메시지 큐 서비스로, 메시지를 안전하게 전송하고 저장할 수 있습니다.
   - 서버 측 암호화를 활성화하여 데이터를 보호할 수 있습니다.

2. **Amazon SNS**
   - SNS는 메시지 전송 서비스로, 주제 기반의 메시지를 전송할 수 있습니다.
   - 서버 측 암호화를 활성화하여 데이터를 보호할 수 있습니다.

3. **AWS KMS (Key Management Service)**
   - KMS는 암호화 키를 생성하고 관리할 수 있는 서비스입니다.
   - 고객 관리 키를 사용하여 키 정책을 적용하고, 승인된 인원만 키를 사용할 수 있도록 제한할 수 있습니다.

4. **TLS (Transport Layer Security)**
   - TLS는 데이터 전송 중에 암호화를 제공하여 데이터를 보호합니다.
   - 정책 조건을 설정하여 TLS를 통한 암호화된 연결만 허용할 수 있습니다.

### 추가 학습 자료
- [Amazon SQS 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html)
- [Amazon SNS 공식 문서](https://docs.aws.amazon.com/ko_kr/sns/latest/dg/welcome.html)
- [AWS KMS 공식 문서](https://docs.aws.amazon.com/ko_kr/kms/latest/developerguide/overview.html)
- [TLS 공식 문서](https://tools.ietf.org/html/rfc5246)

이 자료들을 통해 SQS, SNS, KMS, TLS의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 365번
### 정답
C. Automated backups

### 해설
Amazon RDS의 자동 백업 기능은 데이터베이스를 특정 시점으로 복구할 수 있는 기능을 제공합니다. 자동 백업은 데이터베이스 인스턴스의 전체 스냅샷을 정기적으로 생성하고, 트랜잭션 로그를 지속적으로 백업하여 특정 시점으로 복구할 수 있도록 합니다. 이를 통해 데이터베이스를 5분 전의 상태로 복구할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Read replicas**
  - 읽기 전용 복제본은 데이터베이스의 읽기 성능을 향상시키기 위해 사용되며, 데이터 복구를 위한 기능은 제공하지 않습니다.

- **B. Manual snapshots**
  - 수동 스냅샷은 사용자가 직접 생성해야 하며, 지속적인 트랜잭션 로그 백업을 제공하지 않으므로 특정 시점으로 복구하는 데 적합하지 않습니다.

- **C. Automated backups**
  - 자동 백업은 정기적인 스냅샷과 지속적인 트랜잭션 로그 백업을 통해 특정 시점으로 데이터베이스를 복구할 수 있는 기능을 제공합니다. 이는 5분 전의 상태로 복구할 수 있는 요구 사항을 충족합니다.

- **D. Multi-AZ deployments**
  - Multi-AZ 배포는 고가용성과 장애 복구를 제공하지만, 데이터 복구를 위한 특정 시점 복구 기능은 제공하지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Amazon RDS 자동 백업**
   - 자동 백업은 데이터베이스 인스턴스의 전체 스냅샷을 정기적으로 생성하고, 트랜잭션 로그를 지속적으로 백업하여 특정 시점으로 복구할 수 있도록 합니다.
   - 최대 35일 동안 백업을 보관할 수 있습니다.

2. **Amazon RDS 수동 스냅샷**
   - 수동 스냅샷은 사용자가 직접 생성해야 하며, 지속적인 트랜잭션 로그 백업을 제공하지 않습니다.

3. **Amazon RDS 읽기 전용 복제본**
   - 읽기 전용 복제본은 데이터베이스의 읽기 성능을 향상시키기 위해 사용됩니다.
   - 데이터 복구를 위한 기능은 제공하지 않습니다.

4. **Amazon RDS Multi-AZ 배포**
   - Multi-AZ 배포는 고가용성과 장애 복구를 제공하지만, 데이터 복구를 위한 특정 시점 복구 기능은 제공하지 않습니다.

### 추가 학습 자료
- [Amazon RDS 자동 백업 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html)
- [Amazon RDS 수동 스냅샷 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html)
- [Amazon RDS 읽기 전용 복제본 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_ReadRepl.html)
- [Amazon RDS Multi-AZ 배포 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html)

이 자료들을 통해 Amazon RDS의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.



# 366번
### 정답
D. Implement API usage plans and API keys to limit the access of users who do not have a subscription.

### 해설
API Gateway의 API 사용 계획(API usage plans)과 API 키를 사용하면 구독이 없는 사용자의 접근을 제한할 수 있습니다. API 사용 계획을 통해 각 사용자에게 할당된 API 키를 기반으로 요청을 제한할 수 있으며, 구독이 있는 사용자에게만 특정 API 엔드포인트에 접근할 수 있는 권한을 부여할 수 있습니다. 이는 최소한의 운영 오버헤드로 구독 기반 접근 제어를 구현할 수 있는 방법입니다.

각 옵션을 자세히 살펴보면:

- **A. Enable API caching and throttling on the API Gateway API.**
  - API 캐싱과 스로틀링은 성능 최적화와 요청 제한을 위한 기능이지만, 구독 기반 접근 제어를 구현하는 데는 적합하지 않습니다.

- **B. Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.**
  - AWS WAF는 웹 애플리케이션 방화벽으로, 보안 규칙을 설정하여 트래픽을 필터링할 수 있지만, 구독 기반 접근 제어를 구현하는 데는 적합하지 않습니다.

- **C. Apply fine-grained IAM permissions to the premium content in the DynamoDB table.**
  - 세밀한 IAM 권한을 적용하는 것은 가능하지만, 이는 DynamoDB 테이블 수준에서의 접근 제어로, API Gateway와 Lambda 함수의 접근 제어를 구현하는 데는 적합하지 않습니다.

- **D. Implement API usage plans and API keys to limit the access of users who do not have a subscription.**
  - API 사용 계획과 API 키를 사용하면 구독이 없는 사용자의 접근을 제한할 수 있습니다. 이는 최소한의 운영 오버헤드로 구독 기반 접근 제어를 구현할 수 있는 방법입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **API Gateway API 사용 계획 (API Usage Plans)**
   - API 사용 계획을 통해 각 사용자에게 할당된 API 키를 기반으로 요청을 제한할 수 있습니다.
   - 구독이 있는 사용자에게만 특정 API 엔드포인트에 접근할 수 있는 권한을 부여할 수 있습니다.

2. **API 키 (API Keys)**
   - API 키를 사용하여 각 사용자의 요청을 식별하고, API 사용 계획과 연계하여 접근을 제어할 수 있습니다.

3. **Amazon Cognito**
   - Cognito 사용자 풀을 사용하여 애플리케이션의 개별 사용자를 식별할 수 있습니다.
   - 사용자 인증 및 권한 부여를 관리할 수 있습니다.

### 추가 학습 자료
- [Amazon API Gateway 사용 계획 공식 문서](https://docs.aws.amazon.com/ko_kr/apigateway/latest/developerguide/api-gateway-api-usage-plans.html)
- [Amazon API Gateway API 키 공식 문서](https://docs.aws.amazon.com/ko_kr/apigateway/latest/developerguide/api-gateway-api-key-source.html)
- [Amazon Cognito 공식 문서](https://docs.aws.amazon.com/ko_kr/cognito/latest/developerguide/what-is-amazon-cognito.html)

이 자료들을 통해 API Gateway 사용 계획과 API 키, Cognito의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 371번
### 출제 의도
이 문제는 Amazon EKS 클러스터와 Amazon EBS 볼륨을 사용하여 데이터를 암호화하는 방법을 이해하고, 최소한의 운영 오버헤드로 고객 관리 키를 사용하여 데이터를 암호화할 수 있는 능력을 평가합니다.

### 정답
C. Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.
D. Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.

### 해설
회사는 Amazon EKS 클러스터를 사용하여 디지털 미디어 스트리밍 애플리케이션을 호스팅하고, Amazon EBS 볼륨을 사용하여 데이터를 저장합니다. 모든 데이터를 고객 관리 키를 사용하여 암호화해야 합니다. 최소한의 운영 오버헤드로 이 요구 사항을 충족하려면 다음 두 가지 조치를 취해야 합니다:

- **C. Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.**
  - AWS 관리 콘솔에서 EBS 암호화를 기본값으로 설정하고, 고객 관리 키를 기본 키로 선택하면, 모든 새로 생성되는 EBS 볼륨이 자동으로 암호화됩니다. 이는 운영 오버헤드를 최소화하는 데 도움이 됩니다.

- **D. Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.**
  - EKS 클러스터를 생성하고, 고객 관리 키에 대한 권한을 부여하는 IAM 역할을 생성하여 클러스터에 연결하면, 클러스터 내에서 생성되는 모든 리소스가 해당 키를 사용할 수 있습니다.

다른 옵션을 살펴보면:

- **A. Use a Kubernetes plugin that uses the customer managed key to perform data encryption.**
  - Kubernetes 플러그인을 사용하는 것은 추가적인 설정과 관리가 필요하며, 운영 오버헤드를 증가시킬 수 있습니다.

- **B. After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer managed key.**
  - 클러스터 생성 후에 EBS 볼륨을 찾아 암호화를 활성화하는 것은 수동 작업이 필요하며, 운영 오버헤드를 증가시킵니다.

- **E. Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed key to encrypt the EBS volumes.**
  - Kubernetes 시크릿을 사용하여 키를 저장하고 EBS 볼륨을 암호화하는 것은 복잡하며, 운영 오버헤드를 증가시킬 수 있습니다.

따라서, 최소한의 운영 오버헤드로 요구 사항을 충족하는 솔루션은 옵션 C와 D의 조합입니다.

### 필요한 학습 내용

1. **Amazon EKS (Elastic Kubernetes Service)**
   - EKS 클러스터 생성 및 관리 방법
   - EKS 클러스터와 IAM 역할 연동 방법

2. **Amazon EBS (Elastic Block Store)**
   - EBS 볼륨 암호화 방법
   - EBS 암호화를 기본값으로 설정하는 방법

3. **AWS Key Management Service (KMS)**
   - 고객 관리 키 (Customer Managed Key) 생성 및 관리 방법
   - KMS 키를 사용하여 EBS 볼륨 암호화 방법
   - IAM 역할을 통해 KMS 키에 대한 권한 부여 방법

### 추가 학습 자료
- [Amazon EKS 공식 문서](https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/what-is-eks.html)
- [Amazon EBS 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/AmazonEBS.html)
- [AWS KMS 공식 문서](https://docs.aws.amazon.com/ko_kr/kms/latest/developerguide/overview.html)
- [EBS 암호화 기본값 설정](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/EBSEncryption.html#encryption-by-default)

이 자료들을 통해 EKS와 EBS의 다양한 기능과 보안 설정을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 376번
### 출제 의도
이 문제는 Amazon RDS for MySQL 데이터베이스 인스턴스에 대한 연결 문제를 해결하고, 서버리스 애플리케이션의 트래픽 변동성을 처리할 수 있는 솔루션을 선택하는 능력을 평가합니다. 특히, 최소한의 운영 오버헤드로 데이터베이스 연결 거부 오류를 해결하는 방법을 찾는 것이 중요합니다.

### 정답
A. Create a proxy in RDS Proxy. Configure the users’ applications to use the DB instance through RDS Proxy.

### 해설
Amazon RDS Proxy는 데이터베이스 연결을 관리하고, 연결 풀링을 통해 데이터베이스에 대한 연결 수를 최적화하여 성능을 향상시킵니다. 이는 특히 서버리스 애플리케이션과 같이 트래픽 변동성이 큰 경우에 유용합니다. RDS Proxy를 사용하면 데이터베이스 연결 거부 오류를 줄이고, 최소한의 운영 오버헤드로 문제를 해결할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Create a proxy in RDS Proxy. Configure the users’ applications to use the DB instance through RDS Proxy.**
  - RDS Proxy는 데이터베이스 연결을 관리하고 최적화하여, 트래픽 변동성이 큰 경우에도 안정적인 연결을 제공합니다. 이는 최소한의 운영 오버헤드로 문제를 해결할 수 있는 솔루션입니다.

- **B. Deploy Amazon ElastiCache for Memcached between the users’ applications and the DB instance.**
  - ElastiCache는 캐싱 솔루션으로, 데이터베이스 부하를 줄이는 데 유용하지만, 연결 거부 오류를 직접적으로 해결하지는 않습니다.

- **C. Migrate the DB instance to a different instance class that has higher I/O capacity. Configure the users’ applications to use the new DB instance.**
  - 인스턴스 클래스를 업그레이드하는 것은 비용이 많이 들며, 트래픽 변동성 문제를 근본적으로 해결하지 못할 수 있습니다.

- **D. Configure Multi-AZ for the DB instance. Configure the users’ applications to switch between the DB instances.**
  - Multi-AZ 설정은 고가용성을 제공하지만, 연결 거부 오류를 해결하는 데 직접적인 도움이 되지 않습니다.

따라서, 회사의 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Amazon RDS Proxy**
   - RDS Proxy는 데이터베이스 연결을 관리하고 최적화하여, 트래픽 변동성이 큰 경우에도 안정적인 연결을 제공합니다.
   - 연결 풀링을 통해 데이터베이스에 대한 연결 수를 최적화하고, 성능을 향상시킵니다.

2. **Amazon ElastiCache**
   - ElastiCache는 인메모리 캐싱 서비스로, 데이터베이스 부하를 줄이는 데 유용합니다.
   - Memcached와 Redis를 지원하며, 데이터베이스 읽기 성능을 향상시킬 수 있습니다.

3. **Amazon RDS 인스턴스 클래스**
   - RDS 인스턴스 클래스는 데이터베이스 인스턴스의 성능을 결정합니다.
   - 더 높은 I/O 용량을 가진 인스턴스 클래스로 업그레이드하면 성능이 향상될 수 있지만, 비용이 증가합니다.

4. **Amazon RDS Multi-AZ**
   - Multi-AZ 배포는 고가용성을 제공하며, 장애 발생 시 자동으로 장애 조치를 수행합니다.
   - 데이터베이스 연결 거부 오류를 해결하는 데는 직접적인 도움이 되지 않습니다.

### 추가 학습 자료
- [Amazon RDS Proxy 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/rds-proxy.html)
- [Amazon ElastiCache 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonElastiCache/latest/mem-ug/WhatIs.html)
- [Amazon RDS 인스턴스 클래스](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html)
- [Amazon RDS Multi-AZ 배포](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html)

이 자료들을 통해 RDS Proxy와 ElastiCache, RDS 인스턴스 클래스, Multi-AZ 배포의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 379번
### 출제 의도
이 문제는 Amazon API Gateway와 AWS Lambda를 사용하여 백엔드와 통합된 프론트엔드 애플리케이션의 응답 지연 시간을 최소화하는 방법을 평가합니다. 특히, 최소한의 운영 변경으로 응답 지연 시간을 줄이는 방법을 찾는 것이 중요합니다.

### 정답
B. Configure provisioned concurrency for the Lambda function that handles the requests.

### 해설
AWS Lambda 함수는 호출될 때마다 콜드 스타트 문제로 인해 지연 시간이 발생할 수 있습니다. 특히, 많은 라이브러리를 로드해야 하는 경우 이 지연 시간이 더 길어질 수 있습니다. 프로비저닝된 동시성(Provisioned Concurrency)을 사용하면 Lambda 함수가 항상 준비 상태를 유지하여 콜드 스타트 지연을 줄일 수 있습니다. 이는 최소한의 운영 변경으로 응답 지연 시간을 줄이는 가장 효과적인 방법입니다.

각 옵션을 자세히 살펴보면:

- **A. Establish a connection between the frontend application and the database to make queries faster by bypassing the API.**
  - 프론트엔드 애플리케이션이 직접 데이터베이스에 연결하는 것은 보안 및 아키텍처 측면에서 바람직하지 않습니다. 또한, API Gateway와 Lambda를 우회하는 것은 많은 변경이 필요합니다.

- **B. Configure provisioned concurrency for the Lambda function that handles the requests.**
  - 프로비저닝된 동시성을 사용하면 Lambda 함수가 항상 준비 상태를 유지하여 콜드 스타트 지연을 줄일 수 있습니다. 이는 최소한의 운영 변경으로 응답 지연 시간을 줄이는 가장 효과적인 방법입니다.

- **C. Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.**
  - 쿼리 결과를 S3에 캐시하는 것은 일부 경우에 유용할 수 있지만, 모든 요청에 대해 항상 최신 데이터를 제공하지 못할 수 있습니다. 또한, 캐시를 관리하는 추가적인 복잡성이 발생합니다.

- **D. Increase the size of the database to increase the number of connections Lambda can establish at one time.**
  - 데이터베이스 크기를 늘리는 것은 비용이 많이 들며, Lambda 함수의 콜드 스타트 문제를 해결하지 못합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **AWS Lambda 프로비저닝된 동시성 (Provisioned Concurrency)**
   - 프로비저닝된 동시성을 사용하면 Lambda 함수가 항상 준비 상태를 유지하여 콜드 스타트 지연을 줄일 수 있습니다.
   - 이는 응답 지연 시간을 최소화하는 데 매우 효과적입니다.

2. **Amazon API Gateway**
   - API Gateway는 RESTful API를 생성, 배포 및 관리할 수 있는 서비스입니다.
   - Lambda 함수와 통합하여 백엔드 로직을 처리할 수 있습니다.

3. **Amazon RDS**
   - RDS는 관리형 관계형 데이터베이스 서비스로, 다양한 데이터베이스 엔진을 지원합니다.
   - Lambda 함수와 통합하여 데이터베이스 작업을 수행할 수 있습니다.

### 추가 학습 자료
- [AWS Lambda 프로비저닝된 동시성 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/configuration-concurrency.html)
- [Amazon API Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/apigateway/latest/developerguide/welcome.html)
- [Amazon RDS 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Welcome.html)

이 자료들을 통해 Lambda 프로비저닝된 동시성, API Gateway, RDS의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 383번
### 정답
A. Dedicated Reserved Hosts

### 해설
Dedicated Hosts는 물리적 서버를 전용으로 사용할 수 있는 옵션으로, 소켓과 코어 기반의 소프트웨어 라이선싱 모델을 지원합니다. 또한, Reserved Hosts는 예약된 용량을 제공하여 비용을 절감할 수 있습니다. 회사가 기존 라이선스를 사용하고, 예측 가능한 용량과 가동 시간 요구 사항이 있는 경우, Dedicated Reserved Hosts가 가장 비용 효율적인 선택입니다.

각 옵션을 자세히 살펴보면:

- **A. Dedicated Reserved Hosts**
  - Dedicated Hosts는 물리적 서버를 전용으로 사용할 수 있는 옵션으로, 소켓과 코어 기반의 소프트웨어 라이선싱 모델을 지원합니다. Reserved Hosts는 예약된 용량을 제공하여 비용을 절감할 수 있습니다. 이는 가장 비용 효율적인 선택입니다.

- **B. Dedicated On-Demand Hosts**
  - Dedicated On-Demand Hosts는 물리적 서버를 전용으로 사용할 수 있지만, 예약되지 않은 용량을 사용하므로 비용이 더 많이 들 수 있습니다.

- **C. Dedicated Reserved Instances**
  - Dedicated Reserved Instances는 물리적 서버를 전용으로 사용하지 않으며, 소켓과 코어 기반의 소프트웨어 라이선싱 모델을 지원하지 않습니다.

- **D. Dedicated On-Demand Instances**
  - Dedicated On-Demand Instances는 물리적 서버를 전용으로 사용하지 않으며, 예약되지 않은 용량을 사용하므로 비용이 더 많이 들 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Dedicated Hosts**
   - Dedicated Hosts는 물리적 서버를 전용으로 사용할 수 있는 옵션입니다.
   - 소켓과 코어 기반의 소프트웨어 라이선싱 모델을 지원합니다.

2. **Reserved Hosts**
   - Reserved Hosts는 예약된 용량을 제공하여 비용을 절감할 수 있습니다.
   - 예측 가능한 용량과 가동 시간 요구 사항이 있는 경우에 적합합니다.

3. **Dedicated Instances**
   - Dedicated Instances는 물리적 서버를 전용으로 사용하지 않으며, 소켓과 코어 기반의 소프트웨어 라이선싱 모델을 지원하지 않습니다.

4. **On-Demand Instances**
   - On-Demand Instances는 예약되지 않은 용량을 사용하므로, 비용이 더 많이 들 수 있습니다.

### 추가 학습 자료
- [Amazon EC2 Dedicated Hosts 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html)
- [Amazon EC2 Reserved Instances 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ec2-reserved-instances.html)
- [Amazon EC2 On-Demand Instances 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html)

이 자료들을 통해 Dedicated Hosts, Reserved Hosts, Dedicated Instances, On-Demand Instances의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 390번
### 출제 의도
이 문제는 세션 데이터를 내구성 있게 저장하고, 고객 세션 관리를 최적화하는 방법을 평가합니다. 특히, 세션 데이터를 안전하게 저장하고, Auto Scaling 환경에서 세션 관리를 효율적으로 처리할 수 있는 솔루션을 선택하는 것이 중요합니다.

### 정답
B. Use an Amazon DynamoDB table to store customer session information.
D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.

### 해설
세션 데이터를 내구성 있게 저장하고, Auto Scaling 환경에서 세션 관리를 최적화하려면, 세션 데이터를 중앙 집중식으로 저장할 수 있는 솔루션이 필요합니다. DynamoDB와 ElastiCache for Redis는 모두 세션 데이터를 내구성 있게 저장하고, 빠르게 접근할 수 있는 솔루션을 제공합니다.

각 옵션을 자세히 살펴보면:

- **A. Turn on the sticky sessions feature (session affinity) on the ALB.**
  - ALB의 스티키 세션 기능은 세션 데이터를 내구성 있게 저장하지 않습니다. 이는 세션 데이터를 특정 인스턴스에 고정시키는 방법으로, 인스턴스가 종료되면 세션 데이터가 손실될 수 있습니다.

- **B. Use an Amazon DynamoDB table to store customer session information.**
  - DynamoDB는 완전 관리형 NoSQL 데이터베이스로, 세션 데이터를 내구성 있게 저장할 수 있습니다. 또한, 높은 가용성과 성능을 제공합니다.

- **C. Deploy an Amazon Cognito user pool to manage user session information.**
  - Amazon Cognito는 사용자 인증 및 관리에 적합하지만, 세션 데이터를 내구성 있게 저장하는 데는 적합하지 않습니다.

- **D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.**
  - ElastiCache for Redis는 인메모리 데이터 저장소로, 세션 데이터를 빠르게 저장하고 접근할 수 있습니다. 또한, Redis의 내구성 옵션을 사용하여 데이터를 안전하게 저장할 수 있습니다.

- **E. Use AWS Systems Manager Application Manager in the application to manage user session information.**
  - AWS Systems Manager Application Manager는 애플리케이션 관리를 위한 도구로, 세션 데이터를 내구성 있게 저장하는 데는 적합하지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B와 D입니다.

### 필요한 학습 내용

1. **Amazon DynamoDB**
   - 완전 관리형 NoSQL 데이터베이스로, 세션 데이터를 내구성 있게 저장할 수 있습니다.
   - 높은 가용성과 성능을 제공합니다.

2. **Amazon ElastiCache for Redis**
   - 인메모리 데이터 저장소로, 세션 데이터를 빠르게 저장하고 접근할 수 있습니다.
   - Redis의 내구성 옵션을 사용하여 데이터를 안전하게 저장할 수 있습니다.

3. **Application Load Balancer (ALB)**
   - ALB는 HTTP 및 HTTPS 트래픽을 로드 밸런싱할 수 있는 서비스입니다.
   - 스티키 세션 기능은 세션 데이터를 특정 인스턴스에 고정시키는 방법으로, 내구성 있는 세션 데이터 저장에는 적합하지 않습니다.

4. **Amazon Cognito**
   - 사용자 인증 및 관리를 위한 서비스로, 세션 데이터를 내구성 있게 저장하는 데는 적합하지 않습니다.

5. **AWS Systems Manager Application Manager**
   - 애플리케이션 관리를 위한 도구로, 세션 데이터를 내구성 있게 저장하는 데는 적합하지 않습니다.

### 추가 학습 자료
- [Amazon DynamoDB 공식 문서](https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/Introduction.html)
- [Amazon ElastiCache for Redis 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonElastiCache/latest/red-ug/WhatIs.html)
- [Application Load Balancer 공식 문서](https://docs.aws.amazon.com/ko_kr/elasticloadbalancing/latest/application/introduction.html)
- [Amazon Cognito 공식 문서](https://docs.aws.amazon.com/ko_kr/cognito/latest/developerguide/what-is-amazon-cognito.html)
- [AWS Systems Manager Application Manager 공식 문서](https://docs.aws.amazon.com/ko_kr/systems-manager/latest/userguide/app-manager.html)

이 자료들을 통해 DynamoDB와 ElastiCache for Redis의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 391번
### 출제 의도
이 문제는 세 가지 계층으로 구성된 무상태 웹 애플리케이션의 백업 전략을 설계할 때, 확장성을 극대화하고 리소스 활용을 최적화하는 방법을 평가합니다. 특히, 2시간의 복구 시점 목표(RPO)를 충족하는 백업 전략을 선택하는 것이 중요합니다.

### 정답
C. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

### 해설
세 가지 계층으로 구성된 무상태 웹 애플리케이션의 경우, EC2 인스턴스는 상태를 유지하지 않으므로, 인스턴스 자체를 백업할 필요가 없습니다. 대신, 최신 Amazon Machine Images (AMIs)를 유지하여 필요할 때 인스턴스를 신속하게 재생성할 수 있습니다. 데이터베이스 계층의 경우, Amazon RDS의 자동 백업 기능을 사용하고, 포인트 인 타임 복구(Point-in-Time Recovery)를 통해 2시간의 RPO를 충족할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.**
  - EC2 인스턴스가 무상태이므로, EBS 볼륨 스냅샷을 2시간마다 찍는 것은 불필요한 리소스 낭비입니다. 데이터베이스의 경우, RDS의 자동 백업 기능을 사용하는 것이 더 효율적입니다.

- **B. Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.**
  - EBS 스냅샷 라이프사이클 정책을 설정하는 것은 EC2 인스턴스가 무상태인 경우 불필요합니다. RDS의 자동 백업 기능을 사용하는 것은 적절하지만, EC2 인스턴스에 대한 백업 전략이 최적화되지 않았습니다.

- **C. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.**
  - 최신 AMIs를 유지하면 EC2 인스턴스를 신속하게 재생성할 수 있으며, RDS의 자동 백업과 포인트 인 타임 복구를 통해 데이터베이스의 RPO를 충족할 수 있습니다. 이는 확장성을 극대화하고 리소스 활용을 최적화하는 가장 효율적인 방법입니다.

- **D. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.**
  - EC2 인스턴스가 무상태이므로, EBS 볼륨 스냅샷을 2시간마다 찍는 것은 불필요한 리소스 낭비입니다. 데이터베이스의 경우, RDS의 자동 백업 기능을 사용하는 것이 더 효율적입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Amazon Machine Images (AMIs)**
   - AMI는 EC2 인스턴스를 생성하는 데 필요한 정보를 포함하는 템플릿입니다.
   - 최신 AMIs를 유지하면 EC2 인스턴스를 신속하게 재생성할 수 있습니다.

2. **Amazon RDS 자동 백업**
   - RDS는 자동으로 데이터베이스 백업을 생성하고, 포인트 인 타임 복구를 지원합니다.
   - 자동 백업을 사용하면 데이터베이스의 RPO를 충족할 수 있습니다.

3. **포인트 인 타임 복구 (Point-in-Time Recovery)**
   - RDS는 특정 시점으로 데이터베이스를 복구할 수 있는 기능을 제공합니다.
   - 이를 통해 데이터 손실을 최소화하고, RPO를 충족할 수 있습니다.

### 추가 학습 자료
- [Amazon Machine Images (AMIs) 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/AMIs.html)
- [Amazon RDS 자동 백업 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html)
- [포인트 인 타임 복구 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_PIT.html)

이 자료들을 통해 AMIs와 RDS 자동 백업, 포인트 인 타임 복구의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 393번
### 출제 의도
이 문제는 음성 파일에서 텍스트를 추출하고, 고객의 개인 식별 정보를 제거하는 방법을 평가합니다. 특히, AWS 서비스를 사용하여 이 작업을 자동화하고 효율적으로 수행하는 방법을 찾는 것이 중요합니다.

### 정답
C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.

### 해설
Amazon Transcribe는 음성 파일을 텍스트로 변환하는 서비스로, PII(개인 식별 정보) 제거 기능을 제공합니다. 이 기능을 사용하면 음성 파일에서 텍스트를 추출하면서 자동으로 PII를 제거할 수 있습니다. S3 버킷에 파일이 업로드될 때 Lambda 함수를 호출하여 Transcribe 작업을 시작하고, 결과를 별도의 S3 버킷에 저장하면 요구 사항을 충족할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known PII patterns.**
  - Kinesis Video Streams는 비디오 스트리밍 데이터를 처리하는 데 사용되며, 음성 파일을 처리하는 데 적합하지 않습니다. 또한, Lambda 함수를 사용하여 PII 패턴을 스캔하는 것은 복잡하고 비효율적일 수 있습니다.

- **B. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to analyze the call recordings.**
  - Amazon Textract는 문서에서 텍스트와 데이터를 추출하는 서비스로, 음성 파일을 처리하는 데 적합하지 않습니다.

- **C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.**
  - Amazon Transcribe는 음성 파일을 텍스트로 변환하고, PII 제거 기능을 제공합니다. Lambda 함수를 사용하여 Transcribe 작업을 시작하고, 결과를 별도의 S3 버킷에 저장하면 요구 사항을 충족할 수 있습니다.

- **D. Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed an AWS Lambda function to scan for known PII patterns. Use Amazon EventBridge to start the contact flow when an audio file is uploaded to the S3 bucket.**
  - Amazon Connect는 콜 센터 솔루션으로, 음성 파일을 처리하는 데 적합하지 않습니다. 또한, Lambda 함수를 사용하여 PII 패턴을 스캔하는 것은 복잡하고 비효율적일 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Amazon Transcribe**
   - 음성 파일을 텍스트로 변환하는 서비스입니다.
   - PII 제거 기능을 제공하여, 개인 식별 정보를 자동으로 제거할 수 있습니다.

2. **AWS Lambda**
   - 서버리스 컴퓨팅 서비스로, 이벤트에 응답하여 코드를 실행할 수 있습니다.
   - S3 버킷에 파일이 업로드될 때 Lambda 함수를 호출하여 Transcribe 작업을 시작할 수 있습니다.

3. **Amazon S3**
   - 객체 스토리지 서비스로, 음성 파일과 변환된 텍스트를 저장할 수 있습니다.
   - S3 이벤트를 사용하여 파일 업로드 시 Lambda 함수를 호출할 수 있습니다.

### 추가 학습 자료
- [Amazon Transcribe 공식 문서](https://docs.aws.amazon.com/ko_kr/transcribe/latest/dg/what-is-transcribe.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/Welcome.html)

이 자료들을 통해 Amazon Transcribe, AWS Lambda, Amazon S3의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 394번
### 출제 의도
이 문제는 Amazon RDS 데이터베이스의 성능을 개선하기 위해 적절한 스토리지 옵션을 선택하는 능력을 평가합니다. 특히, 높은 IOPS 요구 사항을 충족할 수 있는 스토리지 솔루션을 선택하는 것이 중요합니다.

### 정답
B. Increase the number of IOPS on the gp3 volume.

### 해설
Amazon EBS gp3 볼륨은 IOPS와 스토리지 용량을 독립적으로 조정할 수 있는 기능을 제공합니다. 현재 gp3 볼륨의 IOPS가 20,000을 초과할 때 성능 저하가 발생하므로, gp3 볼륨의 IOPS를 증가시키는 것이 가장 적절한 해결책입니다. 이는 최소한의 변경으로 성능 문제를 해결할 수 있는 방법입니다.

각 옵션을 자세히 살펴보면:

- **A. Replace the volume with a magnetic volume.**
  - 마그네틱 볼륨은 매우 낮은 IOPS를 제공하므로, 성능을 크게 저하시킬 수 있습니다. 이 옵션은 적절하지 않습니다.

- **B. Increase the number of IOPS on the gp3 volume.**
  - gp3 볼륨은 IOPS를 독립적으로 조정할 수 있으므로, IOPS를 증가시키면 성능 문제를 해결할 수 있습니다. 이는 최소한의 변경으로 성능을 개선할 수 있는 방법입니다.

- **C. Replace the volume with a Provisioned IOPS SSD (io2) volume.**
  - io2 볼륨은 높은 IOPS를 제공하지만, 비용이 더 많이 들 수 있습니다. gp3 볼륨의 IOPS를 증가시키는 것이 더 비용 효율적일 수 있습니다.

- **D. Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.**
  - 두 개의 1,000 GB gp3 볼륨으로 교체하는 것은 복잡성을 증가시키며, 성능 문제를 해결하는 데 직접적인 도움이 되지 않을 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **Amazon EBS gp3 볼륨**
   - gp3 볼륨은 IOPS와 스토리지 용량을 독립적으로 조정할 수 있습니다.
   - 높은 IOPS를 제공하며, 비용 효율적입니다.

2. **Amazon EBS io2 볼륨**
   - io2 볼륨은 매우 높은 IOPS를 제공하지만, 비용이 더 많이 들 수 있습니다.
   - 높은 성능이 필요한 워크로드에 적합합니다.

3. **Amazon RDS**
   - RDS는 관리형 관계형 데이터베이스 서비스로, 다양한 스토리지 옵션을 제공합니다.
   - Multi-AZ 배포를 통해 고가용성과 내구성을 제공합니다.

### 추가 학습 자료
- [Amazon EBS gp3 볼륨 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp3)
- [Amazon EBS io2 볼륨 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_io2)
- [Amazon RDS 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Welcome.html)

이 자료들을 통해 gp3와 io2 볼륨의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 395번
### 정답
C. AWS CloudTrail

### 해설
AWS CloudTrail은 AWS 계정에서 이루어진 모든 API 호출 및 활동을 기록하는 서비스입니다. 이를 통해 누가 어떤 작업을 수행했는지 추적할 수 있습니다. 보안 그룹 규칙 변경과 같은 구성 변경 사항을 추적하고, 어떤 IAM 사용자가 이러한 변경을 수행했는지 확인할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Amazon GuardDuty**
  - GuardDuty는 AWS 계정과 워크로드를 모니터링하여 악의적인 활동과 비정상적인 동작을 탐지하는 서비스입니다. 그러나, 특정 구성 변경을 수행한 사용자를 추적하는 데는 적합하지 않습니다.

- **B. Amazon Inspector**
  - Inspector는 AWS 인프라의 보안 취약성을 평가하는 서비스입니다. 특정 구성 변경을 수행한 사용자를 추적하는 데는 적합하지 않습니다.

- **C. AWS CloudTrail**
  - CloudTrail은 AWS 계정에서 이루어진 모든 API 호출 및 활동을 기록합니다. 이를 통해 누가 어떤 작업을 수행했는지 추적할 수 있습니다. 보안 그룹 규칙 변경과 같은 구성 변경 사항을 추적하고, 어떤 IAM 사용자가 이러한 변경을 수행했는지 확인할 수 있습니다.

- **D. AWS Config**
  - Config는 AWS 리소스의 구성 상태를 평가, 감사 및 평가하는 서비스입니다. 리소스의 현재 및 과거 구성 상태를 추적할 수 있지만, 특정 구성 변경을 수행한 사용자를 추적하는 데는 CloudTrail이 더 적합합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **AWS CloudTrail**
   - CloudTrail은 AWS 계정에서 이루어진 모든 API 호출 및 활동을 기록하는 서비스입니다.
   - 이를 통해 누가 어떤 작업을 수행했는지 추적할 수 있습니다.

2. **Amazon GuardDuty**
   - GuardDuty는 AWS 계정과 워크로드를 모니터링하여 악의적인 활동과 비정상적인 동작을 탐지하는 서비스입니다.

3. **Amazon Inspector**
   - Inspector는 AWS 인프라의 보안 취약성을 평가하는 서비스입니다.

4. **AWS Config**
   - Config는 AWS 리소스의 구성 상태를 평가, 감사 및 평가하는 서비스입니다.
   - 리소스의 현재 및 과거 구성 상태를 추적할 수 있습니다.

### 추가 학습 자료
- [AWS CloudTrail 공식 문서](https://docs.aws.amazon.com/ko_kr/awscloudtrail/latest/userguide/cloudtrail-user-guide.html)
- [Amazon GuardDuty 공식 문서](https://docs.aws.amazon.com/ko_kr/guardduty/latest/ug/what-is-guardduty.html)
- [Amazon Inspector 공식 문서](https://docs.aws.amazon.com/ko_kr/inspector/latest/userguide/inspector_introduction.html)
- [AWS Config 공식 문서](https://docs.aws.amazon.com/ko_kr/config/latest/developerguide/WhatIsConfig.html)

이 자료들을 통해 CloudTrail, GuardDuty, Inspector, Config의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 

# 397번
### 출제 의도
이 문제는 최소한의 운영 노력으로 매일 예약된 작업을 실행하는 솔루션을 설계하는 능력을 평가합니다. 특히, 작업의 CPU 및 메모리 사용량이 일정하고, 작업이 최대 1시간 동안 실행될 수 있는 경우에 적합한 솔루션을 선택하는 것이 중요합니다.

### 정답
C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

### 해설
Amazon ECS와 AWS Fargate를 사용하면 서버를 관리할 필요 없이 컨테이너를 실행할 수 있습니다. Fargate는 작업의 CPU 및 메모리 요구 사항을 기반으로 자동으로 리소스를 할당하므로, 운영 오버헤드를 최소화할 수 있습니다. 또한, Amazon EventBridge를 사용하여 작업을 예약할 수 있습니다. 이는 최소한의 운영 노력으로 작업을 실행할 수 있는 가장 적절한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the EventBridge event to run once a day.**
  - Lambda 함수는 최대 15분 동안 실행될 수 있으므로, 최대 1시간이 걸리는 작업에는 적합하지 않습니다.

- **B. Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.**
  - Lambda 함수는 최대 15분 동안 실행될 수 있으므로, 최대 1시간이 걸리는 작업에는 적합하지 않습니다.

- **C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.**
  - Fargate는 서버를 관리할 필요 없이 컨테이너를 실행할 수 있으며, 작업의 CPU 및 메모리 요구 사항을 기반으로 자동으로 리소스를 할당합니다. 이는 최소한의 운영 노력으로 작업을 실행할 수 있는 가장 적절한 솔루션입니다.

- **D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.**
  - EC2 인스턴스를 관리해야 하므로, 운영 오버헤드가 증가합니다. Fargate를 사용하는 것이 더 효율적입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Amazon ECS (Elastic Container Service)**
   - ECS는 컨테이너화된 애플리케이션을 실행하고 관리할 수 있는 서비스입니다.
   - Fargate와 EC2 런치 타입을 지원합니다.

2. **AWS Fargate**
   - Fargate는 서버를 관리할 필요 없이 컨테이너를 실행할 수 있는 서버리스 컴퓨팅 엔진입니다.
   - 작업의 CPU 및 메모리 요구 사항을 기반으로 자동으로 리소스를 할당합니다.

3. **Amazon EventBridge**
   - EventBridge는 이벤트 기반 아키텍처를 구축할 수 있는 서비스입니다.
   - 예약된 이벤트를 생성하여 작업을 자동으로 실행할 수 있습니다.

4. **AWS Lambda**
   - Lambda는 서버리스 컴퓨팅 서비스로, 이벤트에 응답하여 코드를 실행할 수 있습니다.
   - 최대 실행 시간이 15분으로 제한되어 있습니다.

### 추가 학습 자료
- [Amazon ECS 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/developerguide/Welcome.html)
- [AWS Fargate 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/developerguide/AWS_Fargate.html)
- [Amazon EventBridge 공식 문서](https://docs.aws.amazon.com/ko_kr/eventbridge/latest/userguide/what-is-amazon-eventbridge.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)

이 자료들을 통해 ECS, Fargate, EventBridge, Lambda의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.
# 399번
### 출제 의도
이 문제는 HTTP 플러드 공격으로부터 웹 애플리케이션을 보호하기 위해 최소한의 운영 오버헤드로 효과적인 솔루션을 설계하는 능력을 평가합니다.

### 정답
B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.

### 해설
AWS WAF (Web Application Firewall)은 웹 애플리케이션을 다양한 공격으로부터 보호할 수 있는 서비스입니다. WAF의 레이트 기반 규칙(rate-based rule)을 사용하면 특정 시간 동안의 요청 수를 기준으로 트래픽을 제어할 수 있습니다. 이를 통해 HTTP 플러드 공격을 효과적으로 방어할 수 있습니다. 또한, WAF는 API Gateway와 쉽게 통합할 수 있으며, 최소한의 운영 오버헤드로 설정할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.**
  - CloudFront는 콘텐츠 배포 네트워크(CDN)로, 캐싱을 통해 성능을 향상시킬 수 있지만, HTTP 플러드 공격을 직접적으로 방어하지는 않습니다.

- **B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.**
  - AWS WAF의 레이트 기반 규칙을 사용하면 HTTP 플러드 공격을 효과적으로 방어할 수 있습니다. 이는 최소한의 운영 오버헤드로 설정할 수 있는 솔루션입니다.

- **C. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predefined rate is reached.**
  - CloudWatch를 사용하여 모니터링하고 경고를 설정하는 것은 유용하지만, 공격을 직접적으로 방어하지는 않습니다.

- **D. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predefined rate.**
  - Lambda@Edge와 CloudFront를 사용하여 요청을 차단하는 것은 가능하지만, 설정과 관리가 복잡하며 운영 오버헤드가 증가할 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **AWS WAF (Web Application Firewall)**
   - WAF는 웹 애플리케이션을 다양한 공격으로부터 보호할 수 있는 서비스입니다.
   - 레이트 기반 규칙을 사용하여 특정 시간 동안의 요청 수를 기준으로 트래픽을 제어할 수 있습니다.

2. **Amazon API Gateway**
   - API Gateway는 RESTful API를 생성, 배포 및 관리할 수 있는 서비스입니다.
   - WAF와 통합하여 API를 보호할 수 있습니다.

3. **Amazon CloudFront**
   - CloudFront는 콘텐츠 배포 네트워크(CDN)로, 캐싱을 통해 성능을 향상시킬 수 있습니다.
   - HTTP 플러드 공격을 직접적으로 방어하지는 않습니다.

4. **Amazon CloudWatch**
   - CloudWatch는 AWS 리소스와 애플리케이션을 모니터링할 수 있는 서비스입니다.
   - 모니터링과 경고 설정은 가능하지만, 공격을 직접적으로 방어하지는 않습니다.

### 추가 학습 자료
- [AWS WAF 공식 문서](https://docs.aws.amazon.com/ko_kr/waf/latest/developerguide/what-is-aws-waf.html)
- [Amazon API Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/apigateway/latest/developerguide/welcome.html)
- [Amazon CloudFront 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonCloudFront/latest/DeveloperGuide/Introduction.html)
- [Amazon CloudWatch 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html)

이 자료들을 통해 AWS WAF, API Gateway, CloudFront, CloudWatch의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 402번
### 정답
C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.

### 해설
Amazon Kinesis Data Streams는 스트리밍 데이터를 실시간으로 처리할 수 있는 서비스입니다. 데이터가 S3 버킷에 모두 도달하지 않는 문제는 Kinesis Data Streams의 처리 용량이 부족하여 발생할 수 있습니다. Kinesis Data Streams는 샤드(shard) 단위로 데이터를 처리하며, 각 샤드는 초당 1MB의 쓰기 및 2MB의 읽기 용량을 제공합니다. 데이터의 양이 증가하면 샤드의 수를 늘려야 합니다. 따라서, Kinesis 샤드의 수를 늘려 데이터 처리 용량을 증가시키는 것이 적절한 해결책입니다.

각 옵션을 자세히 살펴보면:

- **A. Update the Kinesis Data Streams default settings by modifying the data retention period.**
  - 데이터 보존 기간을 늘리는 것은 데이터 손실을 방지하는 데 도움이 되지만, 처리 용량 문제를 해결하지는 않습니다.

- **B. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.**
  - KPL을 사용하면 데이터 전송을 최적화할 수 있지만, 처리 용량 문제를 직접적으로 해결하지는 않습니다.

- **C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.**
  - Kinesis 샤드의 수를 늘리면 데이터 처리 용량이 증가하여 데이터 손실 문제를 해결할 수 있습니다.

- **D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.**
  - S3 버킷 버전 관리는 데이터 손실 문제를 해결하지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Amazon Kinesis Data Streams**
   - Kinesis Data Streams는 스트리밍 데이터를 실시간으로 처리할 수 있는 서비스입니다.
   - 샤드 단위로 데이터를 처리하며, 각 샤드는 초당 1MB의 쓰기 및 2MB의 읽기 용량을 제공합니다.

2. **Kinesis 샤드**
   - 샤드는 Kinesis Data Streams의 기본 단위로, 데이터 처리 용량을 결정합니다.
   - 데이터의 양이 증가하면 샤드의 수를 늘려야 합니다.

3. **Kinesis Producer Library (KPL)**
   - KPL은 Kinesis Data Streams에 데이터를 전송하는 애플리케이션을 최적화할 수 있는 라이브러리입니다.
   - 데이터 전송을 최적화할 수 있지만, 처리 용량 문제를 직접적으로 해결하지는 않습니다.

4. **Amazon S3 버전 관리**
   - S3 버전 관리는 객체의 모든 버전을 보존하여 데이터 손실을 방지할 수 있습니다.
   - 데이터 처리 용량 문제를 해결하지는 않습니다.

### 추가 학습 자료
- [Amazon Kinesis Data Streams 공식 문서](https://docs.aws.amazon.com/ko_kr/streams/latest/dev/introduction.html)
- [Kinesis 샤드 공식 문서](https://docs.aws.amazon.com/ko_kr/streams/latest/dev/key-concepts.html#shard)
- [Kinesis Producer Library (KPL) 공식 문서](https://docs.aws.amazon.com/ko_kr/streams/latest/dev/developing-producers-with-kpl.html)
- [Amazon S3 버전 관리 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/ObjectVersioning.html)

이 자료들을 통해 Kinesis Data Streams, 샤드, KPL, S3 버전 관리의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 404번
### 정답
D. Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda.

### 해설
마케팅 캠페인 후에 많은 문서가 처리되지 않은 문제는 Lambda 함수의 동시 실행 제한 또는 S3 이벤트 트리거의 제한으로 인해 발생할 수 있습니다. Amazon SQS를 사용하여 요청을 큐에 보내고, 큐를 Lambda 함수의 이벤트 소스로 구성하면, Lambda 함수가 큐에서 메시지를 처리할 수 있습니다. 이는 Lambda 함수의 동시 실행 제한을 우회하고, 문서가 손실되지 않도록 보장할 수 있습니다.


* **이벤트 소스(Event Source)**
   * Lambda 함수를 트리거하여 이벤트를 발생시키는 서비스나 리소스
   * Push-Based: API Gateway, S3, SNS, EventBridge
   * Poll-Based: SQS, Kinesis/DynamoDB Streams 
   * Event Source Mapping
      - SQS 큐와 Lambda 함수를 연결하여, Lambda 서비스가 자동으로 SQS 큐를 폴링하고 메시지를 Lambda 함수로 전달
      - create-event-source-mapping API를 사용하여 설정할 수 있음
* **Lambda 함수 동시 실행 제한**
   * 기본적으로 AWS 계정당 리전별로 1,000개의 제한이 설정


각 옵션을 자세히 살펴보면:

- **A. Set the Lambda function's runtime timeout value to 15 minutes.**
  - Lambda 함수의 실행 시간을 늘리는 것은 긴 실행 시간의 작업을 처리하는 데 도움이 될 수 있지만, 많은 문서가 처리되지 않는 문제를 해결하지는 않습니다.

- **B. Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.**
  - S3 버킷 복제 정책을 구성하는 것은 데이터 복제에 도움이 될 수 있지만, 문서 처리 문제를 직접적으로 해결하지는 않습니다.

- **C. Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.**
  - 추가 Lambda 함수를 배포하고 로드 밸런싱을 설정하는 것은 도움이 될 수 있지만, S3 이벤트 트리거의 제한을 해결하지는 않습니다.

- **D. Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda.**
  - SQS 큐를 사용하여 요청을 큐에 보내고, 큐를 Lambda 함수의 이벤트 소스로 구성하면, Lambda 함수가 큐에서 메시지를 처리할 수 있습니다. 이는 Lambda 함수의 동시 실행 제한을 우회하고, 문서가 손실되지 않도록 보장할 수 있습니다. 이는 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **Amazon SQS (Simple Queue Service)**
   - SQS는 메시지 큐 서비스로, 메시지를 안전하게 전송하고 저장할 수 있습니다.
   - Lambda 함수의 이벤트 소스로 구성하여 메시지를 처리할 수 있습니다.

2. **AWS Lambda**
   - Lambda는 서버리스 컴퓨팅 서비스로, 이벤트에 응답하여 코드를 실행할 수 있습니다.
   - SQS 큐를 이벤트 소스로 구성하여 메시지를 처리할 수 있습니다.

3. **Amazon S3**
   - S3는 객체 스토리지 서비스로, 데이터를 안전하게 저장하고 관리할 수 있습니다.
   - S3 이벤트 트리거를 사용하여 Lambda 함수를 호출할 수 있습니다.

### 추가 학습 자료
- [Amazon SQS 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/Welcome.html)



# 405번
### 정답
D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.
E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.

### 해설
솔루션 아키텍트는 소프트웨어 데모 환경을 설계하고 있으며, 이 환경은 작업 시간 동안 트래픽이 크게 증가하지만 주말에는 운영할 필요가 없습니다. 이를 위해 Auto Scaling 그룹을 사용하여 EC2 인스턴스를 자동으로 확장하고 축소할 수 있도록 설정해야 합니다.

각 옵션을 자세히 살펴보면:

- **A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate.**
  - ALB는 자체적으로 트래픽을 처리할 수 있으며, Auto Scaling 그룹과 함께 사용하여 EC2 인스턴스를 확장 및 축소하는 것이 더 적절합니다. ALB의 용량을 조정하는 것은 필요하지 않습니다.

- **B. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.**
  - VPC 인터넷 게이트웨이는 확장 가능한 서비스로, Auto Scaling을 통해 용량을 조정할 필요가 없습니다.

- **C. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.**
  - 여러 리전에서 EC2 인스턴스를 실행하는 것은 복잡성을 증가시키며, 단일 리전 내에서 Auto Scaling을 사용하는 것이 더 적절합니다.

- **D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.**
  - 타겟 추적 스케일링 정책을 사용하여 인스턴스 CPU 사용률을 기준으로 Auto Scaling 그룹을 확장 및 축소할 수 있습니다. 이는 트래픽 증가에 따라 자동으로 인스턴스를 추가하거나 제거하는 데 유용합니다.

- **E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.**
  - 예약된 스케일링을 사용하여 주말 동안 Auto Scaling 그룹의 최소, 최대 및 원하는 용량을 0으로 설정하고, 주 초에 기본 값으로 되돌릴 수 있습니다. 이는 주말 동안 리소스를 절약하는 데 유용합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D와 E입니다.

### 필요한 학습 내용

1. **AWS Auto Scaling**
   - Auto Scaling은 EC2 인스턴스를 자동으로 확장 및 축소할 수 있는 기능을 제공합니다.
   - 타겟 추적 스케일링 정책을 사용하여 인스턴스 CPU 사용률을 기준으로 확장 및 축소할 수 있습니다.
   - 예약된 스케일링을 사용하여 특정 시간에 Auto Scaling 그룹의 용량을 조정할 수 있습니다.

2. **Application Load Balancer (ALB)**
   - ALB는 트래픽을 여러 EC2 인스턴스에 분산시켜 고가용성과 확장성을 제공합니다.
   - Auto Scaling 그룹과 함께 사용하여 트래픽 증가에 따라 인스턴스를 자동으로 추가하거나 제거할 수 있습니다.

### 추가 학습 자료
- [AWS Auto Scaling 공식 문서](https://docs.aws.amazon.com/ko_kr/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html)
- [Application Load Balancer 공식 문서](https://docs.aws.amazon.com/ko_kr/elasticloadbalancing/latest/application/introduction.html)

이 자료들을 통해 Auto Scaling과 ALB의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 408번
### 정답
B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLB. Process the data in Amazon ECS.

### 해설
회사는 지리적으로 분산된 원격 장치에서 UDP 데이터를 수신하고, 데이터를 즉시 처리하며, 필요시 장치에 메시지를 다시 보내는 애플리케이션을 운영하고 있습니다. 이 솔루션은 데이터 전송의 지연 시간을 최소화하고, 다른 AWS 리전으로의 빠른 장애 조치를 제공해야 합니다.

각 옵션을 자세히 살펴보면:

- **A. Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.**
  - NLB는 UDP 트래픽을 처리할 수 있지만, Lambda 함수는 짧은 실행 시간에 적합하며, 지속적인 데이터 처리를 위해서는 적합하지 않을 수 있습니다.

- **B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLB. Process the data in Amazon ECS.**
  - AWS Global Accelerator는 전 세계적으로 최적의 경로를 통해 트래픽을 전달하여 지연 시간을 최소화합니다. NLB는 UDP 트래픽을 처리할 수 있으며, ECS와 Fargate를 사용하여 데이터를 처리할 수 있습니다. 이는 지연 시간을 최소화하고, 빠른 장애 조치를 제공하는 데 적합한 솔루션입니다.

- **C. Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.**
  - ALB는 UDP 트래픽을 처리할 수 없습니다. 따라서 이 옵션은 적절하지 않습니다.

- **D. Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of the two Regions. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.**
  - ALB는 UDP 트래픽을 처리할 수 없습니다. 따라서 이 옵션은 적절하지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **AWS Global Accelerator**
   - Global Accelerator는 전 세계적으로 최적의 경로를 통해 트래픽을 전달하여 지연 시간을 최소화합니다.
   - 여러 리전 간의 빠른 장애 조치를 제공합니다.

2. **Network Load Balancer (NLB)**
   - NLB는 TCP 및 UDP 트래픽을 처리할 수 있는 고성능 로드 밸런서입니다.
   - ECS 서비스와 통합하여 데이터를 처리할 수 있습니다.

3. **Amazon Elastic Container Service (ECS)**
   - ECS는 컨테이너화된 애플리케이션을 관리할 수 있는 서비스입니다.
   - Fargate를 사용하여 서버를 관리하지 않고 컨테이너를 실행할 수 있습니다.

### 추가 학습 자료
- [AWS Global Accelerator 공식 문서](https://docs.aws.amazon.com/ko_kr/global-accelerator/latest/dg/what-is-global-accelerator.html)
- [Network Load Balancer 공식 문서](https://docs.aws.amazon.com/ko_kr/elasticloadbalancing/latest/network/introduction.html)
- [Amazon Elastic Container Service (ECS) 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/developerguide/Welcome.html)

이 자료들을 통해 Global Accelerator, NLB, ECS의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 410번
### 정답
B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.

### 해설
Amazon EBS 볼륨을 암호화된 볼륨으로 생성하면, 해당 볼륨에 기록되는 모든 데이터가 자동으로 암호화됩니다. 이는 EBS 볼륨의 데이터가 저장 중에 암호화되도록 보장하는 가장 직접적이고 확실한 방법입니다.

각 옵션을 자세히 살펴보면:

- **A. Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.**
  - IAM 역할은 EC2 인스턴스에 권한을 부여하는 데 사용되지만, EBS 볼륨의 암호화를 직접적으로 설정하지는 않습니다.

- **B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.**
  - EBS 볼륨을 암호화된 볼륨으로 생성하면, 해당 볼륨에 기록되는 모든 데이터가 자동으로 암호화됩니다. 이는 데이터가 저장 중에 암호화되도록 보장하는 가장 직접적이고 확실한 방법입니다.

- **C. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.**
  - 태그는 리소스를 식별하거나 관리하는 데 사용되지만, EBS 볼륨의 암호화를 직접적으로 설정하지는 않습니다.

- **D. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.**
  - KMS 키 정책은 키 사용을 제어하는 데 사용되지만, EBS 볼륨의 암호화를 직접적으로 설정하지는 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **Amazon EBS (Elastic Block Store)**
   - EBS는 EC2 인스턴스에 연결할 수 있는 블록 스토리지 서비스입니다.
   - EBS 볼륨을 암호화된 볼륨으로 생성하면, 해당 볼륨에 기록되는 모든 데이터가 자동으로 암호화됩니다.

2. **AWS Key Management Service (KMS)**
   - KMS는 암호화 키를 생성하고 관리할 수 있는 서비스입니다.
   - KMS 키를 사용하여 EBS 볼륨을 암호화할 수 있습니다.

3. **IAM 역할 (IAM Roles)**
   - IAM 역할은 AWS 리소스에 권한을 부여하는 데 사용됩니다.
   - EBS 볼륨의 암호화를 직접적으로 설정하지는 않습니다.

4. **EC2 인스턴스 태그**
   - 태그는 리소스를 식별하거나 관리하는 데 사용됩니다.
   - EBS 볼륨의 암호화를 직접적으로 설정하지는 않습니다.

### 추가 학습 자료
- [Amazon EBS 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/AmazonEBS.html)
- [AWS KMS 공식 문서](https://docs.aws.amazon.com/ko_kr/kms/latest/developerguide/overview.html)
- [IAM 역할 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles.html)
- [EC2 인스턴스 태그 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/Using_Tags.html)

이 자료들을 통해 EBS, KMS, IAM 역할, EC2 인스턴스 태그의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.



# 412번
### 정답
D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.

### 해설
Amazon S3의 Block Public Access 기능을 사용하면 S3 버킷과 객체에 대한 공용 액세스를 차단할 수 있습니다. 이 설정을 계정 수준에서 적용하면, 계정 내 모든 S3 객체가 공용으로 노출되는 것을 방지할 수 있습니다. 또한, AWS Organizations의 서비스 제어 정책(SCP)을 사용하여 IAM 사용자가 이 설정을 변경하지 못하도록 할 수 있습니다. 이는 가장 효과적이고 관리 오버헤드가 적은 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.**
  - GuardDuty는 보안 위협을 모니터링하는 데 사용되며, S3 버킷 정책을 모니터링하는 데 적합하지 않습니다. 또한, Lambda 함수를 사용한 자동 수정은 복잡할 수 있습니다.

- **B. Use AWS Trusted Advisor to find publicly accessible S3 buckets. Configure email notifications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.**
  - Trusted Advisor는 공용으로 접근 가능한 S3 버킷을 찾는 데 유용하지만, 수동으로 변경하는 것은 관리 오버헤드가 큽니다.

- **C. Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function when a change is detected. Deploy a Lambda function that programmatically remediates the change.**
  - Resource Access Manager는 주로 리소스 공유를 관리하는 데 사용됩니다. SNS와 Lambda를 사용한 자동 수정은 복잡할 수 있습니다.

- **D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.**
  - S3 Block Public Access 기능을 계정 수준에서 사용하면 모든 S3 객체가 공용으로 노출되는 것을 방지할 수 있습니다. SCP를 사용하여 설정 변경을 방지하면 관리 오버헤드가 최소화됩니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **S3 Block Public Access**
   - S3 Block Public Access 기능을 사용하면 S3 버킷과 객체에 대한 공용 액세스를 차단할 수 있습니다.
   - 계정 수준에서 적용할 수 있습니다.

2. **AWS Organizations**
   - AWS Organizations는 여러 AWS 계정을 중앙에서 관리할 수 있는 서비스입니다.
   - 서비스 제어 정책(SCP)을 사용하여 특정 작업을 제한할 수 있습니다.

3. **IAM 정책**
   - IAM 정책은 특정 작업에 대한 권한을 부여하거나 거부할 수 있습니다.
   - SCP를 사용하여 IAM 사용자가 특정 설정을 변경하지 못하도록 할 수 있습니다.

### 추가 학습 자료
- [S3 Block Public Access 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/access-control-block-public-access.html)
- [AWS Organizations 공식 문서](https://docs.aws.amazon.com/ko_kr/organizations/latest/userguide/orgs_introduction.html)
- [IAM 정책 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/access_policies.html)

이 자료들을 통해 S3 Block Public Access, AWS Organizations, IAM 정책의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 413번
### 정답
B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).

### 해설
Amazon Simple Email Service (Amazon SES)는 대규모 이메일 전송을 위한 완전 관리형 서비스로, 이메일 전송의 복잡성을 줄이고 운영 오버헤드를 최소화할 수 있습니다. SES를 사용하면 이메일 전송 인프라를 관리할 필요가 없으며, 높은 신뢰성과 확장성을 제공합니다.

각 옵션을 자세히 살펴보면:

- **A. Create a separate application tier using EC2 instances dedicated to email processing.**
  - 별도의 EC2 인스턴스를 사용하여 이메일 처리를 전담하는 것은 가능하지만, 이는 추가적인 관리 오버헤드를 발생시킵니다.

- **B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).**
  - Amazon SES는 이메일 전송을 위한 완전 관리형 서비스로, 이메일 전송의 복잡성을 줄이고 운영 오버헤드를 최소화할 수 있습니다. 이는 가장 적합한 솔루션입니다.

- **C. Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).**
  - Amazon SNS는 주로 알림을 전송하는 데 사용되며, 이메일 전송을 위한 최적의 솔루션은 아닙니다.

- **D. Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in an Auto Scaling group.**
  - 별도의 EC2 인스턴스를 사용하여 이메일 처리를 전담하는 것은 가능하지만, 이는 추가적인 관리 오버헤드를 발생시킵니다. Auto Scaling을 사용하더라도 SES를 사용하는 것이 더 적합합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **Amazon Simple Email Service (SES)**
   - SES는 대규모 이메일 전송을 위한 완전 관리형 서비스입니다.
   - 높은 신뢰성과 확장성을 제공합니다.
   - 이메일 전송 인프라를 관리할 필요가 없습니다.

2. **Amazon Simple Notification Service (SNS)**
   - SNS는 주로 알림을 전송하는 데 사용됩니다.
   - 이메일 전송을 위한 최적의 솔루션은 아닙니다.

3. **EC2 인스턴스**
   - EC2 인스턴스를 사용하여 애플리케이션을 배포할 수 있습니다.
   - 별도의 인스턴스를 사용하여 이메일 처리를 전담하는 것은 추가적인 관리 오버헤드를 발생시킵니다.

### 추가 학습 자료
- [Amazon Simple Email Service (SES) 공식 문서](https://docs.aws.amazon.com/ko_kr/ses/latest/DeveloperGuide/Welcome.html)
- [Amazon Simple Notification Service (SNS) 공식 문서](https://docs.aws.amazon.com/ko_kr/sns/latest/dg/welcome.html)
- [Amazon EC2 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/concepts.html)

이 자료들을 통해 SES, SNS, EC2의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 416번
### 정답
B. Set up an Amazon CloudFront distribution.
D. Create a read replica for the RDS DB instance.

### 해설
웹 애플리케이션의 페이지 로드 속도를 개선하기 위해서는 정적 콘텐츠와 동적 콘텐츠 모두에 대한 최적화가 필요합니다. Amazon CloudFront는 전 세계적으로 콘텐츠를 캐시하여 정적 콘텐츠의 로드 시간을 줄일 수 있습니다. 또한, RDS DB 인스턴스에 대한 읽기 복제본을 생성하면 데이터베이스 읽기 작업을 분산시켜 데이터베이스 성능을 향상시킬 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Configure an Amazon Redshift cluster.**
  - Amazon Redshift는 데이터 웨어하우징 솔루션으로, OLTP 데이터베이스 성능 문제를 해결하는 데 적합하지 않습니다.

- **B. Set up an Amazon CloudFront distribution.**
  - CloudFront는 전 세계적으로 콘텐츠를 캐시하여 정적 콘텐츠의 로드 시간을 줄일 수 있습니다. 이는 웹 애플리케이션의 페이지 로드 속도를 개선하는 데 도움이 됩니다.

- **C. Host the dynamic web content in Amazon S3.**
  - 동적 웹 콘텐츠는 서버 측에서 생성되므로 S3에 호스팅하는 것은 적절하지 않습니다. 정적 콘텐츠는 S3에 호스팅할 수 있지만, 동적 콘텐츠는 서버 측에서 처리해야 합니다.

- **D. Create a read replica for the RDS DB instance.**
  - RDS DB 인스턴스에 대한 읽기 복제본을 생성하면 데이터베이스 읽기 작업을 분산시켜 데이터베이스 성능을 향상시킬 수 있습니다. 이는 OLTP 데이터베이스의 성능 문제를 해결하는 데 도움이 됩니다.

- **E. Configure a Multi-AZ deployment for the RDS DB instance.**
  - Multi-AZ 배포는 고가용성을 제공하지만, 데이터베이스 성능을 직접적으로 향상시키지는 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B와 D입니다.

### 필요한 학습 내용

1. **Amazon CloudFront**
   - CloudFront는 전 세계적으로 콘텐츠를 캐시하여 정적 콘텐츠의 로드 시간을 줄일 수 있는 콘텐츠 전송 네트워크(CDN)입니다.
   - 웹 애플리케이션의 페이지 로드 속도를 개선하는 데 도움이 됩니다.

2. **Amazon RDS 읽기 복제본**
   - RDS 읽기 복제본은 데이터베이스 읽기 작업을 분산시켜 데이터베이스 성능을 향상시킬 수 있습니다.
   - OLTP 데이터베이스의 성능 문제를 해결하는 데 도움이 됩니다.

3. **Amazon Redshift**
   - Redshift는 데이터 웨어하우징 솔루션으로, OLTP 데이터베이스 성능 문제를 해결하는 데 적합하지 않습니다.

4. **Amazon S3**
   - S3는 객체 스토리지 서비스로, 정적 콘텐츠를 호스팅하는 데 적합합니다.
   - 동적 콘텐츠는 서버 측에서 처리해야 합니다.

5. **Amazon RDS Multi-AZ 배포**
   - Multi-AZ 배포는 고가용성을 제공하지만, 데이터베이스 성능을 직접적으로 향상시키지는 않습니다.

### 추가 학습 자료
- [Amazon CloudFront 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonCloudFront/latest/DeveloperGuide/Introduction.html)
- [Amazon RDS 읽기 복제본 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_ReadRepl.html)
- [Amazon Redshift 공식 문서](https://docs.aws.amazon.com/ko_kr/redshift/latest/mgmt/welcome.html)
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/Welcome.html)
- [Amazon RDS Multi-AZ 배포 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html)

이 자료들을 통해 CloudFront, RDS 읽기 복제본, Redshift, S3, RDS Multi-AZ 배포의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 417번 
### 정답
C. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.

### 해설
Compute Savings Plan은 EC2 인스턴스와 Lambda 함수 모두에 대해 비용 절감을 제공합니다. Lambda 함수의 실행 시간, 메모리 사용량, 호출 횟수 및 데이터 전송량을 최적화하면 비용을 더욱 절감할 수 있습니다. Lambda 함수를 EC2 인스턴스가 있는 프라이빗 서브넷에 연결하면 네트워크 지연 시간을 최소화할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Purchase an EC2 Instance Savings Plan. Optimize the Lambda functions’ duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.**
  - EC2 Instance Savings Plan은 EC2 인스턴스에 대해서만 비용 절감을 제공합니다. Lambda 함수에 대한 비용 절감은 제공하지 않습니다.

- **B. Purchase an EC2 Instance Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.**
  - Lambda 함수를 퍼블릭 서브넷에 연결하면 보안 문제가 발생할 수 있으며, EC2 Instance Savings Plan은 Lambda 함수에 대한 비용 절감을 제공하지 않습니다.

- **C. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.**
  - Compute Savings Plan은 EC2 인스턴스와 Lambda 함수 모두에 대해 비용 절감을 제공합니다. Lambda 함수를 프라이빗 서브넷에 연결하면 네트워크 지연 시간을 최소화할 수 있습니다.

- **D. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC.**
  - Lambda 함수를 Lambda 서비스 VPC에 유지하면 네트워크 지연 시간이 증가할 수 있습니다.

* VPC 간 통신은 지연시간과 비용이 증가할 수 있음

# 418번 
### 정답
B. Add the development account as a principal in the trust policy of the role in the production account.

### 해설
원칙적으로 최소 권한 원칙을 준수하면서 두 개의 AWS 계정(개발 계정과 프로덕션 계정)에서 S3 버킷에 접근할 수 있도록 하기 위해서는, 개발 계정의 사용자들이 프로덕션 계정의 IAM 역할을 가정할 수 있도록 설정해야 합니다. 이를 위해서는 프로덕션 계정의 역할 신뢰 정책에 개발 계정을 주체로 추가해야 합니다.

각 옵션을 자세히 살펴보면:

- **A. Attach the Administrator Access policy to the development account users.**
  - 관리자 액세스 정책을 개발 계정 사용자에게 부여하면 과도한 권한이 부여되어 최소 권한 원칙을 위반하게 됩니다.

- **B. Add the development account as a principal in the trust policy of the role in the production account.**
  - 개발 계정을 프로덕션 계정의 역할 신뢰 정책에 주체로 추가하면, 개발 계정의 사용자들이 프로덕션 계정의 역할을 가정할 수 있게 되어 최소 권한 원칙을 준수하면서 접근을 허용할 수 있습니다.

- **C. Turn off the S3 Block Public Access feature on the S3 bucket in the production account.**
  - S3 퍼블릭 액세스 차단 기능을 끄는 것은 보안 위험을 초래할 수 있으며, 최소 권한 원칙을 준수하지 않습니다.

- **D. Create a user in the production account with unique credentials for each team member.**
  - 프로덕션 계정에 각 팀원을 위한 별도의 사용자를 생성하는 것은 관리 오버헤드가 증가하며, 최소 권한 원칙을 준수하지 않을 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **IAM 역할 신뢰 정책**
   - IAM 역할 신뢰 정책은 어떤 주체(사용자, 그룹, 계정 등)가 역할을 가정할 수 있는지를 정의합니다.
   - 다른 계정의 사용자가 역할을 가정할 수 있도록 설정할 수 있습니다.

2. **최소 권한 원칙**
   - 최소 권한 원칙은 사용자가 작업을 수행하는 데 필요한 최소한의 권한만 부여하는 보안 원칙입니다.
   - 과도한 권한 부여를 방지하여 보안을 강화합니다.

### 추가 학습 자료
- [IAM 역할 신뢰 정책 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles_manage_modify.html#roles-modify_trust-policy)
- [최소 권한 원칙 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/best-practices.html#grant-least-privilege)

이 자료들을 통해 IAM 역할 신뢰 정책과 최소 권한 원칙의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 419번
### 정답
A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.
C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.

### 해설
회사는 모든 데이터가 암호화된 상태로 저장되도록 보장하고, 새로운 EBS 볼륨이 암호화되지 않은 상태로 생성되지 않도록 해야 합니다. 이를 위해 기본 EBS 암호화 설정을 사용하고, 서비스 제어 정책(SCP)을 통해 암호화되지 않은 EBS 볼륨 생성을 방지할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.**
  - Amazon EC2 콘솔에서 EBS 암호화 계정 속성을 선택하고 기본 암호화 키를 정의하면, 새로 생성되는 모든 EBS 볼륨이 기본적으로 암호화됩니다. 이는 직원들에게 최소한의 영향을 미치면서 요구 사항을 충족합니다.

- **B. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Define the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.**
  - IAM 권한 경계는 특정 사용자나 역할에 대한 권한을 제한하는 데 사용되며, 조직 전체에 적용하기에는 적합하지 않습니다.

- **C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.**
  - SCP를 사용하여 조직 전체에 정책을 적용할 수 있습니다. ec2:CreateVolume 작업을 ec2:Encrypted 조건이 false일 때 거부하도록 정의하면, 암호화되지 않은 EBS 볼륨 생성을 방지할 수 있습니다.

- **D. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.**
  - 각 계정의 IAM 정책을 업데이트하는 것은 관리 오버헤드가 크며, 조직 전체에 일관된 정책을 적용하기 어렵습니다.

- **E. In the Organizations management account, specify the Default EBS volume encryption setting.**
  - AWS Organizations 관리 계정에서 기본 EBS 볼륨 암호화 설정을 지정하는 옵션은 현재 존재하지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A와 C입니다.

### 필요한 학습 내용

1. **Amazon EC2 기본 EBS 암호화 설정**
   - 기본 EBS 암호화 설정을 사용하면 새로 생성되는 모든 EBS 볼륨이 자동으로 암호화됩니다.
   - 기본 암호화 키를 정의할 수 있습니다.

2. **서비스 제어 정책 (SCP)**
   - SCP는 AWS Organizations에서 조직 전체에 적용할 수 있는 정책입니다.
   - 특정 작업을 조건부로 허용하거나 거부할 수 있습니다.

### 추가 학습 자료
- [Amazon EC2 기본 EBS 암호화 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/EBSEncryption.html)
- [AWS Organizations 서비스 제어 정책 (SCP) 공식 문서](https://docs.aws.amazon.com/ko_kr/organizations/latest/userguide/orgs_manage_policies_scp.html)

이 자료들을 통해 기본 EBS 암호화 설정과 SCP의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 420번
### 정답
D. Use an Amazon RDS Multi-AZ DB cluster deployment. Point the read workload to the reader endpoint.

### 해설
Amazon RDS Multi-AZ DB 클러스터 배포는 고가용성과 자동 장애 조치 기능을 제공하며, 읽기 작업을 오프로드할 수 있는 리더 엔드포인트를 제공합니다. 이는 고가용성을 보장하고, 읽기 작업을 기본 인스턴스에서 분리하여 성능을 최적화할 수 있습니다. 또한, 리더 엔드포인트를 사용하면 읽기 작업을 자동으로 분산시킬 수 있어 관리 오버헤드가 줄어듭니다.

각 옵션을 자세히 살펴보면:

- **A. Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.**
  - Multi-AZ DB 인스턴스 배포는 고가용성을 제공하지만, 읽기 복제본을 추가로 생성해야 하므로 비용이 증가할 수 있습니다.

- **B. Use an Amazon RDS Multi-AZ DB cluster deployment. Create two read replicas and point the read workload to the read replicas.**
  - Multi-AZ DB 클러스터 배포는 고가용성을 제공하지만, 두 개의 읽기 복제본을 추가로 생성하면 비용이 증가할 수 있습니다.

- **C. Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.**
  - Multi-AZ DB 인스턴스 배포에서 보조 인스턴스는 장애 조치 용도로만 사용되며, 읽기 작업을 처리할 수 없습니다.

- **D. Use an Amazon RDS Multi-AZ DB cluster deployment. Point the read workload to the reader endpoint.**
  - Multi-AZ DB 클러스터 배포는 고가용성과 자동 장애 조치 기능을 제공하며, 리더 엔드포인트를 사용하여 읽기 작업을 오프로드할 수 있습니다. 이는 비용을 최소화하면서 요구 사항을 충족하는 최적의 솔루션입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **Amazon RDS Multi-AZ DB 클러스터**
   - Multi-AZ DB 클러스터는 고가용성과 자동 장애 조치 기능을 제공하는 RDS 배포 옵션입니다.
   - 리더 엔드포인트를 사용하여 읽기 작업을 오프로드할 수 있습니다.

2. **Amazon RDS 읽기 복제본**
   - 읽기 복제본은 데이터베이스의 읽기 작업을 분산시켜 성능을 최적화할 수 있습니다.
   - 추가 비용이 발생할 수 있습니다.

3. **Amazon RDS Multi-AZ DB 인스턴스**
   - Multi-AZ DB 인스턴스 배포는 고가용성을 제공하지만, 보조 인스턴스는 장애 조치 용도로만 사용됩니다.

### 추가 학습 자료
- [Amazon RDS Multi-AZ DB 클러스터 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/multi-az-db-clusters.html)
- [Amazon RDS 읽기 복제본 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_ReadRepl.html)
- [Amazon RDS Multi-AZ DB 인스턴스 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html)

이 자료들을 통해 Multi-AZ DB 클러스터, 읽기 복제본, Multi-AZ DB 인스턴스의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 421번
### 정답
C. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

### 해설
AWS Transfer Family는 SFTP, FTPS, 및 FTP를 지원하는 완전 관리형 서비스로, 서버리스 옵션을 제공합니다. Amazon S3를 백엔드 스토리지로 사용하면 높은 IOPS 성능과 유연한 보안 구성을 제공할 수 있습니다. 또한, AWS Transfer Family를 사용하면 사용자 권한을 세밀하게 제어할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.**
  - AWS Transfer Family는 EBS 볼륨을 직접 지원하지 않습니다. 따라서 이 옵션은 적합하지 않습니다.

- **B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.**
  - AWS Transfer Family는 EFS 볼륨을 직접 지원하지 않습니다. 따라서 이 옵션은 적합하지 않습니다.

- **C. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.**
  - AWS Transfer Family는 S3 버킷을 백엔드 스토리지로 지원합니다. S3 버킷에 기본 암호화를 설정하고, 공용 엔드포인트를 사용하여 신뢰할 수 있는 IP 주소만 허용하면 보안과 성능을 모두 충족할 수 있습니다.

- **D. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.**
  - VPC 엔드포인트를 사용하면 내부 접근만 가능하게 설정할 수 있지만, 공용 엔드포인트를 사용하는 것이 더 간단하고 운영 오버헤드가 적습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **AWS Transfer Family**
   - AWS Transfer Family는 SFTP, FTPS, 및 FTP를 지원하는 완전 관리형 서비스입니다.
   - Amazon S3를 백엔드 스토리지로 사용할 수 있습니다.

2. **Amazon S3**
   - S3는 객체 스토리지 서비스로, 높은 IOPS 성능과 유연한 보안 구성을 제공합니다.
   - 기본 암호화를 설정하여 데이터를 보호할 수 있습니다.

3. **보안 그룹 및 VPC 엔드포인트**
   - 보안 그룹을 사용하여 특정 IP 주소만 접근할 수 있도록 설정할 수 있습니다.
   - VPC 엔드포인트를 사용하여 내부 접근만 가능하게 설정할 수 있습니다.

### 추가 학습 자료
- [AWS Transfer Family 공식 문서](https://docs.aws.amazon.com/ko_kr/transfer/latest/userguide/what-is-aws-transfer.html)
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/Welcome.html)
- [보안 그룹 공식 문서](https://docs.aws.amazon.com/ko_kr/vpc/latest/userguide/VPC_SecurityGroups.html)
- [VPC 엔드포인트 공식 문서](https://docs.aws.amazon.com/ko_kr/vpc/latest/userguide/vpc-endpoints.html)

이 자료들을 통해 AWS Transfer Family, S3, 보안 그룹, VPC 엔드포인트의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 422번
### 정답
D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

### 해설
이 문제는 비정기적인 사용 패턴을 가진 머신 러닝 모델을 효율적으로 배포하고 확장하는 방법을 평가합니다. Amazon SQS와 Amazon ECS를 사용하여 비동기 API 요청을 처리하고, Auto Scaling을 통해 클러스터와 서비스 인스턴스를 동적으로 확장하는 것이 가장 적합한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.**
  - Lambda 함수는 짧은 실행 시간에 적합하며, 1GB의 모델 데이터를 메모리에 로드하는 작업에는 적합하지 않습니다. 또한, NLB는 비동기 요청 처리에 적합하지 않습니다.

- **B. Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.**
  - ALB와 ECS를 사용하는 것은 적절하지만, AWS App Mesh는 서비스 메쉬를 관리하는 데 사용되며, Auto Scaling을 직접적으로 지원하지 않습니다.

- **C. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.**
  - Lambda 함수는 짧은 실행 시간에 적합하며, 1GB의 모델 데이터를 메모리에 로드하는 작업에는 적합하지 않습니다. 또한, Lambda 함수는 vCPU를 직접적으로 확장할 수 없습니다.

- **D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.**
  - SQS 큐를 사용하여 비동기 요청을 처리하고, ECS 서비스를 사용하여 모델을 배포하는 것은 적절한 접근 방식입니다. Auto Scaling을 통해 클러스터와 서비스 인스턴스를 동적으로 확장할 수 있어 비정기적인 사용 패턴을 효율적으로 처리할 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **Amazon Simple Queue Service (SQS)**
   - SQS는 메시지를 큐에 저장하고, 이를 소비자에게 전달하는 메시징 서비스입니다.
   - 비동기 요청을 처리하는 데 적합합니다.

2. **Amazon Elastic Container Service (ECS)**
   - ECS는 컨테이너 오케스트레이션 서비스로, 애플리케이션을 컨테이너로 배포하고 관리할 수 있습니다.
   - Auto Scaling을 통해 클러스터와 서비스 인스턴스를 동적으로 확장할 수 있습니다.

3. **AWS Auto Scaling**
   - Auto Scaling은 애플리케이션의 수요에 따라 리소스를 자동으로 확장하거나 축소할 수 있는 서비스입니다.
   - ECS 클러스터와 서비스 인스턴스를 동적으로 확장할 수 있습니다.

4. **AWS Lambda**
   - Lambda는 서버리스 컴퓨팅을 제공하는 서비스로, 특정 이벤트에 따라 코드를 실행할 수 있습니다.
   - 짧은 실행 시간에 적합하며, 1GB의 모델 데이터를 메모리에 로드하는 작업에는 적합하지 않습니다.

5. **Network Load Balancer (NLB) 및 Application Load Balancer (ALB)**
   - NLB는 TCP/UDP 트래픽을 처리하는 로드 밸런서입니다.
   - ALB는 HTTP/HTTPS 트래픽을 처리하는 로드 밸런서로, 비동기 요청 처리에는 적합하지 않습니다.

### 추가 학습 자료
- [Amazon SQS 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html)
- [Amazon ECS 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/developerguide/Welcome.html)
- [AWS Auto Scaling 공식 문서](https://docs.aws.amazon.com/ko_kr/autoscaling/latest/userguide/WhatIsAutoScaling.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)
- [Network Load Balancer (NLB) 공식 문서](https://docs.aws.amazon.com/ko_kr/elasticloadbalancing/latest/network/introduction.html)
- [Application Load Balancer (ALB) 공식 문서](https://docs.aws.amazon.com/ko_kr/elasticloadbalancing/latest/application/introduction.html)

이 자료들을 통해 SQS, ECS, Auto Scaling, Lambda, NLB, ALB의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 423번
### 정답
A. Role
B. Group

### 해설
IAM 정책은 사용자, 그룹, 역할과 같은 IAM 주체(principal)에 연결할 수 있습니다. 이 정책은 특정 작업(`ssm:ListDocuments` 및 `ssm:GetDocument`)에 대한 권한을 부여합니다. IAM 정책은 리소스 기반 정책이 아니므로, Amazon ECS 리소스나 Amazon EC2 리소스에 직접 연결할 수 없습니다.

각 옵션을 자세히 살펴보면:

- **A. Role**
  - IAM 역할은 특정 작업을 수행할 수 있는 권한을 부여받은 주체입니다. 이 정책을 역할에 연결할 수 있습니다.

- **B. Group**
  - IAM 그룹은 여러 사용자를 포함할 수 있는 주체입니다. 이 정책을 그룹에 연결할 수 있습니다.

- **C. Organization**
  - IAM 정책은 AWS Organizations의 조직 전체에 직접 연결할 수 없습니다. 조직 단위(OU)나 계정 수준에서 SCP(서비스 제어 정책)를 사용할 수 있지만, 이는 다른 유형의 정책입니다.

- **D. Amazon Elastic Container Service (Amazon ECS) resource**
  - IAM 정책은 Amazon ECS 리소스에 직접 연결할 수 없습니다. 대신, ECS 작업 정의에서 IAM 역할을 사용하여 권한을 부여할 수 있습니다.

- **E. Amazon EC2 resource**
  - IAM 정책은 Amazon EC2 인스턴스에 직접 연결할 수 없습니다. 대신, EC2 인스턴스 프로파일을 사용하여 IAM 역할을 연결할 수 있습니다.

따라서, 이 정책을 연결할 수 있는 IAM 주체는 옵션 A와 B입니다.

### 필요한 학습 내용

1. **IAM 정책**
   - IAM 정책은 사용자, 그룹, 역할과 같은 IAM 주체에 연결할 수 있습니다.
   - 특정 작업에 대한 권한을 부여할 수 있습니다.

2. **IAM 역할**
   - IAM 역할은 특정 작업을 수행할 수 있는 권한을 부여받은 주체입니다.
   - 다른 AWS 서비스나 사용자에게 권한을 위임할 수 있습니다.

3. **IAM 그룹**
   - IAM 그룹은 여러 사용자를 포함할 수 있는 주체입니다.
   - 그룹에 연결된 정책은 그룹의 모든 사용자에게 적용됩니다.

4. **AWS Organizations**
   - AWS Organizations는 여러 AWS 계정을 중앙에서 관리할 수 있는 서비스입니다.
   - 서비스 제어 정책(SCP)을 사용하여 조직 단위(OU)나 계정 수준에서 권한을 제어할 수 있습니다.

5. **Amazon ECS 및 Amazon EC2**
   - ECS 작업 정의에서 IAM 역할을 사용하여 권한을 부여할 수 있습니다.
   - EC2 인스턴스 프로파일을 사용하여 IAM 역할을 연결할 수 있습니다.

### 추가 학습 자료
- [IAM 정책 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/access_policies.html)
- [IAM 역할 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles.html)
- [IAM 그룹 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_groups.html)
- [AWS Organizations 공식 문서](https://docs.aws.amazon.com/ko_kr/organizations/latest/userguide/orgs_introduction.html)
- [Amazon ECS 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/developerguide/Welcome.html)
- [Amazon EC2 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/concepts.html)

이 자료들을 통해 IAM 정책, 역할, 그룹, AWS Organizations, Amazon ECS, Amazon EC2의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 426번
### 정답
A. Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.

### 해설
회사는 의료 애플리케이션의 데이터를 저장해야 하며, 데이터가 자주 변경됩니다. 새로운 규정에 따라 저장된 데이터의 모든 수준에서 감사 액세스가 필요합니다. AWS로 데이터를 안전하게 마이그레이션하고, 감사 요구 사항을 충족해야 합니다.

각 옵션을 자세히 살펴보면:

- **A. Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.**
  - AWS DataSync는 데이터를 AWS로 안전하게 전송하는 데 사용됩니다. Amazon S3는 데이터를 저장하는 데 적합하며, AWS CloudTrail을 사용하여 데이터 이벤트를 로깅하면 감사 요구 사항을 충족할 수 있습니다.

- **B. Use AWS Snowcone to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.**
  - AWS Snowcone은 소규모 데이터 전송에 적합하며, 대규모 데이터 전송에는 적합하지 않습니다. 또한, 관리 이벤트만 로깅하는 것은 감사 요구 사항을 충족하지 못합니다.

- **C. Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.**
  - Amazon S3 Transfer Acceleration은 데이터 전송 속도를 높이는 데 사용되지만, DataSync가 더 적합한 솔루션입니다. CloudTrail을 사용하여 데이터 이벤트를 로깅하는 것은 적절합니다.

- **D. Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.**
  - AWS Storage Gateway는 온프레미스 데이터를 AWS로 이동하는 데 사용될 수 있지만, DataSync가 더 적합한 솔루션입니다. 또한, 관리 이벤트만 로깅하는 것은 감사 요구 사항을 충족하지 못합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **AWS DataSync**
   - DataSync는 데이터를 AWS로 안전하게 전송하는 데 사용됩니다.
   - 대규모 데이터 전송에 적합합니다.

2. **Amazon S3**
   - S3는 객체 스토리지 서비스로, 데이터를 안전하게 저장할 수 있습니다.
   - 데이터 이벤트를 로깅할 수 있습니다.

3. **AWS CloudTrail**
   - CloudTrail은 AWS 계정의 API 호출 및 관련 이벤트를 로깅하는 서비스입니다.
   - 데이터 이벤트와 관리 이벤트를 로깅할 수 있습니다.

4. **AWS Snowcone**
   - Snowcone은 소규모 데이터 전송에 적합한 장치입니다.
   - 대규모 데이터 전송에는 적합하지 않습니다.

5. **Amazon S3 Transfer Acceleration**
   - Transfer Acceleration은 데이터 전송 속도를 높이는 데 사용됩니다.
   - DataSync가 더 적합한 솔루션입니다.

6. **AWS Storage Gateway**
   - Storage Gateway는 온프레미스 데이터를 AWS로 이동하는 데 사용될 수 있습니다.
   - DataSync가 더 적합한 솔루션입니다.

### 추가 학습 자료
- [AWS DataSync 공식 문서](https://docs.aws.amazon.com/ko_kr/datasync/latest/userguide/what-is-datasync.html)
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/Welcome.html)
- [AWS CloudTrail 공식 문서](https://docs.aws.amazon.com/ko_kr/awscloudtrail/latest/userguide/cloudtrail-user-guide.html)
- [AWS Snowcone 공식 문서](https://docs.aws.amazon.com/ko_kr/snowcone/latest/developer-guide/what-is-snowcone.html)
- [Amazon S3 Transfer Acceleration 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/transfer-acceleration.html)
- [AWS Storage Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/storagegateway/latest/userguide/WhatIsStorageGateway.html)

이 자료들을 통해 DataSync, S3, CloudTrail, Snowcone, Transfer Acceleration, Storage Gateway의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 428번
### 정답
B. Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the DynamoDB table. Update the configuration of the Lambda function to use the new role as the execution role.

### 해설
AWS Lambda 함수가 DynamoDB 테이블에 접근할 수 있도록 가장 안전한 방법은 IAM 역할을 사용하는 것입니다. IAM 역할을 사용하면 Lambda 함수가 필요한 권한만을 가지도록 제한할 수 있으며, 액세스 키와 시크릿 키를 코드나 환경 변수에 저장할 필요가 없습니다.

각 옵션을 자세히 살펴보면:

- **A. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters as part of the Lambda environment variables. Ensure that other AWS users do not have read and write access to the Lambda function configuration.**
  - IAM 사용자를 생성하고 액세스 키와 시크릿 키를 환경 변수에 저장하는 것은 보안 위험이 있습니다. 키가 노출될 가능성이 있으며, 이는 안전하지 않은 방법입니다.

- **B. Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the DynamoDB table. Update the configuration of the Lambda function to use the new role as the execution role.**
  - IAM 역할을 사용하면 Lambda 함수가 필요한 권한만을 가지도록 제한할 수 있습니다. 이는 가장 안전한 방법입니다. Lambda 함수의 실행 역할로 이 역할을 사용하도록 구성하면 됩니다.

- **C. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters in AWS Systems Manager Parameter Store as secure string parameters. Update the Lambda function code to retrieve the secure string parameters before connecting to the DynamoDB table.**
  - IAM 사용자를 생성하고 키를 Parameter Store에 저장하는 것은 보안이 강화되지만, 여전히 키를 코드에서 사용해야 하므로 보안 위험이 있습니다.

- **D. Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that allows read and write access from the Lambda function. Update the code of the Lambda function to attach to the new role as an execution role.**
  - DynamoDB를 신뢰할 수 있는 서비스로 포함하는 것은 잘못된 설정입니다. Lambda를 신뢰할 수 있는 서비스로 포함해야 합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **IAM 역할**
   - IAM 역할은 특정 작업을 수행할 수 있는 권한을 부여받은 주체입니다.
   - Lambda 함수가 필요한 권한만을 가지도록 제한할 수 있습니다.

2. **AWS Lambda**
   - Lambda는 서버리스 컴퓨팅을 제공하는 서비스로, 특정 이벤트에 따라 코드를 실행할 수 있습니다.
   - IAM 역할을 사용하여 필요한 권한을 부여할 수 있습니다.

3. **Amazon DynamoDB**
   - DynamoDB는 완전 관리형 NoSQL 데이터베이스 서비스입니다.
   - IAM 역할을 사용하여 Lambda 함수가 DynamoDB 테이블에 접근할 수 있도록 권한을 부여할 수 있습니다.

### 추가 학습 자료
- [IAM 역할 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)
- [Amazon DynamoDB 공식 문서](https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/Introduction.html)

이 자료들을 통해 IAM 역할, Lambda, DynamoDB의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 430번
### 정답
B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket. Invoke the Lambda function when a .csv file is uploaded.
C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days.

### 해설
이 문제는 비용 효율적으로 .csv 파일을 이미지로 변환하고, 이미지와 .csv 파일을 적절히 관리하는 방법을 평가합니다. AWS Lambda를 사용하여 .csv 파일을 이미지로 변환하고, S3 Lifecycle 규칙을 사용하여 스토리지 비용을 최적화하는 것이 가장 적합한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the image files, and uploads the images to the S3 bucket.**
  - EC2 Spot 인스턴스를 사용하는 것은 가능하지만, Lambda를 사용하는 것이 더 비용 효율적이고 관리 오버헤드가 적습니다.

- **B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket. Invoke the Lambda function when a .csv file is uploaded.**
  - Lambda 함수는 서버리스 컴퓨팅을 제공하며, .csv 파일이 업로드될 때 자동으로 이미지를 생성할 수 있습니다. 이는 비용 효율적이고 관리 오버헤드가 적습니다.

- **C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days.**
  - S3 Lifecycle 규칙을 사용하여 .csv 파일을 S3 Glacier로 전환하면 비용을 절감할 수 있습니다. 이미지 파일은 30일 후에 만료시켜 스토리지 비용을 최적화할 수 있습니다.

- **D. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 1 day after they are uploaded. Expire the image files after 30 days.**
  - S3 One Zone-IA는 비용이 저렴하지만, 데이터 내구성이 낮습니다. S3 Glacier를 사용하는 것이 더 적합합니다.

- **E. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 1 day after they are uploaded. Keep the image files in Reduced Redundancy Storage (RRS).**
  - S3 Standard-IA는 비용이 저렴하지만, S3 Glacier를 사용하는 것이 더 비용 효율적입니다. RRS는 더 이상 권장되지 않는 스토리지 클래스입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B와 C입니다.

### 필요한 학습 내용

1. **AWS Lambda**
   - Lambda는 서버리스 컴퓨팅을 제공하며, 특정 이벤트에 따라 코드를 실행할 수 있습니다.
   - .csv 파일이 업로드될 때 자동으로 이미지를 생성할 수 있습니다.

2. **Amazon S3**
   - S3는 객체 스토리지 서비스로, 데이터를 안전하게 저장할 수 있습니다.
   - S3 Lifecycle 규칙을 사용하여 스토리지 비용을 최적화할 수 있습니다.

3. **S3 Glacier**
   - S3 Glacier는 장기 아카이빙을 위한 저비용 스토리지 서비스입니다.
   - 자주 접근하지 않는 데이터를 저장하는 데 적합합니다.

4. **S3 Lifecycle 규칙**
   - S3 Lifecycle 규칙을 사용하여 객체의 스토리지 클래스를 자동으로 전환하거나 만료시킬 수 있습니다.
   - 스토리지 비용을 최적화하는 데 도움이 됩니다.

### 추가 학습 자료
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/Welcome.html)
- [S3 Glacier 공식 문서](https://docs.aws.amazon.com/ko_kr/amazonglacier/latest/dev/introduction.html)
- [S3 Lifecycle 규칙 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)

이 자료들을 통해 Lambda, S3, S3 Glacier, S3 Lifecycle 규칙의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.



# 431번
### 정답
B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.

### 해설
Amazon ElastiCache for Redis는 높은 성능과 낮은 지연 시간을 제공하는 인메모리 데이터 저장소로, 실시간 데이터 처리와 캐싱에 적합합니다. Redis는 데이터 구조를 지원하며, 순위 계산과 같은 작업을 효율적으로 처리할 수 있습니다. 이를 통해 실시간으로 업데이트되는 상위 10개 점수판을 표시할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web application to display.**
  - Memcached는 단순한 키-값 저장소로, 순위 계산과 같은 복잡한 작업을 처리하는 데 적합하지 않습니다.

- **B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.**
  - Redis는 데이터 구조를 지원하며, 순위 계산과 같은 작업을 효율적으로 처리할 수 있습니다. 또한, Redis는 높은 성능과 낮은 지연 시간을 제공하여 실시간 데이터 처리에 적합합니다.

- **C. Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard in a section of the application.**
  - CloudFront는 콘텐츠 배포 네트워크(CDN)로, 정적 콘텐츠를 캐싱하는 데 적합합니다. 실시간으로 업데이트되는 점수판을 캐싱하는 데는 적합하지 않습니다.

- **D. Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and serve the read traffic to the web application.**
  - RDS 읽기 복제본을 사용하여 순위를 계산하는 것은 가능하지만, 데이터베이스에 부하를 줄 수 있으며, 실시간 데이터 처리에 적합하지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **Amazon ElastiCache for Redis**
   - Redis는 높은 성능과 낮은 지연 시간을 제공하는 인메모리 데이터 저장소입니다.
   - 데이터 구조를 지원하며, 순위 계산과 같은 작업을 효율적으로 처리할 수 있습니다.

2. **Amazon ElastiCache for Memcached**
   - Memcached는 단순한 키-값 저장소로, 순위 계산과 같은 복잡한 작업을 처리하는 데 적합하지 않습니다.

3. **Amazon CloudFront**
   - CloudFront는 콘텐츠 배포 네트워크(CDN)로, 정적 콘텐츠를 캐싱하는 데 적합합니다.
   - 실시간으로 업데이트되는 점수판을 캐싱하는 데는 적합하지 않습니다.

4. **Amazon RDS 읽기 복제본**
   - RDS 읽기 복제본을 사용하여 데이터베이스의 읽기 부하를 분산할 수 있습니다.
   - 순위 계산과 같은 작업을 처리하는 데는 적합하지 않습니다.

### 추가 학습 자료
- [Amazon ElastiCache for Redis 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonElastiCache/latest/red-ug/WhatIs.html)
- [Amazon ElastiCache for Memcached 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonElastiCache/latest/mem-ug/WhatIs.html)
- [Amazon CloudFront 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonCloudFront/latest/DeveloperGuide/Introduction.html)
- [Amazon RDS 읽기 복제본 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_ReadRepl.html)

이 자료들을 통해 ElastiCache for Redis, ElastiCache for Memcached, CloudFront, RDS 읽기 복제본의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 434번
### 정답
A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

### 해설
이 문제는 애플리케이션을 다른 AWS 리전에서 최소한의 다운타임으로 사용할 수 있도록 하는 방법을 평가합니다. 가장 적합한 솔루션은 재해 복구 리전에 Auto Scaling 그룹과 로드 밸런서를 미리 설정하고, DynamoDB 테이블을 글로벌 테이블로 구성하여 데이터 복제를 보장하는 것입니다. 또한, DNS 장애 조치를 구성하여 재해 발생 시 새로운 리전의 로드 밸런서로 트래픽을 전환할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.**
  - 이 옵션은 재해 복구 리전에 미리 Auto Scaling 그룹과 로드 밸런서를 설정하고, DynamoDB 테이블을 글로벌 테이블로 구성하여 데이터 복제를 보장합니다. DNS 장애 조치를 통해 재해 발생 시 새로운 리전의 로드 밸런서로 트래픽을 전환할 수 있습니다. 이는 최소한의 다운타임을 보장하는 솔루션입니다.

- **B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed. Configure DNS failover to point to the new disaster recovery Region's load balancer.**
  - 이 옵션은 재해 발생 시 CloudFormation 템플릿을 사용하여 리소스를 생성하는 방법입니다. 그러나 리소스를 생성하는 데 시간이 걸리므로 다운타임이 발생할 수 있습니다.

- **C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.**
  - 이 옵션은 재해 발생 시 CloudFormation 템플릿을 사용하여 EC2 인스턴스와 로드 밸런서를 생성하는 방법입니다. DynamoDB 테이블을 글로벌 테이블로 구성하는 것은 적절하지만, 리소스를 생성하는 데 시간이 걸리므로 다운타임이 발생할 수 있습니다.

- **D. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.**
  - 이 옵션은 CloudWatch 알람과 Lambda 함수를 사용하여 Route 53을 업데이트하는 방법입니다. 이는 자동화된 솔루션이지만, DNS 장애 조치를 통해 트래픽을 전환하는 것이 더 간단하고 신뢰할 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Auto Scaling 그룹**
   - Auto Scaling 그룹은 EC2 인스턴스를 자동으로 확장하거나 축소할 수 있는 기능을 제공합니다.
   - 재해 복구 리전에 미리 설정하여 최소한의 다운타임을 보장할 수 있습니다.

2. **Elastic Load Balancer (ELB)**
   - ELB는 트래픽을 여러 EC2 인스턴스에 분산시켜 고가용성을 보장합니다.
   - 재해 복구 리전에 미리 설정하여 최소한의 다운타임을 보장할 수 있습니다.

3. **Amazon DynamoDB 글로벌 테이블**
   - 글로벌 테이블은 여러 리전에 걸쳐 데이터를 자동으로 복제하여 고가용성과 내구성을 보장합니다.
   - 재해 복구 시 데이터 손실을 방지할 수 있습니다.

4. **DNS 장애 조치**
   - DNS 장애 조치는 재해 발생 시 트래픽을 다른 리전으로 전환할 수 있는 기능을 제공합니다.
   - Route 53을 사용하여 구성할 수 있습니다.

### 추가 학습 자료
- [Auto Scaling 그룹 공식 문서](https://docs.aws.amazon.com/ko_kr/autoscaling/latest/userguide/what-is-amazon-ec2-auto-scaling.html)
- [Elastic Load Balancer (ELB) 공식 문서](https://docs.aws.amazon.com/ko_kr/elasticloadbalancing/latest/userguide/what-is-load-balancing.html)
- [Amazon DynamoDB 글로벌 테이블 공식 문서](https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/GlobalTables.html)
- [DNS 장애 조치 공식 문서](https://docs.aws.amazon.com/ko_kr/Route53/latest/DeveloperGuide/dns-failover.html)

이 자료들을 통해 Auto Scaling 그룹, ELB, DynamoDB 글로벌 테이블, DNS 장애 조치의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 436번 
### 정답
A. Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB instance larger.

### 해설
회사는 증가된 워크로드를 처리하기 위해 인프라를 추가하지 않고 비용 효율적으로 데이터베이스 성능을 향상시키고자 합니다. Amazon RDS에서 예약 인스턴스를 구매하면 장기적으로 비용을 절감할 수 있으며, 인스턴스 크기를 늘려 성능을 향상시킬 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB instance larger.**
  - 예약 인스턴스를 구매하면 장기적으로 비용을 절감할 수 있습니다. 인스턴스 크기를 늘리면 성능을 향상시킬 수 있습니다. 이는 가장 비용 효율적인 솔루션입니다.

- **B. Make the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance.**
  - Multi-AZ 배포는 고가용성과 내구성을 제공하지만, 성능 향상에는 직접적인 영향을 미치지 않습니다. 또한, 비용이 증가할 수 있습니다.

- **C. Buy reserved DB instances for the total workload. Add another Amazon RDS for PostgreSQL DB instance.**
  - 추가 인스턴스를 생성하면 비용이 증가할 수 있습니다. 인프라를 추가하지 않고 성능을 향상시키는 것이 목표이므로 적합하지 않습니다.

- **D. Make the Amazon RDS for PostgreSQL DB instance an on-demand DB instance.**
  - 온디맨드 인스턴스는 유연성을 제공하지만, 장기적으로 예약 인스턴스보다 비용이 더 많이 들 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Amazon RDS 예약 인스턴스**
   - 예약 인스턴스를 구매하면 장기적으로 비용을 절감할 수 있습니다.
   - 특정 기간 동안 인스턴스를 예약하여 할인된 가격으로 사용할 수 있습니다.

2. **Amazon RDS 인스턴스 크기 조정**
   - 인스턴스 크기를 늘리면 데이터베이스 성능을 향상시킬 수 있습니다.
   - 증가된 워크로드를 처리할 수 있습니다.

3. **Multi-AZ 배포**
   - Multi-AZ 배포는 고가용성과 내구성을 제공하지만, 성능 향상에는 직접적인 영향을 미치지 않습니다.
   - 비용이 증가할 수 있습니다.

4. **온디맨드 인스턴스**
   - 온디맨드 인스턴스는 유연성을 제공하지만, 장기적으로 예약 인스턴스보다 비용이 더 많이 들 수 있습니다.

### 추가 학습 자료
- [Amazon RDS 예약 인스턴스 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_WorkingWithReservedDBInstances.html)
- [Amazon RDS 인스턴스 크기 조정 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Overview.DBInstance.Modifying.html)
- [Multi-AZ 배포 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html)
- [온디맨드 인스턴스 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html)

이 자료들을 통해 예약 인스턴스, 인스턴스 크기 조정, Multi-AZ 배포, 온디맨드 인스턴스의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 439번
### 정답
A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.

### 해설
기존 VPC에 추가 IPv4 CIDR 블록을 추가하면 IP 주소 공간을 확장할 수 있습니다. 이는 운영 오버헤드를 최소화하면서 현재 VPC 내에서 IP 주소 부족 문제를 해결하는 가장 간단하고 효율적인 방법입니다.

각 옵션을 자세히 살펴보면:

- **A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.**
  - 기존 VPC에 추가 CIDR 블록을 추가하면 IP 주소 공간을 확장할 수 있습니다. 이는 운영 오버헤드를 최소화하면서 현재 VPC 내에서 IP 주소 부족 문제를 해결하는 가장 간단하고 효율적인 방법입니다.

- **B. Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC. Update the routes and create new resources in the subnets of the second VPC.**
  - 두 번째 VPC를 생성하고 피어링 연결을 설정하는 것은 가능하지만, 이는 운영 오버헤드가 증가할 수 있습니다. 또한, 라우팅 테이블을 업데이트해야 하므로 복잡성이 증가합니다.

- **C. Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPC. Update the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.**
  - Transit Gateway를 사용하여 두 번째 VPC를 연결하는 것은 가능하지만, 이는 운영 오버헤드가 증가할 수 있습니다. 또한, 라우팅 테이블을 업데이트해야 하므로 복잡성이 증가합니다.

- **D. Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the traffic through the VPN. Create new resources in the subnets of the second VPC.**
  - Site-to-Site VPN 연결을 설정하는 것은 가능하지만, 이는 운영 오버헤드가 크게 증가할 수 있습니다. 또한, 라우팅 테이블을 업데이트해야 하므로 복잡성이 증가합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **VPC CIDR 블록 추가**
   - 기존 VPC에 추가 CIDR 블록을 추가하여 IP 주소 공간을 확장할 수 있습니다.
   - 이는 운영 오버헤드를 최소화하면서 IP 주소 부족 문제를 해결하는 가장 간단한 방법입니다.

2. **VPC 피어링**
   - 두 VPC 간에 피어링 연결을 설정하여 트래픽을 라우팅할 수 있습니다.
   - 라우팅 테이블을 업데이트해야 하므로 복잡성이 증가할 수 있습니다.

3. **AWS Transit Gateway**
   - Transit Gateway는 여러 VPC와 온프레미스 네트워크를 연결하는 중앙 허브 역할을 합니다.
   - 라우팅 테이블을 업데이트해야 하므로 복잡성이 증가할 수 있습니다.

4. **Site-to-Site VPN**
   - Site-to-Site VPN은 두 네트워크 간에 암호화된 연결을 설정하는 데 사용됩니다.
   - 운영 오버헤드가 크게 증가할 수 있습니다.

### 추가 학습 자료
- [VPC CIDR 블록 추가 공식 문서](https://docs.aws.amazon.com/ko_kr/vpc/latest/userguide/VPC_Subnets.html#vpc-resize)
- [VPC 피어링 공식 문서](https://docs.aws.amazon.com/ko_kr/vpc/latest/peering/what-is-vpc-peering.html)
- [AWS Transit Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/vpc/latest/tgw/what-is-transit-gateway.html)
- [Site-to-Site VPN 공식 문서](https://docs.aws.amazon.com/ko_kr/vpn/latest/s2svpn/VPC_VPN.html)

이 자료들을 통해 VPC CIDR 블록 추가, VPC 피어링, AWS Transit Gateway, Site-to-Site VPN의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 440번
### 정답
A. Import the RDS snapshot directly into Aurora.
C. Upload the database dump to Amazon S3. Then import the database dump into Aurora.

### 해설
Amazon Aurora는 MySQL과 호환되며, RDS 스냅샷을 직접 가져오거나 데이터베이스 덤프를 사용하여 데이터를 가져올 수 있습니다. 이 두 가지 방법은 새로운 DB 인스턴스를 생성하는 데 가장 적합한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Import the RDS snapshot directly into Aurora.**
  - Amazon Aurora는 MySQL과 호환되므로, RDS 스냅샷을 직접 가져와서 새로운 Aurora DB 인스턴스를 생성할 수 있습니다.

- **B. Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.**
  - RDS 스냅샷을 S3로 업로드한 후 Aurora로 가져오는 것은 필요하지 않습니다. Aurora는 RDS 스냅샷을 직접 가져올 수 있습니다.

- **C. Upload the database dump to Amazon S3. Then import the database dump into Aurora.**
  - 데이터베이스 덤프를 S3에 업로드한 후, Aurora로 가져올 수 있습니다. 이는 데이터베이스 덤프를 사용하는 일반적인 방법입니다.

- **D. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.**
  - AWS DMS는 데이터베이스 마이그레이션을 위한 서비스이지만, RDS 스냅샷을 직접 가져오는 데는 필요하지 않습니다.

- **E. Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.**
  - 데이터베이스 덤프를 S3에 업로드한 후, AWS DMS를 사용하여 Aurora로 가져오는 것은 가능하지만, DMS를 사용하지 않고 직접 가져오는 것이 더 간단합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A와 C입니다.

### 필요한 학습 내용

1. **Amazon Aurora**
   - Aurora는 MySQL과 호환되는 고성능 관계형 데이터베이스 서비스입니다.
   - RDS 스냅샷을 직접 가져올 수 있습니다.

2. **RDS 스냅샷**
   - RDS 스냅샷은 데이터베이스의 백업을 생성하는 데 사용됩니다.
   - Aurora로 직접 가져올 수 있습니다.

3. **데이터베이스 덤프**
   - 데이터베이스 덤프는 데이터베이스의 내용을 파일로 내보내는 방법입니다.
   - S3에 업로드한 후 Aurora로 가져올 수 있습니다.

### 추가 학습 자료
- [Amazon Aurora 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/AuroraUserGuide/Welcome.html)
- [RDS 스냅샷 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html)
- [데이터베이스 덤프 공식 문서](https://dev.mysql.com/doc/refman/8.0/en/mysqldump.html)

이 자료들을 통해 Aurora, RDS 스냅샷, 데이터베이스 덤프의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 442번
### 정답
D. Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts.

### 해설
AWS Lake Formation의 태그 기반 접근 제어(TBAC)는 데이터에 태그를 할당하고, 이러한 태그를 기반으로 접근 권한을 관리할 수 있는 기능을 제공합니다. 이를 통해 여러 계정 간에 데이터를 안전하게 공유할 수 있으며, 운영 오버헤드를 최소화할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Copy the required data to a common account. Create an IAM access role in that account. Grant access by specifying a permission policy that includes users from the engineering team accounts as trusted entities.**
  - 데이터를 공통 계정으로 복사하고 IAM 역할을 생성하는 것은 가능하지만, 데이터 복사와 권한 관리를 위한 추가적인 작업이 필요하므로 운영 오버헤드가 증가할 수 있습니다.

- **B. Use the Lake Formation permissions Grant command in each account where the data is stored to allow the required engineering team users to access the data.**
  - 각 계정에서 Lake Formation 권한 부여 명령을 사용하는 것은 가능하지만, 여러 계정에서 개별적으로 권한을 관리해야 하므로 운영 오버헤드가 증가할 수 있습니다.

- **C. Use AWS Data Exchange to privately publish the required data to the required engineering team accounts.**
  - AWS Data Exchange는 데이터 공유를 위한 서비스이지만, 데이터 교환을 위해 추가적인 설정과 관리가 필요할 수 있습니다.

- **D. Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts.**
  - Lake Formation의 태그 기반 접근 제어(TBAC)를 사용하면 여러 계정 간에 데이터를 안전하게 공유할 수 있으며, 운영 오버헤드를 최소화할 수 있습니다. 이는 가장 효율적인 솔루션입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **AWS Lake Formation**
   - Lake Formation은 데이터 레이크를 쉽게 설정하고 관리할 수 있는 서비스입니다.
   - 태그 기반 접근 제어(TBAC)를 통해 데이터 접근 권한을 관리할 수 있습니다.

2. **태그 기반 접근 제어 (TBAC)**
   - 데이터에 태그를 할당하고, 이러한 태그를 기반으로 접근 권한을 관리할 수 있는 기능입니다.
   - 여러 계정 간에 데이터를 안전하게 공유할 수 있습니다.

3. **IAM 역할 및 권한**
   - IAM 역할을 생성하고 권한을 부여하여 특정 리소스에 대한 접근을 제어할 수 있습니다.
   - 여러 계정 간에 데이터를 공유할 때 사용할 수 있습니다.

### 추가 학습 자료
- [AWS Lake Formation 공식 문서](https://docs.aws.amazon.com/ko_kr/lake-formation/latest/dg/what-is-lake-formation.html)
- [태그 기반 접근 제어 (TBAC) 공식 문서](https://docs.aws.amazon.com/ko_kr/lake-formation/latest/dg/tag-based-access-control.html)
- [IAM 역할 및 권한 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles.html)

이 자료들을 통해 Lake Formation, TBAC, IAM 역할 및 권한의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 443번
### 정답
A. Use Amazon S3 with Transfer Acceleration to host the application.

### 해설
Amazon S3 Transfer Acceleration은 전 세계적으로 분산된 엣지 로케이션을 사용하여 S3 버킷으로의 업로드 및 다운로드 속도를 향상시킵니다. 이는 특히 대용량 데이터를 전송할 때 유용하며, 지리적으로 분산된 사용자에게 낮은 지연 시간과 높은 성능을 제공합니다. 또한, S3는 비용 효율적인 스토리지 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Use Amazon S3 with Transfer Acceleration to host the application.**
  - S3 Transfer Acceleration은 전 세계적으로 분산된 엣지 로케이션을 사용하여 업로드 및 다운로드 속도를 향상시킵니다. 이는 대용량 데이터를 전송할 때 유용하며, 지리적으로 분산된 사용자에게 낮은 지연 시간과 높은 성능을 제공합니다.

- **B. Use Amazon S3 with CacheControl headers to host the application.**
  - CacheControl 헤더는 캐싱을 제어하는 데 사용되지만, 업로드 및 다운로드 속도를 직접적으로 향상시키지는 않습니다.

- **C. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.**
  - EC2와 Auto Scaling, CloudFront를 사용하는 것은 가능하지만, S3 Transfer Acceleration을 사용하는 것이 더 비용 효율적이고 관리 오버헤드가 적습니다.

- **D. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.**
  - ElastiCache는 주로 데이터베이스 캐싱에 사용되며, 업로드 및 다운로드 속도를 직접적으로 향상시키지는 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Amazon S3 Transfer Acceleration**
   - S3 Transfer Acceleration은 전 세계적으로 분산된 엣지 로케이션을 사용하여 S3 버킷으로의 업로드 및 다운로드 속도를 향상시킵니다.
   - 대용량 데이터를 전송할 때 유용하며, 지리적으로 분산된 사용자에게 낮은 지연 시간과 높은 성능을 제공합니다.

2. **Amazon S3**
   - S3는 객체 스토리지 서비스로, 정적 웹 콘텐츠를 호스팅하는 데 적합합니다.
   - 비용 효율적이며, Transfer Acceleration을 통해 성능을 향상시킬 수 있습니다.

3. **Amazon CloudFront**
   - CloudFront는 콘텐츠 전송 네트워크(CDN)로, 전 세계에 분산된 엣지 로케이션을 통해 정적 콘텐츠를 캐싱하고 제공할 수 있습니다.
   - S3 Transfer Acceleration과 함께 사용할 수 있습니다.

4. **Amazon ElastiCache**
   - ElastiCache는 주로 데이터베이스 캐싱에 사용됩니다.
   - 업로드 및 다운로드 속도를 직접적으로 향상시키지는 않습니다.

### 추가 학습 자료
- [Amazon S3 Transfer Acceleration 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/transfer-acceleration.html)
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/Welcome.html)
- [Amazon CloudFront 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonCloudFront/latest/DeveloperGuide/Introduction.html)
- [Amazon ElastiCache 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonElastiCache/latest/red-ug/WhatIs.html)

이 자료들을 통해 S3 Transfer Acceleration, S3, CloudFront, ElastiCache의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 445번
### 정답
A. Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start the transfer to an Amazon S3 bucket.

### 해설
AWS DataSync는 온프레미스 스토리지와 AWS 스토리지 간에 데이터를 효율적으로 전송할 수 있는 서비스입니다. DataSync는 데이터 전송 중에도 데이터에 접근하고 업데이트할 수 있도록 지원하며, 10 Gbps Direct Connect 연결을 통해 대량의 데이터를 빠르게 전송할 수 있습니다. 이는 90일 내에 700TB의 데이터를 클라우드로 이동하는 데 가장 적합한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start the transfer to an Amazon S3 bucket.**
  - DataSync는 대량의 데이터를 효율적으로 전송할 수 있으며, 데이터 전송 중에도 데이터에 접근하고 업데이트할 수 있습니다. 10 Gbps Direct Connect 연결을 통해 빠르게 데이터를 전송할 수 있습니다.

- **B. Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.**
  - Snowball Edge 장치를 사용하는 것은 가능하지만, 데이터 전송 중에 데이터에 접근하고 업데이트하는 데는 적합하지 않습니다. 또한, 장치를 AWS 데이터 센터로 배송하는 데 시간이 걸릴 수 있습니다.

- **C. Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the Direct Connect connection.**
  - rsync를 사용하는 것은 가능하지만, DataSync가 제공하는 최적화된 전송 기능과 데이터 접근 및 업데이트 기능을 제공하지 않습니다.

- **D. Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.**
  - 테이프 백업을 사용하는 것은 가능하지만, 데이터 전송 중에 데이터에 접근하고 업데이트하는 데는 적합하지 않습니다. 또한, 테이프를 AWS 데이터 센터로 배송하는 데 시간이 걸릴 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **AWS DataSync**
   - DataSync는 온프레미스 스토리지와 AWS 스토리지 간에 데이터를 효율적으로 전송할 수 있는 서비스입니다.
   - 데이터 전송 중에도 데이터에 접근하고 업데이트할 수 있습니다.

2. **AWS Direct Connect**
   - Direct Connect는 온프레미스 네트워크와 AWS 간에 전용 네트워크 연결을 제공하여 안정적이고 빠른 데이터 전송을 지원합니다.

3. **Amazon S3**
   - S3는 객체 스토리지 서비스로, 대량의 데이터를 저장하고 관리하는 데 적합합니다.

### 추가 학습 자료
- [AWS DataSync 공식 문서](https://docs.aws.amazon.com/ko_kr/datasync/latest/userguide/what-is-datasync.html)
- [AWS Direct Connect 공식 문서](https://docs.aws.amazon.com/ko_kr/directconnect/latest/UserGuide/Welcome.html)
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/Welcome.html)

이 자료들을 통해 DataSync, Direct Connect, S3의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 447번
### 정답
A. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.

### 해설
Amazon Route 53은 DNS 기반의 트래픽 라우팅 서비스로, 여러 리전에 걸쳐 트래픽을 분산시키고 장애 발생 시 자동으로 장애 조치를 수행할 수 있습니다. Route 53의 활성-활성(Active-Active) 장애 조치 구성과 헬스 체크를 사용하면, 여러 리전에 걸쳐 트래픽을 분산시키고, 특정 리전에 장애가 발생할 경우 다른 리전으로 트래픽을 자동으로 라우팅할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.**
  - Route 53 헬스 체크와 활성-활성 장애 조치 구성을 사용하면, 여러 리전에 걸쳐 트래픽을 분산시키고 장애 발생 시 자동으로 장애 조치를 수행할 수 있습니다. 이는 다중 리전 배포와 장애 조치 요구 사항을 충족하는 가장 적합한 솔루션입니다.

- **B. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route traffic.**
  - CloudFront는 주로 콘텐츠 전송 네트워크(CDN)로 사용되며, 다중 리전 트래픽 라우팅을 위한 최적의 솔루션은 아닙니다.

- **C. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.**
  - Transit Gateway는 주로 VPC 간의 네트워크 연결을 관리하는 데 사용되며, 다중 리전 트래픽 라우팅을 위한 최적의 솔루션은 아닙니다.

- **D. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.**
  - Application Load Balancer는 주로 단일 리전 내에서 트래픽을 분산시키는 데 사용되며, 다중 리전 트래픽 라우팅을 위한 최적의 솔루션은 아닙니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Amazon Route 53**
   - Route 53은 DNS 기반의 트래픽 라우팅 서비스로, 여러 리전에 걸쳐 트래픽을 분산시키고 장애 발생 시 자동으로 장애 조치를 수행할 수 있습니다.
   - 활성-활성(Active-Active) 장애 조치 구성을 사용하여 다중 리전 배포와 장애 조치 요구 사항을 충족할 수 있습니다.

2. **헬스 체크**
   - 헬스 체크는 리소스의 상태를 모니터링하고, 장애 발생 시 자동으로 장애 조치를 수행할 수 있는 기능입니다.
   - Route 53 헬스 체크를 사용하여 여러 리전에 걸쳐 트래픽을 분산시키고 장애 발생 시 자동으로 장애 조치를 수행할 수 있습니다.

### 추가 학습 자료
- [Amazon Route 53 공식 문서](https://docs.aws.amazon.com/ko_kr/Route53/latest/DeveloperGuide/Welcome.html)
- [Route 53 헬스 체크 공식 문서](https://docs.aws.amazon.com/ko_kr/Route53/latest/DeveloperGuide/dns-failover.html)

이 자료들을 통해 Route 53과 헬스 체크의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 448번
### 정답
C. Add a second set of VPNs to the Management VPC from a second customer gateway device.

### 해설
현재 아키텍처에서 Management VPC는 단일 고객 게이트웨이 장치를 통해 VPN을 사용하여 데이터 센터에 연결됩니다. 이는 단일 장애 지점(SPOF)을 형성합니다. 이 문제를 해결하려면 두 번째 고객 게이트웨이 장치를 추가하고, Management VPC에 두 번째 VPN 세트를 설정하여 고가용성을 보장해야 합니다.

각 옵션을 자세히 살펴보면:

- **A. Add a set of VPNs between the Management and Production VPCs.**
  - Management VPC와 Production VPC 간의 VPN을 추가하는 것은 두 VPC 간의 통신을 보장할 수 있지만, 데이터 센터와의 연결에 대한 단일 장애 지점을 해결하지는 못합니다.

- **B. Add a second virtual private gateway and attach it to the Management VPC.**
  - 두 번째 가상 프라이빗 게이트웨이를 추가하는 것은 가능하지만, 고객 게이트웨이 장치의 단일 장애 지점을 해결하지는 못합니다.

- **C. Add a second set of VPNs to the Management VPC from a second customer gateway device.**
  - 두 번째 고객 게이트웨이 장치를 추가하고, Management VPC에 두 번째 VPN 세트를 설정하면, 데이터 센터와의 연결에 대한 단일 장애 지점을 해결할 수 있습니다. 이는 고가용성을 보장하는 가장 적합한 솔루션입니다.

- **D. Add a second VPC peering connection between the Management VPC and the Production VPC.**
  - 두 번째 VPC 피어링 연결을 추가하는 것은 두 VPC 간의 통신을 보장할 수 있지만, 데이터 센터와의 연결에 대한 단일 장애 지점을 해결하지는 못합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **VPN 연결**
   - VPN 연결은 온프레미스 네트워크와 AWS VPC 간에 암호화된 연결을 설정하는 데 사용됩니다.
   - 두 번째 고객 게이트웨이 장치를 추가하고, 두 번째 VPN 세트를 설정하여 고가용성을 보장할 수 있습니다.

2. **고객 게이트웨이**
   - 고객 게이트웨이는 온프레미스 네트워크와 AWS 간의 VPN 연결을 설정하는 데 사용되는 장치입니다.
   - 두 번째 고객 게이트웨이 장치를 추가하여 단일 장애 지점을 해결할 수 있습니다.

3. **VPC 피어링**
   - VPC 피어링은 두 VPC 간에 트래픽을 라우팅할 수 있는 기능을 제공합니다.
   - 두 VPC 간의 통신을 보장할 수 있지만, 데이터 센터와의 연결에 대한 단일 장애 지점을 해결하지는 못합니다.

### 추가 학습 자료
- [AWS VPN 연결 공식 문서](https://docs.aws.amazon.com/ko_kr/vpn/latest/s2svpn/VPC_VPN.html)
- [고객 게이트웨이 공식 문서](https://docs.aws.amazon.com/ko_kr/vpn/latest/s2svpn/SetUpVPNConnections.html)
- [VPC 피어링 공식 문서](https://docs.aws.amazon.com/ko_kr/vpc/latest/peering/what-is-vpc-peering.html)

이 자료들을 통해 VPN 연결, 고객 게이트웨이, VPC 피어링의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 454번
### 정답
C. Use Workload Discovery on AWS to generate architecture diagrams of the workloads.

### 해설
Workload Discovery on AWS는 AWS 리소스의 아키텍처 다이어그램을 자동으로 생성하고, 리소스 간의 관계를 시각화하는 도구입니다. 이는 여러 AWS 계정과 리전에 걸쳐 있는 리소스의 관계를 효율적으로 매핑하고 시각화하는 데 가장 적합한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Use AWS Systems Manager Inventory to generate a map view from the detailed view report.**
  - AWS Systems Manager Inventory는 인벤토리 데이터를 수집하고 관리하는 데 유용하지만, 리소스 간의 관계를 시각화하는 데는 적합하지 않습니다.

- **B. Use AWS Step Functions to collect workload details. Build architecture diagrams of the workloads manually.**
  - AWS Step Functions를 사용하여 워크로드 세부 정보를 수집할 수 있지만, 아키텍처 다이어그램을 수동으로 작성해야 하므로 운영 효율성이 떨어집니다.

- **C. Use Workload Discovery on AWS to generate architecture diagrams of the workloads.**
  - Workload Discovery on AWS는 AWS 리소스의 아키텍처 다이어그램을 자동으로 생성하고, 리소스 간의 관계를 시각화하는 데 가장 적합한 솔루션입니다.

- **D. Use AWS X-Ray to view the workload details. Build architecture diagrams with relationships.**
  - AWS X-Ray는 애플리케이션의 성능을 모니터링하고 분석하는 데 유용하지만, 리소스 간의 관계를 시각화하는 데는 적합하지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Workload Discovery on AWS**
   - Workload Discovery on AWS는 AWS 리소스의 아키텍처 다이어그램을 자동으로 생성하고, 리소스 간의 관계를 시각화하는 도구입니다.
   - 여러 AWS 계정과 리전에 걸쳐 있는 리소스의 관계를 효율적으로 매핑하고 시각화할 수 있습니다.

2. **AWS Systems Manager Inventory**
   - Systems Manager Inventory는 인벤토리 데이터를 수집하고 관리하는 데 유용합니다.
   - 리소스 간의 관계를 시각화하는 데는 적합하지 않습니다.

3. **AWS Step Functions**
   - Step Functions는 서버리스 워크플로를 조정하는 데 사용됩니다.
   - 워크로드 세부 정보를 수집할 수 있지만, 아키텍처 다이어그램을 수동으로 작성해야 하므로 운영 효율성이 떨어집니다.

4. **AWS X-Ray**
   - X-Ray는 애플리케이션의 성능을 모니터링하고 분석하는 데 유용합니다.
   - 리소스 간의 관계를 시각화하는 데는 적합하지 않습니다.

### 추가 학습 자료
- [Workload Discovery on AWS 공식 문서](https://aws.amazon.com/solutions/implementations/workload-discovery/)
- [AWS Systems Manager Inventory 공식 문서](https://docs.aws.amazon.com/ko_kr/systems-manager/latest/userguide/systems-manager-inventory.html)
- [AWS Step Functions 공식 문서](https://docs.aws.amazon.com/ko_kr/step-functions/latest/dg/welcome.html)
- [AWS X-Ray 공식 문서](https://docs.aws.amazon.com/ko_kr/xray/latest/devguide/aws-xray.html)

이 자료들을 통해 Workload Discovery on AWS, Systems Manager Inventory, Step Functions, X-Ray의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 455번
### 정답
B. Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the required AWS accounts.
D. Create an IAM role for AWS Budgets to run budget actions with the required permissions.
F. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.

### 해설
AWS Budgets를 사용하여 예산을 설정하고, 예산 임계값에 도달했을 때 알림을 받고 추가 리소스 프로비저닝을 자동으로 방지할 수 있습니다. 이를 위해 IAM 역할을 생성하여 예산 작업을 실행하고, 서비스 제어 정책(SCP)을 사용하여 추가 리소스 프로비저닝을 방지할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports section of the required AWS accounts.**
  - Cost and Usage Reports 섹션에서 예산을 설정하는 것은 적절하지 않습니다. 예산은 Billing 대시보드에서 설정해야 합니다.

- **B. Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the required AWS accounts.**
  - Billing 대시보드에서 예산을 설정하는 것이 적절합니다.

- **C. Create an IAM user for AWS Budgets to run budget actions with the required permissions.**
  - IAM 사용자를 생성하는 대신 IAM 역할을 생성하는 것이 더 적절합니다.

- **D. Create an IAM role for AWS Budgets to run budget actions with the required permissions.**
  - IAM 역할을 생성하여 예산 작업을 실행하는 것이 적절합니다.

- **E. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate config rule to prevent provisioning of additional resources.**
  - Config 규칙을 사용하는 대신 서비스 제어 정책(SCP)을 사용하는 것이 더 적절합니다.

- **F. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.**
  - 예산 임계값에 도달했을 때 알림을 받고, SCP를 사용하여 추가 리소스 프로비저닝을 방지하는 것이 적절합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B, D, F입니다.

### 필요한 학습 내용

1. **AWS Budgets**
   - AWS Budgets를 사용하여 예산을 설정하고, 예산 임계값에 도달했을 때 알림을 받을 수 있습니다.
   - 예산 작업을 통해 추가 리소스 프로비저닝을 방지할 수 있습니다.

2. **IAM 역할**
   - IAM 역할을 생성하여 예산 작업을 실행할 수 있습니다.
   - 필요한 권한을 부여하여 예산 작업을 실행할 수 있습니다.

3. **서비스 제어 정책 (SCP)**
   - SCP를 사용하여 조직 내의 AWS 계정에서 리소스 프로비저닝을 제어할 수 있습니다.
   - 예산 임계값에 도달했을 때 추가 리소스 프로비저닝을 방지할 수 있습니다.

### 추가 학습 자료
- [AWS Budgets 공식 문서](https://docs.aws.amazon.com/ko_kr/cost-management/latest/userguide/budgets-managing-costs.html)
- [IAM 역할 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles.html)
- [서비스 제어 정책 (SCP) 공식 문서](https://docs.aws.amazon.com/ko_kr/organizations/latest/userguide/orgs_manage_policies_scp.html)

이 자료들을 통해 AWS Budgets, IAM 역할, SCP의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 457번
### 정답
C. Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP authentication.

### 해설
AWS Transfer Family는 SFTP, FTPS, FTP 및 AS2와 같은 프로토콜을 지원하여 데이터를 전송할 수 있는 관리형 서비스입니다. 또한, AWS Lambda를 사용하여 사용자 인증을 위한 커스텀 로직을 구현할 수 있습니다. 이 솔루션은 AS2 프로토콜을 지원하며, IdP를 통한 사용자 인증 요구 사항을 충족할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Use AWS DataSync to transfer the data. Create an AWS Lambda function for IdP authentication.**
  - AWS DataSync는 데이터 전송을 자동화하는 데 유용하지만, AS2 프로토콜을 지원하지 않습니다.

- **B. Use Amazon AppFlow flows to transfer the data. Create an Amazon Elastic Container Service (Amazon ECS) task for IdP authentication.**
  - Amazon AppFlow는 SaaS 애플리케이션 간의 데이터 전송을 자동화하는 데 유용하지만, AS2 프로토콜을 지원하지 않습니다.

- **C. Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP authentication.**
  - AWS Transfer Family는 AS2 프로토콜을 지원하며, AWS Lambda를 사용하여 사용자 인증을 위한 커스텀 로직을 구현할 수 있습니다. 이는 요구 사항을 가장 효율적으로 충족하는 솔루션입니다.

- **D. Use AWS Storage Gateway to transfer the data. Create an Amazon Cognito identity pool for IdP authentication.**
  - AWS Storage Gateway는 온프레미스 스토리지와 AWS 클라우드 간의 데이터 전송을 지원하지만, AS2 프로토콜을 지원하지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **AWS Transfer Family**
   - AWS Transfer Family는 SFTP, FTPS, FTP 및 AS2와 같은 프로토콜을 지원하여 데이터를 전송할 수 있는 관리형 서비스입니다.
   - AS2 프로토콜을 지원하며, 사용자 인증을 위한 커스텀 로직을 구현할 수 있습니다.

2. **AWS Lambda**
   - AWS Lambda는 서버리스 컴퓨팅 서비스로, 사용자 인증을 위한 커스텀 로직을 구현할 수 있습니다.
   - Transfer Family와 통합하여 사용자 인증을 처리할 수 있습니다.

3. **IdP 인증**
   - IdP를 사용하여 사용자 인증을 처리할 수 있습니다.
   - Lambda를 사용하여 IdP 인증 로직을 구현할 수 있습니다.

### 추가 학습 자료
- [AWS Transfer Family 공식 문서](https://docs.aws.amazon.com/ko_kr/transfer/latest/userguide/what-is-aws-transfer.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)
- [IdP 인증 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles_providers.html)

이 자료들을 통해 AWS Transfer Family, AWS Lambda, IdP 인증의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 463번
### 정답
C. Use AWS Lambda with a Python script

### 해설
AWS Lambda는 서버리스 컴퓨팅 서비스로, 짧은 시간 동안 실행되는 작업에 대해 비용 효율적인 솔루션을 제공합니다. Lambda 함수는 사용한 만큼만 비용을 지불하며, 인프라 관리가 필요 없습니다. 주어진 시나리오에서 데이터 처리에 필요한 메모리와 실행 시간이 Lambda의 제한 내에 있으므로, Lambda를 사용하는 것이 가장 비용 효율적입니다.

각 옵션을 자세히 살펴보면:

- **A. Use AWS Glue with a Scala job**
  - AWS Glue는 데이터 처리 및 변환을 자동화하는 데 유용하지만, Lambda를 사용하는 것이 더 비용 효율적입니다.

- **B. Use Amazon EMR with an Apache Spark script**
  - Amazon EMR은 대규모 데이터 처리에 적합하지만, 작은 데이터 처리 작업에는 비용이 많이 들 수 있습니다.

- **C. Use AWS Lambda with a Python script**
  - Lambda는 서버리스 컴퓨팅 서비스로, 짧은 시간 동안 실행되는 작업에 대해 비용 효율적인 솔루션을 제공합니다. 주어진 시나리오에서 Lambda를 사용하는 것이 가장 비용 효율적입니다.

- **D. Use AWS Glue with a PySpark job**
  - AWS Glue는 데이터 처리 및 변환을 자동화하는 데 유용하지만, Lambda를 사용하는 것이 더 비용 효율적입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **AWS Lambda**
   - Lambda는 서버리스 컴퓨팅 서비스로, 인프라 관리 없이 코드를 실행할 수 있습니다.
   - 사용한 만큼만 비용을 지불하며, 짧은 시간 동안 실행되는 작업에 대해 비용 효율적인 솔루션을 제공합니다.

2. **AWS Glue**
   - Glue는 데이터 처리 및 변환을 자동화하는 데 유용한 관리형 ETL 서비스입니다.
   - 대규모 데이터 처리에 적합하지만, 작은 데이터 처리 작업에는 비용이 많이 들 수 있습니다.

3. **Amazon EMR**
   - EMR은 대규모 데이터 처리에 적합한 관리형 클러스터 플랫폼입니다.
   - 작은 데이터 처리 작업에는 비용이 많이 들 수 있습니다.

### 추가 학습 자료
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)
- [AWS Glue 공식 문서](https://docs.aws.amazon.com/ko_kr/glue/latest/dg/what-is-glue.html)
- [Amazon EMR 공식 문서](https://docs.aws.amazon.com/ko_kr/emr/latest/ManagementGuide/emr-what-is-emr.html)

이 자료들을 통해 Lambda, Glue, EMR의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 465번
### 정답
C. Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

### 해설
Amazon EBS Multi-Attach 기능은 여러 EC2 인스턴스에서 동시에 하나의 EBS 볼륨에 액세스할 수 있게 해줍니다. 이 기능은 현재 Provisioned IOPS SSD (io2) 볼륨에서만 지원됩니다. io2 볼륨은 높은 성능과 안정성을 제공하므로, 여러 인스턴스에서 동시에 블록 스토리지에 쓰기 작업을 수행해야 하는 애플리케이션에 적합합니다.

각 옵션을 자세히 살펴보면:

- **A. Use General Purpose SSD (gp3) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach**
  - gp3 볼륨은 Multi-Attach 기능을 지원하지 않습니다.

- **B. Use Throughput Optimized HDD (st1) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach**
  - st1 볼륨은 Multi-Attach 기능을 지원하지 않습니다.

- **C. Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach**
  - io2 볼륨은 Multi-Attach 기능을 지원하며, 높은 성능과 안정성을 제공합니다. 이는 여러 인스턴스에서 동시에 블록 스토리지에 쓰기 작업을 수행해야 하는 애플리케이션에 적합합니다.

- **D. Use General Purpose SSD (gp2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach**
  - gp2 볼륨은 Multi-Attach 기능을 지원하지 않습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Amazon EBS Multi-Attach**
   - Multi-Attach 기능은 여러 EC2 인스턴스에서 동시에 하나의 EBS 볼륨에 액세스할 수 있게 해줍니다.
   - 현재 Provisioned IOPS SSD (io2) 볼륨에서만 지원됩니다.

2. **Provisioned IOPS SSD (io2)**
   - io2 볼륨은 높은 성능과 안정성을 제공하며, Multi-Attach 기능을 지원합니다.
   - 여러 인스턴스에서 동시에 블록 스토리지에 쓰기 작업을 수행해야 하는 애플리케이션에 적합합니다.

3. **General Purpose SSD (gp3)**
   - gp3 볼륨은 비용 효율적인 SSD 스토리지 옵션이지만, Multi-Attach 기능을 지원하지 않습니다.

4. **Throughput Optimized HDD (st1)**
   - st1 볼륨은 비용 효율적인 HDD 스토리지 옵션이지만, Multi-Attach 기능을 지원하지 않습니다.

### 추가 학습 자료
- [Amazon EBS Multi-Attach 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ebs-volumes-multi.html)
- [Provisioned IOPS SSD (io2) 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ebs-volume-types.html#io2-ebs-volumes)
- [General Purpose SSD (gp3) 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ebs-volume-types.html#gp3-ebs-volumes)
- [Throughput Optimized HDD (st1) 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ebs-volume-types.html#st1-ebs-volumes)

이 자료들을 통해 EBS Multi-Attach, io2, gp3, st1의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 467번
### 정답
B. Turn on discount sharing from the Billing Preferences section of the account console in the company's Organizations management account.

### 해설
AWS Organizations에서 할인 공유(discount sharing)를 활성화하면, 조직 내의 모든 계정이 Compute Savings Plan의 혜택을 공유할 수 있습니다. 이를 통해 Compute Savings Plan의 사용률을 최적화하고, 전체 조직의 비용을 절감할 수 있습니다. 할인 공유는 조직의 관리 계정에서 활성화해야 합니다.

각 옵션을 자세히 살펴보면:

- **A. Turn on discount sharing from the Billing Preferences section of the account console in the member account that purchased the Compute Savings Plan.**
  - 할인 공유는 조직의 관리 계정에서 활성화해야 하므로, 멤버 계정에서 활성화하는 것은 적절하지 않습니다.

- **B. Turn on discount sharing from the Billing Preferences section of the account console in the company's Organizations management account.**
  - 조직의 관리 계정에서 할인 공유를 활성화하면, 조직 내의 모든 계정이 Compute Savings Plan의 혜택을 공유할 수 있습니다. 이는 Compute Savings Plan의 사용률을 최적화하는 데 가장 적절한 솔루션입니다.

- **C. Migrate additional compute workloads from another AWS account to the account that has the Compute Savings Plan.**
  - 추가적인 컴퓨팅 워크로드를 마이그레이션하는 것은 가능하지만, 할인 공유를 활성화하는 것이 더 효율적입니다.

- **D. Sell the excess Savings Plan commitment in the Reserved Instance Marketplace.**
  - Savings Plan은 Reserved Instance Marketplace에서 판매할 수 없습니다. 이는 Reserved Instances에만 해당됩니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **AWS Organizations**
   - AWS Organizations는 여러 AWS 계정을 중앙에서 관리할 수 있는 서비스입니다.
   - 할인 공유를 통해 조직 내의 모든 계정이 Compute Savings Plan의 혜택을 공유할 수 있습니다.

2. **Compute Savings Plan**
   - Compute Savings Plan은 특정 컴퓨팅 리소스에 대한 비용 절감을 제공하는 예약 요금제입니다.
   - 할인 공유를 통해 조직 내의 모든 계정이 Compute Savings Plan의 혜택을 공유할 수 있습니다.

3. **할인 공유 (Discount Sharing)**
   - 할인 공유를 활성화하면, 조직 내의 모든 계정이 Compute Savings Plan의 혜택을 공유할 수 있습니다.
   - 이는 조직의 관리 계정에서 활성화해야 합니다.

### 추가 학습 자료
- [AWS Organizations 공식 문서](https://docs.aws.amazon.com/ko_kr/organizations/latest/userguide/orgs_introduction.html)
- [Compute Savings Plan 공식 문서](https://docs.aws.amazon.com/ko_kr/savingsplans/latest/userguide/compute-savings-plans.html)
- [할인 공유 공식 문서](https://docs.aws.amazon.com/ko_kr/awsaccountbilling/latest/aboutv2/consolidated-billing.html#turnondiscountsharing)

이 자료들을 통해 AWS Organizations, Compute Savings Plan, 할인 공유의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 468번
### 정답
B. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.

### 해설
Amazon API Gateway는 REST API를 설계하고, Amazon ECS에서 호스팅되는 백엔드 서비스에 액세스할 수 있는 기능을 제공합니다. API Gateway와 Amazon ECS 간의 통신을 위해 VPC 링크를 생성하면, API Gateway가 프라이빗 VPC 서브넷에 있는 ECS 서비스에 안전하게 액세스할 수 있습니다. 이는 REST API를 통해 프론트엔드를 사용자에게 제공하고, 백엔드 서비스에 안전하게 액세스할 수 있는 가장 적절한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.**
  - WebSocket API는 실시간 양방향 통신에 적합하지만, 이 시나리오에서는 REST API가 요구됩니다.

- **B. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.**
  - REST API를 사용하여 프론트엔드를 제공하고, VPC 링크를 통해 API Gateway가 프라이빗 VPC 서브넷에 있는 ECS 서비스에 안전하게 액세스할 수 있습니다. 이는 요구 사항을 가장 효율적으로 충족하는 솔루션입니다.

- **C. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.**
  - WebSocket API는 실시간 양방향 통신에 적합하지만, 이 시나리오에서는 REST API가 요구됩니다. 또한, 보안 그룹만으로는 API Gateway와 ECS 간의 안전한 통신을 보장하기 어렵습니다.

- **D. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.**
  - REST API를 사용하는 것은 적절하지만, 보안 그룹만으로는 API Gateway와 ECS 간의 안전한 통신을 보장하기 어렵습니다. VPC 링크를 사용하는 것이 더 적절합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **Amazon API Gateway**
   - API Gateway는 REST API를 설계하고, 백엔드 서비스에 액세스할 수 있는 기능을 제공합니다.
   - VPC 링크를 통해 프라이빗 VPC 서브넷에 있는 서비스에 안전하게 액세스할 수 있습니다.

2. **Amazon Elastic Container Service (ECS)**
   - ECS는 컨테이너화된 애플리케이션을 관리하고 배포할 수 있는 서비스입니다.
   - 프라이빗 VPC 서브넷에 ECS 서비스를 호스팅할 수 있습니다.

3. **VPC 링크**
   - VPC 링크는 API Gateway와 프라이빗 VPC 서브넷에 있는 서비스 간의 안전한 통신을 보장합니다.
   - API Gateway가 프라이빗 VPC 서브넷에 있는 ECS 서비스에 액세스할 수 있도록 합니다.

### 추가 학습 자료
- [Amazon API Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/apigateway/latest/developerguide/welcome.html)
- [Amazon Elastic Container Service (ECS) 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/developerguide/Welcome.html)
- [VPC 링크 공식 문서](https://docs.aws.amazon.com/ko_kr/apigateway/latest/developerguide/getting-started-with-private-integration.html)

이 자료들을 통해 API Gateway, ECS, VPC 링크의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 470번
### 정답
D. Create an egress-only internet gateway and make it the destination of the subnet's route table

### 해설
Egress-Only Internet Gateway는 IPv6 트래픽에 대해 아웃바운드 인터넷 액세스를 제공하면서, 외부에서의 인바운드 트래픽을 차단하는 데 사용됩니다. 이는 회사의 보안 정책을 준수하면서 EC2 인스턴스가 외부 애플리케이션과 통신할 수 있도록 합니다.

각 옵션을 자세히 살펴보면:

- **A. Create a NAT gateway and make it the destination of the subnet's route table**
  - NAT 게이트웨이는 IPv4 트래픽에 대해 아웃바운드 인터넷 액세스를 제공하지만, IPv6 트래픽에는 적합하지 않습니다.

- **B. Create an internet gateway and make it the destination of the subnet's route table**
  - 인터넷 게이트웨이는 인바운드 및 아웃바운드 트래픽을 모두 허용하므로, 회사의 보안 정책을 준수하지 않습니다.

- **C. Create a virtual private gateway and make it the destination of the subnet's route table**
  - 가상 프라이빗 게이트웨이는 VPC와 온프레미스 네트워크 간의 연결을 제공하는 데 사용되며, 인터넷 액세스에는 적합하지 않습니다.

- **D. Create an egress-only internet gateway and make it the destination of the subnet's route table**
  - Egress-Only Internet Gateway는 IPv6 트래픽에 대해 아웃바운드 인터넷 액세스를 제공하면서, 외부에서의 인바운드 트래픽을 차단합니다. 이는 회사의 보안 정책을 준수하면서 EC2 인스턴스가 외부 애플리케이션과 통신할 수 있도록 합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **Egress-Only Internet Gateway**
   - Egress-Only Internet Gateway는 IPv6 트래픽에 대해 아웃바운드 인터넷 액세스를 제공하면서, 외부에서의 인바운드 트래픽을 차단합니다.
   - 이는 외부 애플리케이션과의 통신을 허용하면서 보안 정책을 준수하는 데 적합합니다.

2. **NAT Gateway**
   - NAT 게이트웨이는 IPv4 트래픽에 대해 아웃바운드 인터넷 액세스를 제공하지만, IPv6 트래픽에는 적합하지 않습니다.

3. **Internet Gateway**
   - 인터넷 게이트웨이는 인바운드 및 아웃바운드 트래픽을 모두 허용하므로, 외부에서의 인바운드 트래픽을 차단하는 데 적합하지 않습니다.

4. **Virtual Private Gateway**
   - 가상 프라이빗 게이트웨이는 VPC와 온프레미스 네트워크 간의 연결을 제공하는 데 사용되며, 인터넷 액세스에는 적합하지 않습니다.

### 추가 학습 자료
- [Egress-Only Internet Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/vpc/latest/userguide/egress-only-internet-gateway.html)
- [NAT Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/vpc/latest/userguide/vpc-nat-gateway.html)
- [Internet Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/vpc/latest/userguide/VPC_Internet_Gateway.html)
- [Virtual Private Gateway 공식 문서](https://docs.aws.amazon.com/ko_kr/vpn/latest/s2svpn/VPC_VPN.html)

이 자료들을 통해 Egress-Only Internet Gateway, NAT Gateway, Internet Gateway, Virtual Private Gateway의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 475번
### 정답
C. Amazon Elastic File System (Amazon EFS) with the Standard storage class

### 해설
Amazon Elastic File System (Amazon EFS)는 고가용성과 내구성을 제공하는 완전 관리형 파일 시스템입니다. EFS는 각 가용 영역에 마운트 타겟을 제공하며, AWS Backup을 사용하여 다른 리전으로 데이터를 복제할 수 있습니다. EFS는 여러 가용 영역에 걸쳐 데이터를 자동으로 복제하여 높은 내구성을 보장하며, AWS Backup을 통해 8시간의 RPO를 충족할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Amazon FSx for Windows File Server with a Multi-AZ deployment**
  - FSx for Windows File Server는 Windows 기반 애플리케이션에 적합하지만, EFS가 더 높은 내구성과 가용성을 제공합니다.

- **B. Amazon FSx for NetApp ONTAP with a Multi-AZ deployment**
  - FSx for NetApp ONTAP는 고성능 파일 시스템을 제공하지만, EFS가 더 높은 내구성과 가용성을 제공합니다.

- **C. Amazon Elastic File System (Amazon EFS) with the Standard storage class**
  - EFS는 고가용성과 내구성을 제공하며, 각 가용 영역에 마운트 타겟을 제공합니다. AWS Backup을 사용하여 다른 리전으로 데이터를 복제할 수 있으며, 8시간의 RPO를 충족할 수 있습니다.

- **D. Amazon FSx for OpenZFS**
  - FSx for OpenZFS는 고성능 파일 시스템을 제공하지만, EFS가 더 높은 내구성과 가용성을 제공합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Amazon Elastic File System (EFS)**
   - EFS는 고가용성과 내구성을 제공하는 완전 관리형 파일 시스템입니다.
   - 각 가용 영역에 마운트 타겟을 제공하며, AWS Backup을 사용하여 다른 리전으로 데이터를 복제할 수 있습니다.

2. **AWS Backup**
   - AWS Backup은 여러 AWS 서비스의 데이터를 백업하고 복구할 수 있는 관리형 서비스입니다.
   - EFS 데이터를 다른 리전으로 복제하여 8시간의 RPO를 충족할 수 있습니다.

3. **Amazon FSx**
   - FSx는 고성능 파일 시스템을 제공하는 서비스입니다.
   - FSx for Windows File Server, FSx for NetApp ONTAP, FSx for OpenZFS는 각각 특정 사용 사례에 적합합니다.

### 추가 학습 자료
- [Amazon Elastic File System (EFS) 공식 문서](https://docs.aws.amazon.com/ko_kr/efs/latest/ug/whatisefs.html)
- [AWS Backup 공식 문서](https://docs.aws.amazon.com/ko_kr/aws-backup/latest/devguide/whatisbackup.html)
- [Amazon FSx 공식 문서](https://docs.aws.amazon.com/ko_kr/fsx/latest/WindowsGuide/what-is.html)

이 자료들을 통해 EFS, AWS Backup, FSx의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.

# 476번
### 정답
C. Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups

### 해설
IAM 그룹에 최소 권한 정책을 적용하여 사용자에게 필요한 최소한의 권한만 부여하는 것이 가장 안전한 방법입니다. 이를 통해 각 부서의 사용자에게 필요한 권한만 부여할 수 있으며, 권한 남용을 방지할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Apply service control policies (SCPs) to manage access permissions**
  - SCP는 AWS Organizations에서 계정 수준의 권한을 관리하는 데 사용되며, 개별 사용자나 그룹에 대한 세부 권한 관리를 위해서는 적합하지 않습니다.

- **B. Create IAM roles that have least privilege permission. Attach the roles to the IAM groups**
  - IAM 역할은 주로 AWS 리소스에 대한 임시 권한을 부여하는 데 사용됩니다. IAM 그룹에 직접 정책을 적용하는 것이 더 적절합니다.

- **C. Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups**
  - IAM 정책을 사용하여 최소 권한을 부여하고, 이를 IAM 그룹에 적용하면 각 부서의 사용자에게 필요한 최소한의 권한만 부여할 수 있습니다. 이는 가장 안전한 방법입니다.

- **D. Create IAM roles. Associate the roles with a permissions boundary that defines the maximum permissions**
  - 권한 경계를 사용하여 최대 권한을 정의할 수 있지만, IAM 그룹에 직접 정책을 적용하는 것이 더 간단하고 효과적입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **IAM 그룹**
   - IAM 그룹은 여러 사용자에게 동일한 권한을 부여할 수 있는 방법입니다.
   - 그룹에 정책을 적용하여 사용자에게 필요한 권한을 부여할 수 있습니다.

2. **IAM 정책**
   - IAM 정책은 AWS 리소스에 대한 권한을 정의하는 JSON 문서입니다.
   - 최소 권한 원칙을 적용하여 사용자에게 필요한 최소한의 권한만 부여할 수 있습니다.

3. **서비스 제어 정책 (SCP)**
   - SCP는 AWS Organizations에서 계정 수준의 권한을 관리하는 데 사용됩니다.
   - 개별 사용자나 그룹에 대한 세부 권한 관리를 위해서는 적합하지 않습니다.

4. **IAM 역할**
   - IAM 역할은 AWS 리소스에 대한 임시 권한을 부여하는 데 사용됩니다.
   - 주로 다른 AWS 서비스나 계정 간의 액세스를 위해 사용됩니다.

### 추가 학습 자료
- [IAM 그룹 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_groups.html)
- [IAM 정책 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/access_policies.html)
- [서비스 제어 정책 (SCP) 공식 문서](https://docs.aws.amazon.com/ko_kr/organizations/latest/userguide/orgs_manage_policies_scp.html)
- [IAM 역할 공식 문서](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles.html)

이 자료들을 통해 IAM 그룹, IAM 정책, SCP, IAM 역할의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 484번
### 정답
A. Create a new organization in AWS Organizations with all features turned on. Create the new AWS accounts in the organization.
E. Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization. Configure IAM Identity Center, and integrate it with the company's corporate directory service.

### 해설
AWS Organizations를 사용하여 여러 AWS 계정을 중앙에서 관리하고, AWS IAM Identity Center (AWS Single Sign-On)를 사용하여 중앙 집중식 인증을 설정하는 것이 가장 적절한 솔루션입니다. AWS Organizations를 사용하면 여러 계정을 중앙에서 관리할 수 있으며, IAM Identity Center를 사용하면 회사의 중앙 디렉터리 서비스와 통합하여 인증을 관리할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Create a new organization in AWS Organizations with all features turned on. Create the new AWS accounts in the organization.**
  - AWS Organizations를 사용하여 여러 계정을 중앙에서 관리할 수 있습니다. 모든 기능을 활성화하면 서비스 제어 정책(SCP) 등 다양한 기능을 사용할 수 있습니다.

- **B. Set up an Amazon Cognito identity pool. Configure AWS IAM Identity Center (AWS Single Sign-On) to accept Amazon Cognito authentication.**
  - Amazon Cognito는 주로 애플리케이션 사용자 인증에 사용되며, 중앙 디렉터리 서비스와의 통합에는 적합하지 않습니다.

- **C. Configure a service control policy (SCP) to manage the AWS accounts. Add AWS IAM Identity Center (AWS Single Sign-On) to AWS Directory Service.**
  - SCP는 계정의 권한을 관리하는 데 사용되지만, 인증을 관리하는 데는 적합하지 않습니다. 또한, IAM Identity Center를 AWS Directory Service에 추가하는 것은 적절하지 않습니다.

- **D. Create a new organization in AWS Organizations. Configure the organization's authentication mechanism to use AWS Directory Service directly.**
  - AWS Organizations는 직접 인증 메커니즘을 제공하지 않습니다. IAM Identity Center를 사용하여 중앙 디렉터리 서비스와 통합하는 것이 더 적절합니다.

- **E. Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization. Configure IAM Identity Center, and integrate it with the company's corporate directory service.**
  - IAM Identity Center를 사용하여 중앙 디렉터리 서비스와 통합하면, 여러 AWS 계정에 대한 중앙 집중식 인증을 설정할 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A와 E입니다.

### 필요한 학습 내용

1. **AWS Organizations**
   - 여러 AWS 계정을 중앙에서 관리할 수 있는 서비스입니다.
   - 모든 기능을 활성화하면 서비스 제어 정책(SCP) 등 다양한 기능을 사용할 수 있습니다.

2. **AWS IAM Identity Center (AWS Single Sign-On)**
   - 여러 AWS 계정에 대한 중앙 집중식 인증을 설정할 수 있는 서비스입니다.
   - 회사의 중앙 디렉터리 서비스와 통합하여 인증을 관리할 수 있습니다.

3. **Amazon Cognito**
   - 주로 애플리케이션 사용자 인증에 사용됩니다.
   - 중앙 디렉터리 서비스와의 통합에는 적합하지 않습니다.

4. **서비스 제어 정책 (SCP)**
   - 계정의 권한을 관리하는 데 사용됩니다.
   - 인증을 관리하는 데는 적합하지 않습니다.

### 추가 학습 자료
- [AWS Organizations 공식 문서](https://docs.aws.amazon.com/ko_kr/organizations/latest/userguide/orgs_introduction.html)
- [AWS IAM Identity Center (AWS Single Sign-On) 공식 문서](https://docs.aws.amazon.com/ko_kr/singlesignon/latest/userguide/what-is.html)
- [Amazon Cognito 공식 문서](https://docs.aws.amazon.com/ko_kr/cognito/latest/developerguide/what-is-amazon-cognito.html)
- [서비스 제어 정책 (SCP) 공식 문서](https://docs.aws.amazon.com/ko_kr/organizations/latest/userguide/orgs_manage_policies_scp.html)

이 자료들을 통해 AWS Organizations, IAM Identity Center, Amazon Cognito, SCP의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 485번
### 정답
A. Store the video archives in Amazon S3 Glacier and use Expedited retrievals.

### 해설
Amazon S3 Glacier는 장기 보관 및 아카이빙을 위한 저비용 스토리지 서비스입니다. S3 Glacier는 데이터를 거의 사용하지 않는 경우에 매우 비용 효율적이며, 필요할 때 데이터를 빠르게 복원할 수 있는 옵션을 제공합니다. Expedited retrievals를 사용하면 데이터를 최대 5분 내에 복원할 수 있습니다. 이는 비용을 최소화하면서도 필요한 경우 데이터를 빠르게 복원할 수 있는 가장 적합한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Store the video archives in Amazon S3 Glacier and use Expedited retrievals.**
  - S3 Glacier는 저비용 스토리지 서비스로, Expedited retrievals를 사용하면 데이터를 최대 5분 내에 복원할 수 있습니다. 이는 비용을 최소화하면서도 필요한 경우 데이터를 빠르게 복원할 수 있는 가장 적합한 솔루션입니다.

- **B. Store the video archives in Amazon S3 Glacier and use Standard retrievals.**
  - Standard retrievals는 데이터 복원 시간이 몇 시간까지 걸릴 수 있으므로, 5분 내에 데이터를 복원해야 하는 요구 사항을 충족하지 못합니다.

- **C. Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).**
  - S3 Standard-IA는 자주 액세스되지 않는 데이터를 위한 스토리지 클래스이지만, S3 Glacier보다 비용이 더 높습니다.

- **D. Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA).**
  - S3 One Zone-IA는 단일 가용 영역에 데이터를 저장하며, S3 Glacier보다 비용이 더 높습니다. 또한, 단일 가용 영역에 데이터를 저장하는 것은 내구성 측면에서 위험할 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Amazon S3 Glacier**
   - S3 Glacier는 장기 보관 및 아카이빙을 위한 저비용 스토리지 서비스입니다.
   - Expedited retrievals를 사용하면 데이터를 최대 5분 내에 복원할 수 있습니다.

2. **Standard retrievals**
   - Standard retrievals는 데이터 복원 시간이 몇 시간까지 걸릴 수 있습니다.
   - 5분 내에 데이터를 복원해야 하는 요구 사항을 충족하지 못합니다.

3. **S3 Standard-Infrequent Access (S3 Standard-IA)**
   - S3 Standard-IA는 자주 액세스되지 않는 데이터를 위한 스토리지 클래스입니다.
   - S3 Glacier보다 비용이 더 높습니다.

4. **S3 One Zone-Infrequent Access (S3 One Zone-IA)**
   - S3 One Zone-IA는 단일 가용 영역에 데이터를 저장하며, S3 Glacier보다 비용이 더 높습니다.
   - 단일 가용 영역에 데이터를 저장하는 것은 내구성 측면에서 위험할 수 있습니다.

### 추가 학습 자료
- [Amazon S3 Glacier 공식 문서](https://docs.aws.amazon.com/ko_kr/amazonglacier/latest/dev/introduction.html)
- [S3 Standard-Infrequent Access (S3 Standard-IA) 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/storage-class-intro.html#sc-infrequent-access)
- [S3 One Zone-Infrequent Access (S3 One Zone-IA) 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/storage-class-intro.html#sc-one-zone-ia)

이 자료들을 통해 S3 Glacier, S3 Standard-IA, S3 One Zone-IA의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 486번
### 정답
A. Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

### 해설
이 솔루션은 세 가지 요구 사항을 모두 충족합니다: 배포를 간소화하고 운영 비용을 줄이는 것입니다.

1. **Amazon S3**는 정적 웹사이트 콘텐츠를 호스팅하는 데 매우 비용 효율적이고 간단한 방법입니다.
2. **Amazon ECS with AWS Fargate**는 서버리스 컨테이너 관리 서비스로, 인프라를 관리할 필요 없이 컨테이너를 실행할 수 있습니다. 이는 운영 비용을 줄이고 배포를 간소화합니다.
3. **Amazon RDS**는 관리형 관계형 데이터베이스 서비스로, 데이터베이스 관리 작업을 자동화하여 운영 비용을 줄입니다.

각 옵션을 자세히 살펴보면:

- **A. Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.**
  - 이 옵션은 정적 콘텐츠 호스팅, 컨테이너화된 애플리케이션 실행, 관리형 데이터베이스 사용을 모두 포함하여 배포를 간소화하고 운영 비용을 줄이는 데 가장 적합합니다.

- **B. Use Amazon CloudFront to host static content. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.**
  - CloudFront는 정적 콘텐츠를 캐시하는 데 유용하지만, S3와 함께 사용하는 것이 더 일반적입니다. 또한, EC2를 사용하면 인프라 관리가 필요하므로 운영 비용이 증가할 수 있습니다.

- **C. Use Amazon S3 to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.**
  - EKS는 Kubernetes 클러스터를 관리하는 데 유용하지만, ECS가 더 간단하고 비용 효율적일 수 있습니다.

- **D. Use Amazon EC2 Reserved Instances to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.**
  - EC2 Reserved Instances를 사용하면 인프라 관리가 필요하므로 운영 비용이 증가할 수 있습니다. 또한, EKS와 EC2를 사용하는 것은 복잡성을 증가시킬 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Amazon S3**
   - 정적 웹사이트 콘텐츠를 호스팅하는 데 매우 비용 효율적이고 간단한 방법입니다.

2. **Amazon Elastic Container Service (ECS) with AWS Fargate**
   - 서버리스 컨테이너 관리 서비스로, 인프라를 관리할 필요 없이 컨테이너를 실행할 수 있습니다.
   - 운영 비용을 줄이고 배포를 간소화합니다.

3. **Amazon RDS**
   - 관리형 관계형 데이터베이스 서비스로, 데이터베이스 관리 작업을 자동화하여 운영 비용을 줄입니다.

4. **Amazon CloudFront**
   - 정적 콘텐츠를 캐시하는 데 유용하지만, S3와 함께 사용하는 것이 더 일반적입니다.

5. **Amazon Elastic Kubernetes Service (EKS)**
   - Kubernetes 클러스터를 관리하는 데 유용하지만, ECS가 더 간단하고 비용 효율적일 수 있습니다.

6. **Amazon EC2 Reserved Instances**
   - 인프라 관리가 필요하므로 운영 비용이 증가할 수 있습니다.

### 추가 학습 자료
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/WebsiteHosting.html)
- [Amazon ECS with AWS Fargate 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/developerguide/what-is-fargate.html)
- [Amazon RDS 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Welcome.html)
- [Amazon CloudFront 공식 문서](https://docs.aws.amazon.com/ko_kr/AmazonCloudFront/latest/DeveloperGuide/Introduction.html)
- [Amazon EKS 공식 문서](https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/what-is-eks.html)
- [Amazon EC2 Reserved Instances 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ec2-reserved-instances.html)

이 자료들을 통해 S3, ECS with Fargate, RDS, CloudFront, EKS, EC2 Reserved Instances의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 487번
### 정답
C. Amazon Elastic File System (Amazon EFS) with multiple mount targets

### 해설
Amazon Elastic File System (Amazon EFS)는 완전 관리형 파일 시스템으로, 여러 Linux 인스턴스에서 동시에 액세스할 수 있으며, 온프레미스 환경에서도 네이티브 프로토콜을 통해 마운트할 수 있습니다. EFS는 고가용성과 확장성을 제공하며, 최소 크기 요구 사항이 없습니다. 여러 마운트 타겟을 사용하면 여러 가용 영역에 걸쳐 고가용성을 보장할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Amazon FSx Multi-AZ deployments**
  - Amazon FSx는 고성능 파일 시스템을 제공하지만, EFS가 더 적합한 요구 사항을 충족합니다.

- **B. Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes**
  - EBS Multi-Attach 볼륨은 여러 인스턴스에서 동시에 액세스할 수 있지만, 파일 시스템으로서의 기능을 제공하지 않으며, 온프레미스 환경에서 네이티브 프로토콜을 통해 마운트할 수 없습니다.

- **C. Amazon Elastic File System (Amazon EFS) with multiple mount targets**
  - EFS는 여러 Linux 인스턴스에서 동시에 액세스할 수 있으며, 온프레미스 환경에서도 네이티브 프로토콜을 통해 마운트할 수 있습니다. 여러 마운트 타겟을 사용하면 고가용성을 보장할 수 있습니다.

- **D. Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points**
  - 단일 마운트 타겟을 사용하는 경우 고가용성을 보장하기 어렵습니다. 여러 마운트 타겟을 사용하는 것이 더 적절합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Amazon Elastic File System (EFS)**
   - 완전 관리형 파일 시스템으로, 여러 Linux 인스턴스에서 동시에 액세스할 수 있습니다.
   - 온프레미스 환경에서도 네이티브 프로토콜을 통해 마운트할 수 있습니다.
   - 고가용성과 확장성을 제공하며, 최소 크기 요구 사항이 없습니다.

2. **Amazon FSx**
   - 고성능 파일 시스템을 제공하지만, EFS가 더 적합한 요구 사항을 충족합니다.

3. **Amazon Elastic Block Store (EBS) Multi-Attach**
   - 여러 인스턴스에서 동시에 액세스할 수 있지만, 파일 시스템으로서의 기능을 제공하지 않습니다.
   - 온프레미스 환경에서 네이티브 프로토콜을 통해 마운트할 수 없습니다.

4. **EFS 마운트 타겟**
   - 여러 마운트 타겟을 사용하면 여러 가용 영역에 걸쳐 고가용성을 보장할 수 있습니다.

### 추가 학습 자료
- [Amazon Elastic File System (EFS) 공식 문서](https://docs.aws.amazon.com/ko_kr/efs/latest/ug/whatisefs.html)
- [Amazon FSx 공식 문서](https://docs.aws.amazon.com/ko_kr/fsx/latest/WindowsGuide/what-is.html)
- [Amazon Elastic Block Store (EBS) Multi-Attach 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/ebs-volumes-multi.html)

이 자료들을 통해 EFS, FSx, EBS Multi-Attach의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 489번
### 정답
C. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.

### 해설
Amazon SNS는 메시지를 전송할 때 실패한 메시지를 처리하기 위해 Dead Letter Queue(DLQ)를 지원합니다. Amazon SQS는 메시지를 최대 14일 동안 보관할 수 있는 큐를 제공하므로, SNS DLQ의 타겟으로 SQS를 설정하면 실패한 메시지를 14일 동안 보관하고 분석할 수 있습니다. 이는 최소한의 개발 노력으로 요구 사항을 충족하는 가장 적합한 솔루션입니다.

각 옵션을 자세히 살펴보면:

- **A. Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days.**
  - Kinesis Data Stream은 실시간 데이터 스트리밍을 위한 서비스로, 메시지 보관 및 분석을 위해서는 추가 개발이 필요합니다.

- **B. Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.**
  - SQS 큐를 SNS와 애플리케이션 사이에 추가하는 것은 불필요한 복잡성을 초래할 수 있습니다. SNS DLQ를 사용하는 것이 더 적절합니다.

- **C. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.**
  - SNS DLQ의 타겟으로 SQS를 설정하면 실패한 메시지를 14일 동안 보관하고 분석할 수 있습니다. 이는 최소한의 개발 노력으로 요구 사항을 충족하는 가장 적합한 솔루션입니다.

- **D. Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL attribute set for a retention period of 14 days.**
  - DynamoDB를 사용하여 메시지를 보관할 수 있지만, SQS를 사용하는 것이 더 간단하고 적절합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 C입니다.

### 필요한 학습 내용

1. **Amazon SNS Dead Letter Queue (DLQ)**
   - SNS는 메시지를 전송할 때 실패한 메시지를 처리하기 위해 DLQ를 지원합니다.
   - DLQ의 타겟으로 SQS를 설정하면 실패한 메시지를 보관하고 분석할 수 있습니다.

2. **Amazon Simple Queue Service (SQS)**
   - SQS는 메시지를 최대 14일 동안 보관할 수 있는 큐를 제공합니다.
   - SNS DLQ의 타겟으로 설정하여 실패한 메시지를 보관하고 분석할 수 있습니다.

3. **Amazon Kinesis Data Stream**
   - 실시간 데이터 스트리밍을 위한 서비스로, 메시지 보관 및 분석을 위해서는 추가 개발이 필요합니다.

4. **Amazon DynamoDB**
   - 메시지를 보관할 수 있지만, SQS를 사용하는 것이 더 간단하고 적절합니다.

### 추가 학습 자료
- [Amazon SNS Dead Letter Queue 공식 문서](https://docs.aws.amazon.com/ko_kr/sns/latest/dg/sns-dead-letter-queues.html)
- [Amazon Simple Queue Service (SQS) 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html)
- [Amazon Kinesis Data Stream 공식 문서](https://docs.aws.amazon.com/ko_kr/streams/latest/dev/introduction.html)
- [Amazon DynamoDB 공식 문서](https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/Introduction.html)

이 자료들을 통해 SNS DLQ, SQS, Kinesis Data Stream, DynamoDB의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 490번
### 정답
B. Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.

### 해설
Amazon DynamoDB는 데이터를 Amazon S3로 직접 내보내는 기능을 제공하며, 이를 통해 연속 백업을 설정할 수 있습니다. 이 방법은 최소한의 코딩으로 구현할 수 있으며, 애플리케이션의 가용성이나 테이블의 읽기 용량 단위(RCU)에 영향을 주지 않습니다. 또한, 포인트 인 타임 복구(PITR)를 활성화하면 특정 시점으로 데이터를 복구할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.**
  - Amazon EMR과 Apache Hive를 사용하는 것은 복잡하고, 백업을 설정하는 데 많은 코딩이 필요합니다.

- **B. Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.**
  - DynamoDB에서 데이터를 S3로 직접 내보내는 기능을 사용하면 최소한의 코딩으로 연속 백업을 설정할 수 있습니다. 이는 애플리케이션의 가용성이나 RCU에 영향을 주지 않습니다.

- **C. Configure Amazon DynamoDB Streams. Create an AWS Lambda function to consume the stream and export the data to an Amazon S3 bucket.**
  - DynamoDB Streams와 Lambda를 사용하는 것은 추가적인 설정과 코딩이 필요합니다. 또한, 스트림을 소비하는 동안 RCU에 영향을 미칠 수 있습니다.

- **D. Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a regular basis. Turn on point-in-time recovery for the table.**
  - Lambda 함수를 사용하여 데이터를 정기적으로 내보내는 것은 추가적인 설정과 코딩이 필요합니다. 또한, 정기적인 내보내기 동안 RCU에 영향을 미칠 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **Amazon DynamoDB 연속 백업**
   - DynamoDB는 데이터를 Amazon S3로 직접 내보내는 기능을 제공하며, 이를 통해 연속 백업을 설정할 수 있습니다.
   - 최소한의 코딩으로 구현할 수 있으며, 애플리케이션의 가용성이나 RCU에 영향을 주지 않습니다.

2. **포인트 인 타임 복구 (PITR)**
   - PITR을 활성화하면 특정 시점으로 데이터를 복구할 수 있습니다.

3. **Amazon EMR과 Apache Hive**
   - EMR과 Hive를 사용하는 것은 복잡하고, 백업을 설정하는 데 많은 코딩이 필요합니다.

4. **Amazon DynamoDB Streams와 AWS Lambda**
   - Streams와 Lambda를 사용하는 것은 추가적인 설정과 코딩이 필요합니다.
   - 스트림을 소비하는 동안 RCU에 영향을 미칠 수 있습니다.

### 추가 학습 자료
- [Amazon DynamoDB 연속 백업 공식 문서](https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/BackupRestore.html)
- [포인트 인 타임 복구 (PITR) 공식 문서](https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/PointInTimeRecovery.html)
- [Amazon EMR 공식 문서](https://docs.aws.amazon.com/ko_kr/emr/latest/ManagementGuide/emr-what-is-emr.html)
- [Amazon DynamoDB Streams 공식 문서](https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/Streams.html)
- [AWS Lambda 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/welcome.html)

이 자료들을 통해 DynamoDB 연속 백업, PITR, EMR과 Hive, DynamoDB Streams와 Lambda의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 491번
### 정답
A. Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS Key Management Service (SSE-KMS) for encryption. Add the kms:Decrypt permission for the Lambda execution role.

### 해설
이 솔루션은 비용 효율적이며, 각 요청을 최소한 한 번 처리할 수 있도록 보장합니다. Amazon SQS 표준 큐는 높은 처리량과 적어도 한 번의 메시지 전달을 보장합니다. AWS Lambda 이벤트 소스 매핑을 사용하여 SQS 표준 큐를 이벤트 소스로 설정하면, Lambda 함수가 큐에서 메시지를 읽고 처리할 수 있습니다. 또한, AWS Key Management Service (SSE-KMS)를 사용하여 데이터를 암호화하고, Lambda 실행 역할에 kms:Decrypt 권한을 추가하여 암호화된 데이터를 처리할 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS Key Management Service (SSE-KMS) for encryption. Add the kms:Decrypt permission for the Lambda execution role.**
  - SQS 표준 큐는 높은 처리량과 적어도 한 번의 메시지 전달을 보장합니다. SSE-KMS를 사용하여 데이터를 암호화하고, Lambda 실행 역할에 kms:Decrypt 권한을 추가하여 암호화된 데이터를 처리할 수 있습니다. 이는 비용 효율적이며 요구 사항을 충족합니다.

- **B. Use AWS Lambda event source mapping. Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use SQS managed encryption keys (SSE-SQS) for encryption. Add the encryption key invocation permission for the Lambda function.**
  - SQS FIFO 큐는 메시지 순서를 보장하지만, 비용이 더 높을 수 있습니다. 또한, SSE-SQS를 사용하면 추가적인 설정이 필요합니다.

- **C. Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use AWS KMS keys (SSE-KMS). Add the kms:Decrypt permission for the Lambda execution role.**
  - SQS FIFO 큐는 메시지 순서를 보장하지만, 비용이 더 높을 수 있습니다. SSE-KMS를 사용하여 데이터를 암호화하고, Lambda 실행 역할에 kms:Decrypt 권한을 추가할 수 있습니다.

- **D. Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS KMS keys (SSE-KMS) for encryption. Add the encryption key invocation permission for the Lambda function.**
  - 이 옵션도 적절하지만, kms:Decrypt 권한을 추가하는 것이 더 명확합니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A입니다.

### 필요한 학습 내용

1. **Amazon SQS 표준 큐**
   - 높은 처리량과 적어도 한 번의 메시지 전달을 보장합니다.

2. **AWS Lambda 이벤트 소스 매핑**
   - Lambda 함수가 SQS 큐에서 메시지를 읽고 처리할 수 있도록 설정합니다.

3. **AWS Key Management Service (SSE-KMS)**
   - 데이터를 암호화하고, Lambda 실행 역할에 kms:Decrypt 권한을 추가하여 암호화된 데이터를 처리할 수 있습니다.

4. **Amazon SQS FIFO 큐**
   - 메시지 순서를 보장하지만, 비용이 더 높을 수 있습니다.

### 추가 학습 자료
- [Amazon SQS 표준 큐 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html)
- [AWS Lambda 이벤트 소스 매핑 공식 문서](https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/invocation-eventsourcemapping.html)
- [AWS Key Management Service (SSE-KMS) 공식 문서](https://docs.aws.amazon.com/ko_kr/kms/latest/developerguide/overview.html)
- [Amazon SQS FIFO 큐 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html)

이 자료들을 통해 SQS 표준 큐, Lambda 이벤트 소스 매핑, SSE-KMS, SQS FIFO 큐의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 492번
### 정답
B. Use AWS Organizations to organize the accounts into organizational units (OUs). Define and attach a service control policy (SCP) to control the usage of EC2 instance types.

### 해설
AWS Organizations를 사용하여 계정을 조직 단위(OU)로 구성하고, 서비스 제어 정책(SCP)을 정의하여 특정 EC2 인스턴스 유형의 사용을 제한하는 것이 가장 적합한 솔루션입니다. SCP는 중앙에서 관리할 수 있으며, 모든 계정에 일관되게 적용할 수 있습니다. 이는 최소한의 개발 노력으로 요구 사항을 충족할 수 있는 방법입니다.

각 옵션을 자세히 살펴보면:

- **A. Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.**
  - Systems Manager 템플릿을 사용하는 것은 추가적인 개발 노력이 필요하며, 모든 사용자가 이를 따르도록 강제하기 어렵습니다.

- **B. Use AWS Organizations to organize the accounts into organizational units (OUs). Define and attach a service control policy (SCP) to control the usage of EC2 instance types.**
  - AWS Organizations와 SCP를 사용하면 중앙에서 계정의 리소스 사용을 제어할 수 있습니다. 이는 최소한의 개발 노력으로 요구 사항을 충족할 수 있는 방법입니다.

- **C. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.**
  - EventBridge와 Lambda를 사용하는 것은 추가적인 개발 노력이 필요하며, 실시간으로 인스턴스를 중지하는 것은 복잡할 수 있습니다.

- **D. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.**
  - Service Catalog를 사용하는 것은 추가적인 설정과 관리가 필요하며, 모든 사용자가 이를 따르도록 강제하기 어렵습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 B입니다.

### 필요한 학습 내용

1. **AWS Organizations**
   - 여러 AWS 계정을 중앙에서 관리할 수 있는 서비스입니다.
   - 조직 단위(OU)로 계정을 구성하고, 서비스 제어 정책(SCP)을 정의하여 리소스 사용을 제어할 수 있습니다.

2. **서비스 제어 정책 (SCP)**
   - SCP는 AWS Organizations에서 계정 수준의 권한을 관리하는 데 사용됩니다.
   - 특정 리소스 유형의 사용을 제한할 수 있습니다.

3. **AWS Systems Manager**
   - Systems Manager 템플릿을 사용하여 EC2 인스턴스를 프로비저닝할 수 있습니다.
   - 추가적인 개발 노력이 필요합니다.

4. **Amazon EventBridge와 AWS Lambda**
   - EventBridge와 Lambda를 사용하여 특정 이벤트에 대한 작업을 자동화할 수 있습니다.
   - 추가적인 개발 노력이 필요합니다.

5. **AWS Service Catalog**
   - Service Catalog를 사용하여 승인된 리소스 템플릿을 제공할 수 있습니다.
   - 추가적인 설정과 관리가 필요합니다.

### 추가 학습 자료
- [AWS Organizations 공식 문서](https://docs.aws.amazon.com/ko_kr/organizations/latest/userguide/orgs_introduction.html)
- [서비스 제어 정책 (SCP) 공식 문서](https://docs.aws.amazon.com/ko_kr/organizations/latest/userguide/orgs_manage_policies_scp.html)
- [AWS Systems Manager 공식 문서](https://docs.aws.amazon.com/ko_kr/systems-manager/latest/userguide/what-is-systems-manager.html)
- [Amazon EventBridge 공식 문서](https://docs.aws.amazon.com/ko_kr/eventbridge/latest/userguide/what-is-amazon-eventbridge.html)
- [AWS Service Catalog 공식 문서](https://docs.aws.amazon.com/ko_kr/servicecatalog/latest/adminguide/introduction.html)

이 자료들을 통해 AWS Organizations, SCP, Systems Manager, EventBridge, Service Catalog의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 499번
### 정답
D. Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account.

### 해설
회사의 평균 연결 사용률이 10% 미만이므로, 1 Gbps Direct Connect 연결은 과도할 수 있습니다. 200 Mbps 호스티드 연결은 비용을 절감하면서도 필요한 대역폭을 제공할 수 있습니다. AWS Direct Connect 파트너를 통해 200 Mbps 호스티드 연결을 주문하면, 보안과 성능을 유지하면서 비용을 줄일 수 있습니다.

각 옵션을 자세히 살펴보면:

- **A. Set up a new 1 Gbps Direct Connect connection. Share the connection with another AWS account.**
  - 1 Gbps 연결을 공유하는 것은 비용을 줄일 수 있지만, 여전히 과도한 대역폭을 제공하며, 다른 계정과의 공유는 보안 문제를 야기할 수 있습니다.

- **B. Set up a new 200 Mbps Direct Connect connection in the AWS Management Console.**
  - AWS Management Console에서는 200 Mbps Direct Connect 연결을 직접 설정할 수 없습니다. AWS Direct Connect 파트너를 통해 호스티드 연결을 주문해야 합니다.

- **C. Contact an AWS Direct Connect Partner to order a 1 Gbps connection. Share the connection with another AWS account.**
  - 1 Gbps 연결을 공유하는 것은 비용을 줄일 수 있지만, 여전히 과도한 대역폭을 제공하며, 다른 계정과의 공유는 보안 문제를 야기할 수 있습니다.

- **D. Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account.**
  - AWS Direct Connect 파트너를 통해 200 Mbps 호스티드 연결을 주문하면, 보안과 성능을 유지하면서 비용을 줄일 수 있습니다. 이는 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션입니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 D입니다.

### 필요한 학습 내용

1. **AWS Direct Connect**
   - AWS Direct Connect는 온프레미스 데이터 센터와 AWS 간의 전용 네트워크 연결을 제공합니다.
   - 다양한 대역폭 옵션을 제공하며, AWS Direct Connect 파트너를 통해 호스티드 연결을 주문할 수 있습니다.

2. **호스티드 연결**
   - AWS Direct Connect 파트너를 통해 제공되는 호스티드 연결은 다양한 대역폭 옵션을 제공하며, 비용을 절감할 수 있습니다.

3. **AWS Management Console**
   - AWS Management Console에서는 Direct Connect 연결을 직접 설정할 수 없습니다. AWS Direct Connect 파트너를 통해 호스티드 연결을 주문해야 합니다.

### 추가 학습 자료
- [AWS Direct Connect 공식 문서](https://docs.aws.amazon.com/ko_kr/directconnect/latest/UserGuide/Welcome.html)
- [AWS Direct Connect 파트너 공식 문서](https://aws.amazon.com/directconnect/partners/)
- [호스티드 연결 공식 문서](https://docs.aws.amazon.com/ko_kr/directconnect/latest/UserGuide/hosted-connections.html)

이 자료들을 통해 AWS Direct Connect, 호스티드 연결, AWS Management Console의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 500번
### 정답
A. Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.
E. Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

### 해설
AWS DataSync와 AWS Snowball Edge Storage Optimized 장치를 사용하면 파일 권한을 유지하면서 데이터를 Amazon FSx for Windows File Server로 마이그레이션하고 통합할 수 있습니다. DataSync는 파일 권한을 포함한 메타데이터를 유지하면서 데이터를 전송할 수 있으며, Snowball Edge는 대량의 데이터를 물리적으로 전송하는 데 적합합니다.

각 옵션을 자세히 살펴보면:

- **A. Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.**
  - DataSync 에이전트를 온프레미스에 배포하고, DataSync 작업을 예약하여 데이터를 FSx for Windows File Server로 전송하면 파일 권한을 유지할 수 있습니다.

- **B. Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.**
  - S3로 데이터를 복사하는 것은 파일 권한을 유지하는 데 적합하지 않습니다.

- **C. Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.**
  - 드라이브를 제거하고 AWS로 배송하는 것은 복잡하고 시간이 많이 걸릴 수 있습니다.

- **D. Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS DataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.**
  - Snowcone 장치는 소규모 데이터 전송에 적합하며, 대량의 데이터를 전송하는 데는 적합하지 않습니다.

- **E. Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.**
  - Snowball Edge Storage Optimized 장치는 대량의 데이터를 물리적으로 전송하는 데 적합하며, DataSync 작업을 예약하여 데이터를 FSx for Windows File Server로 전송할 수 있습니다.

따라서, 회사의 요구 사항을 가장 효율적으로 충족하는 솔루션은 옵션 A와 E입니다.

### 필요한 학습 내용

1. **AWS DataSync**
   - 파일 권한을 포함한 메타데이터를 유지하면서 데이터를 전송할 수 있습니다.
   - 온프레미스에 DataSync 에이전트를 배포하고, DataSync 작업을 예약하여 데이터를 전송할 수 있습니다.

2. **AWS Snowball Edge Storage Optimized**
   - 대량의 데이터를 물리적으로 전송하는 데 적합합니다.
   - 데이터를 AWS로 전송한 후, DataSync 작업을 예약하여 데이터를 FSx for Windows File Server로 전송할 수 있습니다.

3. **Amazon FSx for Windows File Server**
   - Windows 파일 서버와 호환되는 완전 관리형 파일 시스템입니다.
   - 파일 권한을 유지하면서 데이터를 마이그레이션할 수 있습니다.

### 추가 학습 자료
- [AWS DataSync 공식 문서](https://docs.aws.amazon.com/ko_kr/datasync/latest/userguide/what-is-datasync.html)
- [AWS Snowball Edge 공식 문서](https://docs.aws.amazon.com/ko_kr/snowball/latest/developer-guide/what-is-snowball.html)
- [Amazon FSx for Windows File Server 공식 문서](https://docs.aws.amazon.com/ko_kr/fsx/latest/WindowsGuide/what-is.html)

이 자료들을 통해 AWS DataSync, AWS Snowball Edge, Amazon FSx for Windows File Server의 다양한 기능과 설정 방법을 이해하고, 특정 시나리오에서 최적의 솔루션을 선택하는 능력을 키울 수 있습니다.


# 512번
# 514번
# 515번